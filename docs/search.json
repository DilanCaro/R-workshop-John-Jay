[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"ready embark exciting journey world data analysis statistical exploration? Imagine power unlock hidden insights within vast datasets, create stunning visualizations, make data-driven decisions can shape future. Welcome world R programming! R just language; ’s key uncovering stories data tell. Whether ’re budding data scientist, curious researcher, someone simply loves solving puzzles, R offers thrilling adventure ’ll learn command data precision creativity. Get ready amazed endless possibilities R join global community data enthusiasts reshaping world, one analysis time.website, resources, examples practice master R.","code":""},{"path":"index.html","id":"objectives","chapter":"Welcome","heading":"Objectives","text":"","code":""},{"path":"index.html","id":"session-1-r-basics-2-hours","chapter":"Welcome","heading":"Session 1: R Basics (2 hours)","text":"","code":""},{"path":"index.html","id":"part-1-introduction-to-r","chapter":"Welcome","heading":"Part 1: Introduction to R","text":"R use ?Installing R RStudioBasic RStudio layout functionality","code":""},{"path":"index.html","id":"part-2-r-fundamentals","chapter":"Welcome","heading":"Part 2: R Fundamentals","text":"R calculatorVariables data types (numeric, character)Basic arithmetic operations","code":""},{"path":"index.html","id":"part-3-data-structures","chapter":"Welcome","heading":"Part 3: Data Structures","text":"Vectors: Creating, indexing, operationsData frames: Creating exploring data framesImporting exporting data (CSV files)","code":""},{"path":"index.html","id":"part-4-data-manipulation","chapter":"Welcome","heading":"Part 4: Data Manipulation","text":"Subsetting filtering dataAdding, removing, renaming columnsBasic data summary exploration","code":""},{"path":"index.html","id":"part-5-basic-data-visualization","chapter":"Welcome","heading":"Part 5: Basic Data Visualization","text":"Creating simple plots using plot(), hist(), pie(),barplot(),boxplot()","code":""},{"path":"index.html","id":"session-2-intermediate-r-i","chapter":"Welcome","heading":"Session 2: Intermediate R I","text":"","code":""},{"path":"index.html","id":"part-1-functions-and-control-structures","chapter":"Welcome","heading":"Part 1: Functions and Control Structures","text":"Writing using functionsIf statements loops ()","code":""},{"path":"index.html","id":"part-2-data-wrangling-cleaning-and-transformation","chapter":"Welcome","heading":"Part 2: Data Wrangling: Cleaning and Transformation","text":"Reshaping data using dplyr functions (filter, arrange, mutate, summarize)Handling missing data","code":""},{"path":"index.html","id":"session-3-intermediate-r-ii","chapter":"Welcome","heading":"Session 3: Intermediate R II","text":"","code":""},{"path":"index.html","id":"part-1-advanced-data-visualization","chapter":"Welcome","heading":"Part 1: Advanced Data Visualization","text":"Creating customized plots ggplot2Adding titles, labels, themes plots","code":""},{"path":"index.html","id":"part-2-statistical-analysis","chapter":"Welcome","heading":"Part 2: Statistical Analysis","text":"Introduction hypothesis testing statistical testsPerforming t-tests chi-squared tests","code":""},{"path":"index.html","id":"optional-part-3-working-with-dates-and-times","chapter":"Welcome","heading":"(Optional) Part 3: Working with Dates and Times","text":"Handling date time data RCommon date time functions","code":""},{"path":"index.html","id":"optional-session-4-advanced-r","chapter":"Welcome","heading":"(optional) Session 4: Advanced R","text":"","code":""},{"path":"index.html","id":"part-1-advanced-data-manipulation-with-dplyr","chapter":"Welcome","heading":"Part 1: Advanced Data Manipulation with dplyr","text":"Grouping summarizing dataJoining merging datasets","code":""},{"path":"index.html","id":"part-2-text-data-processing","chapter":"Welcome","heading":"Part 2: Text Data Processing","text":"Manipulating analyzing text data using regular expressionsText mining basics","code":""},{"path":"index.html","id":"part-3-building-predictive-models","chapter":"Welcome","heading":"Part 3: Building Predictive Models","text":"Introduction machine learning RCreating predictive models caret","code":""},{"path":"index.html","id":"part-4-bookdown-using-r-markdown-to-create-books","chapter":"Welcome","heading":"Part 4: Bookdown: Using R markdown to create books","text":"Introduction Bookdown PackageCreating simple bookdown book","code":""},{"path":"index.html","id":"part-5-version-control-and-collaboration","chapter":"Welcome","heading":"Part 5: Version Control and Collaboration","text":"Using Git GitHub version control collaboration R projects","code":""},{"path":"r-basics.html","id":"r-basics","chapter":"R Basics","heading":"R Basics","text":"first session, lay foundation R journey. ’ll start introduction R, delving ’s popular choice among data enthusiasts. ’ll learn install R RStudio, user-friendly integrated development environment (IDE). ’ll explore basic layout functionality RStudio, setting stage coding adventures.’ll dive R’s fundamentals, treating trusty calculator. ’ll discover various data types, numeric character, grasp essential arithmetic operations. Moving forward, ’ll explore R’s data structures, including vectors data storage data frames structured datasets. ’ll also become adept importing exporting data using common formats like CSV files.won’t stop . next part session data manipulation. ’ll learn subset filter data, add, remove, rename columns, gain skills basic data summarization exploration. Finally, ’ll wrap basic data visualization, ’ll create simple plots visually represent data.","code":""},{"path":"part-i-introduction-to-r.html","id":"part-i-introduction-to-r","chapter":"Part I: Introduction to R","heading":"Part I: Introduction to R","text":"","code":""},{"path":"part-i-introduction-to-r.html","id":"what-is-r","chapter":"Part I: Introduction to R","heading":"What is R ?","text":"R programming language open-source software environment widely used statistical computing, data analysis, graphics. created Ross Ihaka Robert Gentleman University Auckland, New Zealand, early 1990s now maintained R Development Core Team. R particularly popular among statisticians, data scientists, researchers extensive statistical graphical capabilities.brief sentence: R dialect S.","code":""},{"path":"part-i-introduction-to-r.html","id":"what-is-s","chapter":"Part I: Introduction to R","heading":"What is S ?","text":"S language, developed John Chambers others Bell Telephone Laboratories, began 1976 internal statistical analysis tool, originally based Fortran libraries. evolved significantly time: 1988, rewritten C, leading system akin present form (Version 3).statistical analysis capabilities S detailed 1988 book “Statistical Models S” (white book) Chambers Hastie. current version, Version 4, released 1998 documented Chambers’ “Programming Data” (green book), remains use. ownership S changed hands several times: Bell Labs licensed StatSci (later Insightful Corp.) 1993, Insightful bought Lucent 2004, series acquisitions, TIBCO Software Inc. owned exclusively developed S since 2008. Insightful added features, including GUIs, marketed S-PLUS. Despite changes, core S language remained largely unchanged since 1998, year earned prestigious ACM Software System Award.","code":""},{"path":"part-i-introduction-to-r.html","id":"key-features-and-characteristics-of-r","chapter":"Part I: Introduction to R","heading":"Key features and characteristics of R :","text":"Data Analysis Statistics: R provides wide range statistical techniques libraries data analysis, hypothesis testing, regression analysis, clustering, . ’s known flexibility handling data conducting statistical experiments.Data Analysis Statistics: R provides wide range statistical techniques libraries data analysis, hypothesis testing, regression analysis, clustering, . ’s known flexibility handling data conducting statistical experiments.Data Visualization: R offers powerful tools creating variety high-quality data visualizations, including scatterplots, bar charts, histograms, heatmaps. ggplot2 package, particular, popular choice creating customized graphics.Data Visualization: R offers powerful tools creating variety high-quality data visualizations, including scatterplots, bar charts, histograms, heatmaps. ggplot2 package, particular, popular choice creating customized graphics.Open Source: R open-source software, means freely available anyone use, modify, distribute. led vibrant community users developers contribute packages extensions enhance functionality.Open Source: R open-source software, means freely available anyone use, modify, distribute. led vibrant community users developers contribute packages extensions enhance functionality.Package System: R rich ecosystem packages (libraries) extend core functionality. packages cover wide range domains, machine learning time series analysis bioinformatics geospatial data analysis. Users can easily install use packages tailor R specific needs.Package System: R rich ecosystem packages (libraries) extend core functionality. packages cover wide range domains, machine learning time series analysis bioinformatics geospatial data analysis. Users can easily install use packages tailor R specific needs.Cross-Platform: R runs various operating systems, including Windows, macOS, Linux, making accessible wide range users.Cross-Platform: R runs various operating systems, including Windows, macOS, Linux, making accessible wide range users.Command-Line Interface: R primarily uses command-line interface, although graphical user interfaces (GUIs) available, RStudio, provide user-friendly environment coding data analysis.Command-Line Interface: R primarily uses command-line interface, although graphical user interfaces (GUIs) available, RStudio, provide user-friendly environment coding data analysis.Community Support: R large active community users developers provide support, share code tutorials, contribute ongoing development language.Community Support: R large active community users developers provide support, share code tutorials, contribute ongoing development language.R versatile tool used various fields, including academia, industry, finance, healthcare, , tasks statistical analysis, data visualization, predictive modeling. popularity continues grow data-driven decision-making becomes increasingly important many domains.","code":""},{"path":"part-i-introduction-to-r.html","id":"installing-r-and-rstudio","chapter":"Part I: Introduction to R","heading":"Installing R and Rstudio","text":"Find installation guide pdf file.\nAlternatively , can look videos .Installation R windows Roger PengInstallation R Mac Roger PengHow install Rstudio MacHow install Rstudio Windows","code":""},{"path":"part-i-introduction-to-r.html","id":"basic-rstudio-layout-and-functionality","chapter":"Part I: Introduction to R","heading":"Basic RStudio layout and functionality","text":"\nFigure 1: : http://www.sthda.com/english/wiki/r-basics-quick--easy\nCode Editor/ R script: can write either R code , Rmarkdown. can include instructions computer execute.Code Editor/ R script: can write either R code , Rmarkdown. can include instructions computer execute.R console : see output code run , write code , automatically run enter traced back, R Script useful reusable code.R console : see output code run , write code , automatically run enter traced back, R Script useful reusable code.Workspace history:  space display variables created , use, history , building , git . check data loaded correctly can check see loaded.Workspace history:  space display variables created , use, history , building , git . check data loaded correctly can check see loaded.Plots files:  display graphs plots created, can switch back forth , export, save . Also, can select packages, get help R functions .Plots files:  display graphs plots created, can switch back forth , export, save . Also, can select packages, get help R functions .","code":""},{"path":"part-i-introduction-to-r.html","id":"what-are-packages","chapter":"Part I: Introduction to R","heading":"What are packages?","text":"power R lies packages. Since R open source, many people create packages .e, R scripts contain functions specific problems , may standard deviation , statistics, machine learning .install package, simply typeinstall.packages(\"package name\")Install package Bioconductor: biocLite()Install package Bioconductor: biocLite()Install package GitHub: devtools::install_github()Install package GitHub: devtools::install_github()View list installed packages: installed.packages()View list installed packages: installed.packages()Folder containing installed packages: .libPaths()Folder containing installed packages: .libPaths()load packageTo load packagelibrary(package name)View loaded packagessearch()Unload R package:detach(package name, unload = TRUE)Remove installed packages:remove.packages()Update installed packages:update.packages()","code":""},{"path":"part-ii-r-fundamentals.html","id":"part-ii-r-fundamentals","chapter":"Part II: R Fundamentals","heading":"Part II: R Fundamentals","text":"starting, Official manuals books learning :https://cran.r-project.org/doc/manuals/r-release/R-intro.htmlhttps://cran.r-project.org/doc/manuals/r-release/R-data.htmlhttps://cran.r-project.org/doc/manuals/r-release/R-exts.htmlhttps://cran.r-project.org/doc/manuals/r-release/R-lang.htmlR programming Data Science Roger D. Peng.R Data ScienceIntro R bookR workshopIntro R","code":""},{"path":"part-ii-r-fundamentals.html","id":"r-as-a-calculator","chapter":"Part II: R Fundamentals","heading":"R as a calculator","text":"best way get used R use calculator.can start using R console simple operations.","code":""},{"path":"part-ii-r-fundamentals.html","id":"basic-arithmetic-operations","chapter":"Part II: R Fundamentals","heading":"Basic arithmetic operations","text":"AdditionSubtractionMultiplicationDivisionExponientiation","code":"\n3+5\n#> [1] 8\n143-12\n#> [1] 131\n4*5\n#> [1] 20\n180/23\n#> [1] 7.826087\n4^2\n#> [1] 16"},{"path":"part-ii-r-fundamentals.html","id":"arithmetic-functions","chapter":"Part II: R Fundamentals","heading":"Arithmetic Functions","text":"functions may useful replace calculatorAbsolute valueSquare rootRemainder/moduloLogarithms exponentials\n.e,\\[\\log_a b = c,\\quad ln_e b=, \\quad  e^{}=b\\]","code":"\nabs(-23)\n#> [1] 23\nsqrt(16)\n#> [1] 4\n7 %% 3\n#> [1] 1\nlog2(4)\n#> [1] 2\nlog10(1000)\n#> [1] 3\nlog(4)\n#> [1] 1.386294\nexp(8)\n#> [1] 2980.958\n2.71828^8\n#> [1] 2980.942"},{"path":"part-ii-r-fundamentals.html","id":"variables-and-data-types-numericcharacter","chapter":"Part II: R Fundamentals","heading":"Variables and data types (numeric,character)","text":"","code":""},{"path":"part-ii-r-fundamentals.html","id":"assigment-operators","chapter":"Part II: R Fundamentals","heading":"Assigment Operators","text":"R, create variable , can use assigment symbol <-, = however, later commonly used R.assign value 7 x , doWe can now perform operations variablesRemark: R case sensitive , x different Xcalling print(X) output error","code":"\nx <- 7\nprint(x)\n#> [1] 7\n3*x+3 # 3*7+3 = 21+3 =24\n#> [1] 24\nprint(X)\n#> Error in eval(expr, envir, enclos): object 'X' not found"},{"path":"part-ii-r-fundamentals.html","id":"data-types","chapter":"Part II: R Fundamentals","heading":"Data types","text":"R five basic “atomic” classes objects:characternumeric (real numbers)integercomplexlogical (True/False)","code":""},{"path":"part-ii-r-fundamentals.html","id":"exercise-1","chapter":"Part II: R Fundamentals","heading":"Exercise 1","text":"Run following code, use typeof(), class() functions find data type /class object.Use typeof() class()function find data type variable .","code":"\nmy_numeric <- 42.5\nJohn_jay <- \"university\"\nmy_logical <- TRUE\nmy_date <- as.Date(\"05/29/2018\", \"%m/%d/%Y\")"},{"path":"part-ii-r-fundamentals.html","id":"what-is-the-difference-between-typeof-and-class","chapter":"Part II: R Fundamentals","heading":"What is the difference between typeof() and class()?","text":"can see typeof(my_date) double, class(my_date) Date. typeof output lowest level data type object. class outputs class object.writing code involves checking whether element specific data type , need careful check . Depending function , may give true value reality want false value returned.example, Imagine asked check dates dateframe correct data type.cases might \"05/29/2018\" rare case (maybe due data entry error), \"42.5\"previous comparison , returns true, meaning two data types , maybe youu thought comparing date type, reality , comparing lowest level data types indeed equal (double)Instead, .Character Data type \ncharacter stores character values stringsNumeric Data type numerical values .Integer Data typeFor integers, must specify , , must convert data type. Remark: decimal, remove decimal, acting floor function .can also create integer adding L itRemark: work decimalsComplex Data type\nComplex data types stored x+yi , .e, imaginary componentBoolean Data type \nstores boolean values TRUE FALSE","code":"\ntypeof(my_date) == typeof(my_numeric)\n#> [1] TRUE\nclass(my_date) == class(my_numeric)\n#> [1] FALSE\nchar <- \"This is a character data type\"\nchar\n#> [1] \"This is a character data type\"\ntypeof(char)\n#> [1] \"character\"\nnum <- 3\nprint(num)\n#> [1] 3\nnum_2 <- -2.35\nnum_2\n#> [1] -2.35\ntypeof(num_2)\n#> [1] \"double\"\nclass(num_2)\n#> [1] \"numeric\"\nint <- as.integer(3.6332)\nint\n#> [1] 3\ntypeof(int)\n#> [1] \"integer\"\nint2 <- as.integer(7)\nint2\n#> [1] 7\ntypeof(int2)\n#> [1] \"integer\"\nclass(int2)\n#> [1] \"integer\"\nint3 <- 8L\nint3\n#> [1] 8\nint4 <- 3.4546L\nint4\n#> [1] 3.4546\ncompl <- 13+7i\ncompl\n#> [1] 13+7i\ntypeof(compl)\n#> [1] \"complex\"\ncomplex(real = 23, imaginary = 7)\n#> [1] 23+7i\nmy_bool <- TRUE\nmy_bool\n#> [1] TRUE\n\ntypeof(my_bool)\n#> [1] \"logical\"\n\nmy_boolean <- F\nmy_boolean\n#> [1] FALSE\ntypeof(my_boolean)\n#> [1] \"logical\""},{"path":"part-ii-r-fundamentals.html","id":"converting-data-types","chapter":"Part II: R Fundamentals","heading":"Converting Data types","text":"Convert NumericWe can convert values numeric. Using .numeric() change type keeping values .\nconvertingcomplex: removes imaginary partlogical: TRUE becomes 1 , FALSE becomes 0character: numerical values, letters NAWe can use .numeric() check variable numericConvert integerConverting LogicalReturn FALSE 0 , TRUE otherwise","code":"\n# Complex\nis.numeric(compl)\n#> [1] FALSE\nnumber <- as.numeric(compl)\n#> Warning: imaginary parts discarded in coercion\nnumber\n#> [1] 13\nis.numeric(number)\n#> [1] TRUE\n\n#Logical \nis.numeric(my_bool)\n#> [1] FALSE\nnumber2 <- as.numeric(my_bool)\nnumber2\n#> [1] 1\nis.numeric(number2)\n#> [1] TRUE\n\n# Character\nchar\n#> [1] \"This is a character data type\"\nis.numeric(char)\n#> [1] FALSE\nnumber3 <- as.numeric(char)\n#> Warning: NAs introduced by coercion\nnumber3\n#> [1] NA\nis.numeric(number3)\n#> [1] TRUE\n\nmy_char <- \"2023\"\nis.numeric(my_char)\n#> [1] FALSE\nnumber4 <- as.numeric(my_char)\nnumber4\n#> [1] 2023\nis.numeric(number4)\n#> [1] TRUE\ninte1<-as.integer(\"234\")\ninte1\n#> [1] 234\ntypeof(inte1)\n#> [1] \"integer\"\n\ninte2<-as.integer(23+6i)\n#> Warning: imaginary parts discarded in coercion\ninte2\n#> [1] 23\ntypeof(inte2)\n#> [1] \"integer\"\n\ninte3<-as.integer(F)\ninte3\n#> [1] 0\ntypeof(inte3)\n#> [1] \"integer\"\nprint(as.logical(0))\n#> [1] FALSE\ntypeof(as.logical(0))\n#> [1] \"logical\"\n\nprint(as.logical(-324))\n#> [1] TRUE\ntypeof(as.logical(-324))\n#> [1] \"logical\""},{"path":"part-ii-r-fundamentals.html","id":"exercise-2","chapter":"Part II: R Fundamentals","heading":"Exercise 2","text":"Create 1 datatype : Character, numeric, integer, complex, Boolean","code":""},{"path":"part-ii-r-fundamentals.html","id":"getting-help","chapter":"Part II: R Fundamentals","heading":"Getting help","text":"can use Plots files pane (bottom left pane) click Help search whichever function need help .can also use ? function.open information function plots files pane.","code":"\n?mean"},{"path":"part-iii-data-structures.html","id":"part-iii-data-structures","chapter":"Part III: Data Structures","heading":"Part III: Data Structures","text":"","code":""},{"path":"part-iii-data-structures.html","id":"vectors-creating-indexing-and-operations","chapter":"Part III: Data Structures","heading":"Vectors: Creating, indexing, and operations","text":"can give names columns vector","code":"\n# Creating a vector\nv <- c(1, 2, 3, 4, 5)\nprint(v)\n#> [1] 1 2 3 4 5\n\n# Indexing a vector\nprint(v[2])  # Access the second element\n#> [1] 2\n\n# Vector operations\nv2 <- v * 2  # Multiply each element by 2\nprint(v2)\n#> [1]  2  4  6  8 10\nmy_vector <- c(\"Dilan Caro\", \"Instructor\")\nnames(my_vector) <- c(\"Name\", \"Profession\")\nmy_vector\n#>         Name   Profession \n#> \"Dilan Caro\" \"Instructor\""},{"path":"part-iii-data-structures.html","id":"exercise-1-1","chapter":"Part III: Data Structures","heading":"Exercise 1:","text":"Create vector favorite numbers.Access third element vector.Create new vector square element original vector.Inspect my_vector using:\nattributes(), length() str() function","code":"\nmy_vector <- c(\"Dilan Caro\", \"Instructor\")\nnames(my_vector) <- c(\"Name\", \"Profession\")\nmy_vector\n#>         Name   Profession \n#> \"Dilan Caro\" \"Instructor\""},{"path":"part-iii-data-structures.html","id":"matrices","chapter":"Part III: Data Structures","heading":"Matrices","text":"Matrices vectors dimension attribute. dimension attribute integer vector length 2 (number rows, number columns)Matrices constructed column-wise, entries can thought starting “upper left” corner running columns.Another exampleMatrices can created column-binding row-binding cbind() rbind() functions.","code":"\nm <- matrix(1:6, nrow = 2, ncol = 3) \nm\n#>      [,1] [,2] [,3]\n#> [1,]    1    3    5\n#> [2,]    2    4    6\nmy_matrix <- matrix(1:12, 3, 4, byrow = TRUE)\nmy_matrix\n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    2    3    4\n#> [2,]    5    6    7    8\n#> [3,]    9   10   11   12\nx <- 1:3\ny <- 10:12\ncbind(x, y)\n#>      x  y\n#> [1,] 1 10\n#> [2,] 2 11\n#> [3,] 3 12\nrbind(x, y)\n#>   [,1] [,2] [,3]\n#> x    1    2    3\n#> y   10   11   12"},{"path":"part-iii-data-structures.html","id":"seq-and-rep-functions","chapter":"Part III: Data Structures","heading":"Seq and rep functions","text":"R, seq rep two functions used generate sequences replicate values, respectively.","code":""},{"path":"part-iii-data-structures.html","id":"seq-function","chapter":"Part III: Data Structures","heading":"0.0.1 seq Function:","text":"seq function used create sequence numbers.Usage:seq(, ): Generates sequence ‘’ value ‘’ value default increment 1.seq(, ): Generates sequence ‘’ value ‘’ value default increment 1.seq(, , ): Generates sequence ‘’ value ‘’ value, increment specified ‘’.seq(, , ): Generates sequence ‘’ value ‘’ value, increment specified ‘’.seq(, , length.): Generates sequence ‘’ value ‘’ value specified number equally spaced points.seq(, , length.): Generates sequence ‘’ value ‘’ value specified number equally spaced points.","code":""},{"path":"part-iii-data-structures.html","id":"example","chapter":"Part III: Data Structures","heading":"Example","text":"","code":"\nseq(1, 5)         \n#> [1] 1 2 3 4 5\nseq(1, 10, by = 2)    \n#> [1] 1 3 5 7 9\nseq(1, 10, length.out = 4) \n#> [1]  1  4  7 10"},{"path":"part-iii-data-structures.html","id":"rep-function","chapter":"Part III: Data Structures","heading":"0.0.2 rep Function:","text":"rep function used replicate values vector.Usage:rep(x, times): Replicates element ‘x’ specified number ‘times’.rep(x, times): Replicates element ‘x’ specified number ‘times’.rep(x, ): Replicates element ‘x’ ‘’ times moving next element.rep(x, ): Replicates element ‘x’ ‘’ times moving next element.rep(x, length.): Replicates values ‘x’ ‘length.’ number times total.rep(x, length.): Replicates values ‘x’ ‘length.’ number times total.","code":"\nrep(1:3, times = 2)\n#> [1] 1 2 3 1 2 3\nrep(1:3, each = 2)       \n#> [1] 1 1 2 2 3 3\nrep(1:3, length.out = 7)   \n#> [1] 1 2 3 1 2 3 1"},{"path":"part-iii-data-structures.html","id":"lists","chapter":"Part III: Data Structures","heading":"Lists","text":"Lists special type vector can contain elements different classes. Lists important data type R get know well. Lists, combination various “apply” functions discussed later, make powerful combination.Lists can explicitly created using list() function, takes arbitrary number arguments.","code":"\nx <- list(1, \"a\", TRUE, 1 + 4i) \nx\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] \"a\"\n#> \n#> [[3]]\n#> [1] TRUE\n#> \n#> [[4]]\n#> [1] 1+4i"},{"path":"part-iii-data-structures.html","id":"example-1","chapter":"Part III: Data Structures","heading":"Example","text":"","code":"\nmy_list <- list(one = 1, two = c(1, 2), five = seq(1, 4, length=5),\n          six = c(\"Dilan\", \"April\"))\nnames(my_list)\n#> [1] \"one\"  \"two\"  \"five\" \"six\"\nstr(my_list)\n#> List of 4\n#>  $ one : num 1\n#>  $ two : num [1:2] 1 2\n#>  $ five: num [1:5] 1 1.75 2.5 3.25 4\n#>  $ six : chr [1:2] \"Dilan\" \"April\""},{"path":"part-iii-data-structures.html","id":"factors","chapter":"Part III: Data Structures","heading":"Factors","text":"Factors used represent categorical data can unordered ordered. One can think factor integer vector integer label. Factors important statistical modeling treated specially modelling functions like lm() glm().Using factors labels better using integers factors self-describing. variable values “Male” “Female” better variable values 1 2.Factor objects can created factor() function.Level put alphabetical order, can also define levels.","code":"\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \nx\n#> [1] yes yes no  yes no \n#> Levels: no yes\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"),levels = c(\"yes\", \"no\"))\nx\n#> [1] yes yes no  yes no \n#> Levels: yes no"},{"path":"part-iii-data-structures.html","id":"data-frames-creating-and-exploring-data-frames","chapter":"Part III: Data Structures","heading":"Data frames: Creating and exploring data frames","text":"Data frames used store tabular data R.Data frames represented special type list every element list length. element list can thought column length element list number rows.","code":"\n# Creating a data frame\ndf <- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 35),\n  Salary = c(50000, 60000, 70000)\n)\nprint(df)\n#>      Name Age Salary\n#> 1   Alice  25  50000\n#> 2     Bob  30  60000\n#> 3 Charlie  35  70000\n\n# Exploring data frames\nprint(dim(df))  # Dimensions of the data frame\n#> [1] 3 3\nprint(colnames(df))  # Column names\n#> [1] \"Name\"   \"Age\"    \"Salary\"\nprint(summary(df))  # Summary statistics\n#>      Name                Age           Salary     \n#>  Length:3           Min.   :25.0   Min.   :50000  \n#>  Class :character   1st Qu.:27.5   1st Qu.:55000  \n#>  Mode  :character   Median :30.0   Median :60000  \n#>                     Mean   :30.0   Mean   :60000  \n#>                     3rd Qu.:32.5   3rd Qu.:65000  \n#>                     Max.   :35.0   Max.   :70000"},{"path":"part-iii-data-structures.html","id":"exercise-2-1","chapter":"Part III: Data Structures","heading":"Exercise 2","text":"Create data frame least three columns four rows.Print number rows columns data frame.Display summary statistics data frame.","code":""},{"path":"part-iii-data-structures.html","id":"part-2","chapter":"Part III: Data Structures","heading":"Part 2","text":"","code":""},{"path":"part-iii-data-structures.html","id":"exercise-3","chapter":"Part III: Data Structures","heading":"Exercise 3","text":"Inspect built-data frame, inspect mtcars using str(), head()Get summary variable dataframe, use $ extract variable dataframe.Now inspect tibble, inspect diamonds ggplot2 library. Use str(), head(), summary()Can list differences?","code":"\nmtcars\nstr(mtcars)\nhead(mtcars)\nsummary(mtcars$cyl) # use $ to extract variable from a data frame\nlibrary(ggplot2)\nhead(diamonds)"},{"path":"part-iii-data-structures.html","id":"importing-and-exporting-data-csv-files","chapter":"Part III: Data Structures","heading":"Importing and exporting data (CSV files)","text":"Exporting data CSVImporting data CSV","code":"\nwrite.csv(df, \"my_data.csv\", row.names = FALSE)\ndf_imported <- read.csv(\"my_data.csv\")\nprint(df_imported)\n#>      Name Age Salary\n#> 1   Alice  25  50000\n#> 2     Bob  30  60000\n#> 3 Charlie  35  70000"},{"path":"part-iii-data-structures.html","id":"exercise-4","chapter":"Part III: Data Structures","heading":"Exercise 4","text":"Create vector fav_music names favorite artists.Create vector num_records number records \ncollection artists.Create vector num_concerts number times attended concert artists.Put everything together data frame, assign name my_music data frame change labels information stored columns artist, records concerts.Extract variable num_records data frame my_music.Calculate total number records collection (defined\nset artists).Check structure data frame, ask summary.Previously, exported data imported . may think, purpose already dataframe. prior just example, reality , dataframe loaded R . csv data file coworker shared data engineer procured .First, need obtain data need. , please head tohttps://tinyurl.com/JJAY-R-workshopalternatively,https://drive.google.com/drive/folders/18W5f2AvKT7IVKnJ73McCzQOOqMdP0CwM?usp=sharingDownload data, , click arrow folder, choose download. Find located computer, obtain PathSome useful instructions regarding path names: get working directoryGet working directoryspecify path name, forward slash double back slashuse relative path","code":"\ngetwd()\n#> [1] \"/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay\"\npath <- file.path(\"/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data\")\npath <- file.path(\"./John Jay Workshop Data\")"},{"path":"part-iii-data-structures.html","id":"importing-a-.txt-file","chapter":"Part III: Data Structures","heading":"Importing a .txt file","text":"read.table() one great way import data.like thisWhat happened?","code":"\n\npath.hotdogs <- file.path(path, \"hotdogs.txt\")\npath.hotdogs    # inspect path name\n#> [1] \"./John Jay Workshop Data/hotdogs.txt\"\nhotdogs <- read.table(path.hotdogs, header = FALSE,\n                      col.names = c(\"type\", \"calories\", \"sodium\"))\nstr(hotdogs)    # inspect data imported\n#> 'data.frame':    54 obs. of  3 variables:\n#>  $ type    : chr  \"Beef\" \"Beef\" \"Beef\" \"Beef\" ...\n#>  $ calories: int  186 181 176 149 184 190 158 139 175 148 ...\n#>  $ sodium  : int  495 477 425 322 482 587 370 322 479 375 ...\nhotdogs2 <- read.table(path.hotdogs, header = FALSE,\n                       col.names = c(\"type\", \"calories\", \"sodium\"),\n                       colClasses = c(\"factor\", \"NULL\", \"numeric\"))\nstr(hotdogs2)\n#> 'data.frame':    54 obs. of  2 variables:\n#>  $ type  : Factor w/ 3 levels \"Beef\",\"Meat\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ sodium: num  495 477 425 322 482 587 370 322 479 375 ..."},{"path":"part-iii-data-structures.html","id":"import-.csv-file","chapter":"Part III: Data Structures","heading":"Import .csv file","text":"read.csv() another importing function.example:\n- load data set swimming pools Brisbane\n- column names first row; comma separate values within rows","code":"\npath.pools <- file.path(path, \"swimming_pools.csv\")\npools <- read.csv(path.pools)\nstr(pools)\n#> 'data.frame':    20 obs. of  4 variables:\n#>  $ Name     : chr  \"Acacia Ridge Leisure Centre\" \"Bellbowrie Pool\" \"Carole Park\" \"Centenary Pool (inner City)\" ...\n#>  $ Address  : chr  \"1391 Beaudesert Road, Acacia Ridge\" \"Sugarwood Street, Bellbowrie\" \"Cnr Boundary Road and Waterford Road Wacol\" \"400 Gregory Terrace, Spring Hill\" ...\n#>  $ Latitude : num  -27.6 -27.6 -27.6 -27.5 -27.4 ...\n#>  $ Longitude: num  153 153 153 153 153 ..."},{"path":"part-iii-data-structures.html","id":"import-.xlsx-file","chapter":"Part III: Data Structures","heading":"Import .xlsx file","text":"package read excel data R readxl:external dependencies, easy downloadDesgined work tabular dataSpecify worksheet name number, e.g.inspect re-combine","code":"\nlibrary(readxl)\npath.urbanpop <- file.path(path, \"urbanpop.xlsx\")\nexcel_sheets(path.urbanpop) # list sheet names with excel_sheets()\n#> [1] \"1960-1966\" \"1967-1974\" \"1975-2011\"\npop_1 <- read_excel(path.urbanpop, sheet = 1)\npop_2 <- read_excel(path.urbanpop, sheet = 2)\nstr(pop_1)\n#> tibble [209 × 8] (S3: tbl_df/tbl/data.frame)\n#>  $ country: chr [1:209] \"Afghanistan\" \"Albania\" \"Algeria\" \"American Samoa\" ...\n#>  $ 1960   : num [1:209] 769308 494443 3293999 NA NA ...\n#>  $ 1961   : num [1:209] 814923 511803 3515148 13660 8724 ...\n#>  $ 1962   : num [1:209] 858522 529439 3739963 14166 9700 ...\n#>  $ 1963   : num [1:209] 903914 547377 3973289 14759 10748 ...\n#>  $ 1964   : num [1:209] 951226 565572 4220987 15396 11866 ...\n#>  $ 1965   : num [1:209] 1000582 583983 4488176 16045 13053 ...\n#>  $ 1966   : num [1:209] 1058743 602512 4649105 16693 14217 ...\npop_list <- list(pop_1, pop_2)"},{"path":"part-iii-data-structures.html","id":"import-other-data-formats","chapter":"Part III: Data Structures","heading":"Import other data formats","text":"haven package enables R read write various data formats used statistical packages.supports:SAS: read_sas() reads .sas7bdat .sas7bcat files read_xpt() reads SAS transport files. write_sas() writes .sas7bdat files.SPSS: read_sav() reads .sav files read_por() reads older .por files. write_sav() writes .sav files.Stata: read_dta() reads .dta files. write_dta() writes .dta files.","code":""},{"path":"part-iii-data-structures.html","id":"create-and-format-dates","chapter":"Part III: Data Structures","heading":"0.1 Create and format dates","text":"create Date object simple character string R, can use .Date() function. character string obey format can defined using set symbols (examples correspond 13 January, 1982):%Y: 4-digit year (1982)\n%y: 2-digit year (82)\n%m: 2-digit month (01)\n%d: 2-digit day month (13)\n%: weekday (Wednesday)\n%: abbreviated weekday (Wed)\n%B: month (January)\n%b: abbreviated month (Jan)","code":""},{"path":"part-iii-data-structures.html","id":"exercise-5","chapter":"Part III: Data Structures","heading":"Exercise 5","text":"Load following data sets, available course material:\n- Danish fire insurance losses, stored danish.txt\n- severity data set, stored severity.sas7bdat.","code":""},{"path":"part-iv-data-manipulation.html","id":"part-iv-data-manipulation","chapter":"Part IV: Data Manipulation","heading":"Part IV: Data Manipulation","text":"","code":""},{"path":"part-iv-data-manipulation.html","id":"subsetting-and-filtering-data","chapter":"Part IV: Data Manipulation","heading":"Subsetting and filtering data","text":"Subsetting filtering data involve selecting specific elements, rows, columns dataset based certain conditions criteria. R, subsetting can achieved using square brackets [], subset() function, dplyr package functions like filter() rows select() columns. Filtering refers specifically choosing rows meet certain conditions, values within range matching specific characteristics.","code":"\n# Creating a sample data frame\ndata <- data.frame(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  age = c(25, 30, 22, 28, 24)\n)\n# Subsetting by a specific column\nages <- data$age\nprint(ages)\n#> [1] 25 30 22 28 24\n\n# Filtering data based on a condition\nyoung_adults <- subset(data, age < 30)\nprint(young_adults)\n#>   id    name age\n#> 1  1   Alice  25\n#> 3  3 Charlie  22\n#> 4  4   David  28\n#> 5  5     Eva  24"},{"path":"part-iv-data-manipulation.html","id":"using-the-dplyr-package","chapter":"Part IV: Data Manipulation","heading":"Using the dplyr package","text":"%>% symbol R known pipe operator, ’s used pass result one expression first argument next expressionFiltering data using dplyr individuals younger 30Subsetting columns using dplyr","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nyoung_adults <- data %>% filter(age < 30)\nprint(young_adults)\n#>   id    name age\n#> 1  1   Alice  25\n#> 2  3 Charlie  22\n#> 3  4   David  28\n#> 4  5     Eva  24\nages <- data %>% select(age)\nprint(ages)\n#>   age\n#> 1  25\n#> 2  30\n#> 3  22\n#> 4  28\n#> 5  24"},{"path":"part-iv-data-manipulation.html","id":"adding-removing-and-renaming-columns","chapter":"Part IV: Data Manipulation","heading":"Adding, removing, and renaming columns","text":"Adding new column ‘salary’Removing ‘salary’ columnRenaming ‘name’ column ‘first_name’","code":"\ndata$salary <- c(55000, 50000, 60000, 52000, 58000)\nprint(data)\n#>   id    name age salary\n#> 1  1   Alice  25  55000\n#> 2  2     Bob  30  50000\n#> 3  3 Charlie  22  60000\n#> 4  4   David  28  52000\n#> 5  5     Eva  24  58000\ndata$salary <- NULL\nprint(data)\n#>   id    name age\n#> 1  1   Alice  25\n#> 2  2     Bob  30\n#> 3  3 Charlie  22\n#> 4  4   David  28\n#> 5  5     Eva  24\nnames(data)[names(data) == \"name\"] <- \"first_name\"\nprint(data)\n#>   id first_name age\n#> 1  1      Alice  25\n#> 2  2        Bob  30\n#> 3  3    Charlie  22\n#> 4  4      David  28\n#> 5  5        Eva  24"},{"path":"part-iv-data-manipulation.html","id":"using-the-dplyr-package-1","chapter":"Part IV: Data Manipulation","heading":"Using the dplyr package","text":"Adding new column ‘salary’ using mutateRemoving ‘salary’ column using selectRenaming ‘name’ column ‘first_name’ using rename","code":"\ndata <- data %>%\n  mutate(salary = c(55000, 50000, 60000, 52000, 58000))\nprint(data)\n#>   id    name age salary\n#> 1  1   Alice  25  55000\n#> 2  2     Bob  30  50000\n#> 3  3 Charlie  22  60000\n#> 4  4   David  28  52000\n#> 5  5     Eva  24  58000\ndata <- data %>%\n  select(-salary)\nprint(data)\n#>   id    name age\n#> 1  1   Alice  25\n#> 2  2     Bob  30\n#> 3  3 Charlie  22\n#> 4  4   David  28\n#> 5  5     Eva  24\ndata <- data %>%\n  rename(first_name = name)\nprint(data)\n#>   id first_name age\n#> 1  1      Alice  25\n#> 2  2        Bob  30\n#> 3  3    Charlie  22\n#> 4  4      David  28\n#> 5  5        Eva  24"},{"path":"part-iv-data-manipulation.html","id":"why-use-dyplr","chapter":"Part IV: Data Manipulation","heading":"Why use dyplr","text":"One might think using pipe operator (%>%) magrittr package, prominently used dplyr wider tidyverse unnecessarily complex. may seem complex first, especially accustomed base R functions syntax, offers several benefits can greatly enhance readability, efficiency, overall workflow data analysis. reasons use :Improved Readability ClarityEasier Debugging ModificationEnhanced WorkflowConsistency Community AdoptionEfficiency Writing CodeAn example benefits seen complex operations making readableLet’s just add salary column .Now, using dplyrWithout using pipe operator , looks clear.","code":"\ndata$salary <- c(55000, 50000, 60000, 52000, 58000)\nsubsetting_data <- within(data[data$age < 30, -which(names(data) == \"salary\")], names(name) <- \"first_name\")\nsubsetting_data\n#>   id    name age\n#> 1  1   Alice  25\n#> 3  3 Charlie  22\n#> 4  4   David  28\n#> 5  5     Eva  24\n\ndata <- data %>%\n  mutate(salary = c(55000, 50000, 60000, 52000, 58000))\n\ndata <- data %>%\n  filter(age < 30) %>%\n  select(-salary) %>%\n  rename(first_name = name)\ndata\n#>   id first_name age\n#> 1  1      Alice  25\n#> 2  3    Charlie  22\n#> 3  4      David  28\n#> 4  5        Eva  24\ndata <- data.frame(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  age = c(25, 30, 22, 28, 24)\n)\ndata <- data %>%\n  mutate(salary = c(55000, 50000, 60000, 52000, 58000))\n\n\nsubsetting_data <- rename(select(\n                          filter(data, age < 30), -salary),\n                          first_name = name)\nsubsetting_data\n#>   id first_name age\n#> 1  1      Alice  25\n#> 2  3    Charlie  22\n#> 3  4      David  28\n#> 4  5        Eva  24"},{"path":"part-iv-data-manipulation.html","id":"basic-data-summary-and-exploration","chapter":"Part IV: Data Manipulation","heading":"Basic data summary and exploration","text":"brief summary data exploration given .Summary statistics data frameStructure data frameAverage age individuals data frameCount unique names data frame","code":"\nsummary(data)\n#>        id        name                age      \n#>  Min.   :1   Length:5           Min.   :22.0  \n#>  1st Qu.:2   Class :character   1st Qu.:24.0  \n#>  Median :3   Mode  :character   Median :25.0  \n#>  Mean   :3                      Mean   :25.8  \n#>  3rd Qu.:4                      3rd Qu.:28.0  \n#>  Max.   :5                      Max.   :30.0  \n#>      salary     \n#>  Min.   :50000  \n#>  1st Qu.:52000  \n#>  Median :55000  \n#>  Mean   :55000  \n#>  3rd Qu.:58000  \n#>  Max.   :60000\nstr(data)\n#> 'data.frame':    5 obs. of  4 variables:\n#>  $ id    : int  1 2 3 4 5\n#>  $ name  : chr  \"Alice\" \"Bob\" \"Charlie\" \"David\" ...\n#>  $ age   : num  25 30 22 28 24\n#>  $ salary: num  55000 50000 60000 52000 58000\naverage_age <- mean(data$age)\nprint(average_age)\n#> [1] 25.8\nunique_names_count <- length(unique(data$first_name))\nprint(unique_names_count)\n#> [1] 0"},{"path":"part-iv-data-manipulation.html","id":"exploratory-data-analysis","chapter":"Part IV: Data Manipulation","heading":"Exploratory data analysis","text":"Exploratory Data Analysis (EDA) critical initial step data analysis process, main characteristics dataset examined understand structure, uncover patterns, identify anomalies, test hypotheses. goal use statistical summaries visualizations get sense data, guides analysis modeling decisions. EDA making formal predictions testing hypotheses rather asking questions seeking insights open-ended, exploratory manner.Key Components EDA include:Understanding Distribution various variables dataset. involves looking measures like mean, median, mode, range, variance, standard deviation, using visual tools like histograms, box plots, density plots understand data spread .Understanding Distribution various variables dataset. involves looking measures like mean, median, mode, range, variance, standard deviation, using visual tools like histograms, box plots, density plots understand data spread .Identifying Patterns Relationships variables using scatter plots, pair plots, correlation matrices. helps understanding variables related can guide complex analyses like regression classification.Identifying Patterns Relationships variables using scatter plots, pair plots, correlation matrices. helps understanding variables related can guide complex analyses like regression classification.Detecting Anomalies outliers unexpected values might indicate errors data collection provide insights unusual occurrences data.Detecting Anomalies outliers unexpected values might indicate errors data collection provide insights unusual occurrences data.Cleaning Data addressing missing values, duplicate data, making decisions correct inconsistencies based insights gained.Cleaning Data addressing missing values, duplicate data, making decisions correct inconsistencies based insights gained.Transforming Variables necessary make data suitable analysis. involve normalizing data, creating categorical variables continuous ones, engineering new variables existing ones.Transforming Variables necessary make data suitable analysis. involve normalizing data, creating categorical variables continuous ones, engineering new variables existing ones.Tools TechniquesStatistical Summary Functions R (summary(), mean(), sd(), etc.) provide quick insights basic properties data.Visualization Libraries like ggplot2 R creating wide range plots charts reveal underlying patterns structures data.Importance EDAData Understanding: ensures analyst thorough understanding dataset’s features, values, relationships variables.Data Understanding: ensures analyst thorough understanding dataset’s features, values, relationships variables.Guiding Hypotheses: Insights gained EDA can help form hypotheses statistical testing predictive modeling.Guiding Hypotheses: Insights gained EDA can help form hypotheses statistical testing predictive modeling.Modeling Strategy: Identifying key variables relationships helps choosing appropriate models techniques analysis.Modeling Strategy: Identifying key variables relationships helps choosing appropriate models techniques analysis.summary, EDA essential practice data science making sense data, discovering patterns, identifying potential problems, informing subsequent steps analytical process. blends statistical techniques visual explorations create foundation data-driven task.Now explore dataset library AER , numeric variable first","code":""},{"path":"part-iv-data-manipulation.html","id":"numeric-variable","chapter":"Part IV: Data Manipulation","heading":"Numeric Variable","text":"Obtain summary statistics data frame, check whether numeric, get mean , variance.Now, visualize wage distribution","code":"\n#install.packages(\"AER\")\nlibrary(AER)\n#> Loading required package: car\n#> Loading required package: carData\n#> \n#> Attaching package: 'car'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     recode\n#> Loading required package: lmtest\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> Loading required package: sandwich\n#> Loading required package: survival\ndata(\"CPS1985\")\nstr(CPS1985)\n#> 'data.frame':    534 obs. of  11 variables:\n#>  $ wage      : num  5.1 4.95 6.67 4 7.5 ...\n#>  $ education : num  8 9 12 12 12 13 10 12 16 12 ...\n#>  $ experience: num  21 42 1 4 17 9 27 9 11 9 ...\n#>  $ age       : num  35 57 19 22 35 28 43 27 33 27 ...\n#>  $ ethnicity : Factor w/ 3 levels \"cauc\",\"hispanic\",..: 2 1 1 1 1 1 1 1 1 1 ...\n#>  $ region    : Factor w/ 2 levels \"south\",\"other\": 2 2 2 2 2 2 1 2 2 2 ...\n#>  $ gender    : Factor w/ 2 levels \"male\",\"female\": 2 2 1 1 1 1 1 1 1 1 ...\n#>  $ occupation: Factor w/ 6 levels \"worker\",\"technical\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ sector    : Factor w/ 3 levels \"manufacturing\",..: 1 1 1 3 3 3 3 3 1 3 ...\n#>  $ union     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 1 1 1 1 ...\n#>  $ married   : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 2 1 1 1 2 1 ...\nhead(CPS1985)\n#>       wage education experience age ethnicity region gender\n#> 1     5.10         8         21  35  hispanic  other female\n#> 1100  4.95         9         42  57      cauc  other female\n#> 2     6.67        12          1  19      cauc  other   male\n#> 3     4.00        12          4  22      cauc  other   male\n#> 4     7.50        12         17  35      cauc  other   male\n#> 5    13.07        13          9  28      cauc  other   male\n#>      occupation        sector union married\n#> 1        worker manufacturing    no     yes\n#> 1100     worker manufacturing    no     yes\n#> 2        worker manufacturing    no      no\n#> 3        worker         other    no      no\n#> 4        worker         other    no     yes\n#> 5        worker         other   yes      no\nsummary(CPS1985$wage)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.000   5.250   7.780   9.024  11.250  44.500\nis.numeric(CPS1985$wage)\n#> [1] TRUE\nmean(CPS1985$wage)\n#> [1] 9.024064\nvar(CPS1985$wage)\n#> [1] 26.41032\nhist(log(CPS1985$wage), freq = FALSE, nclass = 20, col = \"light blue\")\nlines(density(log(CPS1985$wage)), col = \"red\")"},{"path":"part-iv-data-manipulation.html","id":"a-factor-variable","chapter":"Part IV: Data Manipulation","heading":"A factor variable","text":"now explore occupation variablechange names levelsvisualize distribution","code":"\nsummary(CPS1985$occupation)\n#>     worker  technical   services     office      sales \n#>        156        105         83         97         38 \n#> management \n#>         55\nlevels(CPS1985$occupation)[c(2, 6)] <- c(\"techn\", \"mgmt\")\nsummary(CPS1985$occupation)\n#>   worker    techn services   office    sales     mgmt \n#>      156      105       83       97       38       55\ntab <- table(CPS1985$occupation)\nprop.table(tab)\n#> \n#>     worker      techn   services     office      sales \n#> 0.29213483 0.19662921 0.15543071 0.18164794 0.07116105 \n#>       mgmt \n#> 0.10299625\nbarplot(tab)\npie(tab, col = gray(seq(0.4, 1.0, length = 6)))"},{"path":"part-iv-data-manipulation.html","id":"two-factor-variables","chapter":"Part IV: Data Manipulation","heading":"Two factor variables","text":"now explore factor variables gender occupation.Use prop.table()prop.table() function R used compute proportion table elements margin specified (). applied contingency table created table() function, transforms table’s counts proportions, making easier analyze relative distribution frequencies across different categories.Now try prop.table(table(gender, occupation), 2)now explore factor variables gender occupation.mosaic plotA mosaic plot, also known Marimekko diagram mosaic chart, graphical representation data allows visualization proportions frequencies categorical variables dataset. ’s type plot provides visual summary contingency table associated two categorical variables. rectangle (tile) mosaic plot represents combination category levels variables, area rectangle proportional frequency proportion observations category combination.Now explore factor gender numeric variable wage.tapply() function R used apply function subsets vector, subsets defined vector, usually factor. basic syntax tapply() :X object split operated .INDEX factor list factors according X split.FUN function applied subset X.… optional arguments FUN.simplify, TRUE, tries simplify result vector, matrix, higher-dimensional array; FALSE, result list.","code":"\nattach(CPS1985) # attach the data set to avoid use the operator $\ntable(gender, occupation) # no name_df$name_var necessary\n#>         occupation\n#> gender   worker techn services office sales mgmt\n#>   male      126    53       34     21    21   34\n#>   female     30    52       49     76    17   21\nprop.table(table(gender, occupation))\n#>         occupation\n#> gender       worker      techn   services     office\n#>   male   0.23595506 0.09925094 0.06367041 0.03932584\n#>   female 0.05617978 0.09737828 0.09176030 0.14232210\n#>         occupation\n#> gender        sales       mgmt\n#>   male   0.03932584 0.06367041\n#>   female 0.03183521 0.03932584\nprop.table(table(gender, occupation), 2) # 1 for row , 2 for columns\n#>         occupation\n#> gender      worker     techn  services    office     sales\n#>   male   0.8076923 0.5047619 0.4096386 0.2164948 0.5526316\n#>   female 0.1923077 0.4952381 0.5903614 0.7835052 0.4473684\n#>         occupation\n#> gender        mgmt\n#>   male   0.6181818\n#>   female 0.3818182\nplot(gender ~ occupation, data = CPS1985)\ntapply(wage, gender, mean)\n#>     male   female \n#> 9.994913 7.878857\ntapply(log(wage), list(gender, occupation), mean)\n#>          worker    techn services   office    sales\n#> male   2.100418 2.446640 1.829568 1.955284 2.141071\n#> female 1.667887 2.307509 1.701674 1.931128 1.579409\n#>            mgmt\n#> male   2.447476\n#> female 2.229256"},{"path":"part-iv-data-manipulation.html","id":"a-factor-and-a-numeric-variable","chapter":"Part IV: Data Manipulation","heading":"A factor and a numeric variable","text":"Explore factor variable numeric variable.\nVisualize distribution wage per genderNow try ","code":"\nboxplot(log(wage) ~ gender, data = CPS1985)\nboxplot(log(wage) ~ gender + occupation, data = CPS1985)\ndetach(CPS1985) # now detach when work is done"},{"path":"part-iv-data-manipulation.html","id":"exercises","chapter":"Part IV: Data Manipulation","heading":"Exercises","text":"Subsetting Data FramesCreate data frame named student_info following columns data:\n- student_id (1 5)\n- student_name (‘Alice’, ‘Bob’, ‘Charlie’, ‘David’, ‘Eva’)\n- student_age (25, 30, 22, 28, 24)\n- student_grade (‘’, ‘B’, ‘’, ‘C’, ‘B’)Write command subset data frame include students older 24.Using Conditional FiltersUse subset() function find students grade ‘’.Display names ages students.Manipulating Data dplyrLoad dplyr package convert student_info tibble.Load dplyr package convert student_info tibble.Use filter() select() show name age students grade better ‘B’.Use filter() select() show name age students grade better ‘B’.Adding Removing ColumnsAdd new column student_major values (‘Math’, ‘Science’, ‘Arts’, ‘Math’, ‘Science’) student_info., remove student_grade column using dplyr.Renaming ColumnsRename student_name column name using base R functions using dplyr.Complex dplyr OperationsCreate new tibble student_info includes students except studying ‘Arts’, rename student_id column id, arrange students age descending order.Exploratory Data Analysis dplyrCalculate average age students grouped major using group_by() summarize() dplyr.","code":""},{"path":"part-v-basic-data-visualization.html","id":"part-v-basic-data-visualization","chapter":"Part V: Basic Data Visualization","heading":"Part V: Basic Data Visualization","text":"","code":""},{"path":"part-v-basic-data-visualization.html","id":"creating-simple-plots-using-plot-hist-piebarplotboxplot","chapter":"Part V: Basic Data Visualization","heading":"Creating simple plots using plot(), hist(), pie(),barplot(),boxplot()","text":"R, plot() function generic function used making variety graphs. simplest, used create scatter plots can customized create line plots, add model lines, much .","code":""},{"path":"part-v-basic-data-visualization.html","id":"basic-arguments","chapter":"Part V: Basic Data Visualization","heading":"Basic Arguments:","text":"x: coordinates points plot. simple scatter plot, typically numeric vector.y: coordinates points plot y-axis. length x.type: type plot drawn. Possible types include “p” points (default), “l” lines, “b” , several others.main: main title plot.xlab: label x-axis.ylab: label y-axis.xlim: Limits x-axis.ylim: Limits y-axis.pch: Plotting character, symbol use plot. Different numbers correspond different symbols.col: Color points. Can also vector color points differently based factor.","code":""},{"path":"part-v-basic-data-visualization.html","id":"additional-customizations","chapter":"Part V: Basic Data Visualization","heading":"Additional Customizations:","text":"cex: numerical value giving amount plotting text symbols magnified relative default.lwd: Line width plot, useful plot type includes lines.bg: Background color open plot symbols specified pch.\nAdvanced Features:abline: function add straight lines plot, either vertical, horizontal, regression lines.lines: function add lines plot, context existing plot; doesn’t start new plot.\npoints: Add points plot.","code":""},{"path":"part-v-basic-data-visualization.html","id":"adding-a-legend","chapter":"Part V: Basic Data Visualization","heading":"Adding a Legend:","text":"add legend, use legend() function. provides number arguments customize appearance:legend: vector text values expression describing text appear legend.\nx, y position: location legend. x y can numeric positions, can use keyword positions like \"topright\", \"bottomleft\", \"bottomright\" , \"bottom\", \"bottomleft\", \"left\", \"topleft\", \"top\", \"right\", \"center\".\npch: plotting symbols points appearing legend, matching plot.\ncol: colors points lines appearing legend, matching plot.\nlwd: line widths lines appearing legend, matching plot.\ncex: Character expansion size legend, determining large text legend .","code":""},{"path":"part-v-basic-data-visualization.html","id":"example-1-simple-scatter-plot","chapter":"Part V: Basic Data Visualization","heading":"Example 1 : Simple scatter plot","text":"","code":"\n\nx <- 1:10\ny <- rnorm(10)\nplot(x, y, main = \"Simple Scatter Plot\", xlab = \"X Axis\", ylab = \"Y Axis\", col = \"blue\")"},{"path":"part-v-basic-data-visualization.html","id":"exercise-1-2","chapter":"Part V: Basic Data Visualization","heading":"Exercise 1","text":"Making Scatter plot:load journals.txt data set save Journals data frameWork following instructionsNow adjust plotting instructionsThe curve() function draws curve corresponding function interval [, ].","code":"\nplot(log(Journals$subs), log(Journals$price))\nrug(log(Journals$subs))\nrug(log(Journals$price), side = 2)\nplot(log(Journals$price) ~ log(Journals$subs), pch = 19,\n     col = \"blue\", xlim = c(0, 7), ylim = c(3, 8),\n     main = \"Library subscriptions\")\nrug(log(Journals$subs))\nrug(log(Journals$price), side=2)\ncurve(dnorm, from = -5, to = 5, col = \"red\", lwd = 3,\n      main = \"Density of the standard normal distribution\")"},{"path":"part-v-basic-data-visualization.html","id":"example-2","chapter":"Part V: Basic Data Visualization","heading":"Example 2","text":"create basic scatter plot, can use mtcars dataset, comes built R. dataset contains various characteristics 32 automobiles.","code":"\n\ndata(mtcars)\n\n\nplot(mtcars$hp, mtcars$mpg, main=\"MPG vs. Horsepower\",\n     xlab=\"Horsepower\", ylab=\"Miles Per Gallon\",\n     pch=19, col=\"blue\")"},{"path":"part-v-basic-data-visualization.html","id":"example-3","chapter":"Part V: Basic Data Visualization","heading":"Example 3","text":"Using pressure dataset, also built R, can create simple line plot. pressure dataset shows temperature resulting vapor pressure mercury.","code":"\n\ndata(pressure)\n\n# Create a line plot\nplot(pressure$temperature, pressure$pressure, type=\"l\",\n     main=\"Vapor Pressure of Mercury\",\n     xlab=\"Temperature\", ylab=\"Pressure\",\n     col=\"red\", lwd=2)"},{"path":"part-v-basic-data-visualization.html","id":"example-4","chapter":"Part V: Basic Data Visualization","heading":"Example 4","text":"plot() generic function create scatter plot.\nmtcars\\(disp mtcars\\)mpg x y coordinates plot, representing engine displacement cubic inches miles per gallon, respectively.col=.factor(mtcars$cyl) specifies colors points plot. cyl variable, represents number cylinders car’s engine, converted factor. levels factor (unique values cyl) automatically given different colors.main main title plot.xlab ylab labels x-axis y-axis, respectively.pch=19 specifies plotting symbol (case, solid circle).cex=1.5 sets size plot symbols; cex stands character expansion factor, 1.5 means 150% default size.legend,legend() adds legend plot.\"topright\" specifies position legend (case, top right corner plotting area).legend= creates text legend pasting word “Cylinders:” front unique value cyl column. indicates color scatter plot corresponds .col= sets colors used legend, match colors used points plot.pch=19 specifies plotting symbols used legend.cex=0.8 sets size symbols legend.","code":"\n# Load the mtcars dataset\ndata(mtcars)\n\n# Plot MPG vs. Displacement, colored by Cylinders\nplot(mtcars$disp, mtcars$mpg, col=as.factor(mtcars$cyl),\n     main=\"Scatter Plot of MPG vs. Displacement\",\n     xlab=\"Displacement (cu.in.)\", ylab=\"MPG\",\n     pch=19, cex=1.5)\n\n# Add a legend to the plot\nlegend(\"topright\", \n       legend=paste(\"Cylinders:\", unique(mtcars$cyl)), \n       col=unique(as.numeric(as.factor(mtcars$cyl))), \n       pch=19, cex=0.8)"},{"path":"part-v-basic-data-visualization.html","id":"example-5","chapter":"Part V: Basic Data Visualization","heading":"Example 5","text":"","code":""},{"path":"part-v-basic-data-visualization.html","id":"scatterplot","chapter":"Part V: Basic Data Visualization","heading":"Scatterplot()","text":"common high level function used produce plots R (rather unsurprisingly) plot() function. example, let’s plot weight petunia plants flowers data frame flower.xlsUse library readxl, read_excel() function.Alternatively , can also use flower.txt use read_tableTo plot scatterplot one numeric variable another numeric variable just need include variables arguments using plot() function. example plot shootarea y axis weight x axis.equivalent approach types plots often causes confusion first. can also use formula notation using plot() function. However, contrast previous method formula method requires specify y axis variable first, ~x axis variable.","code":"\n\nlibrary(readxl)\nflowers <- read_excel('./John Jay Workshop Data/flower.xls')\n\nplot(flowers$weight)\nflowers <- read.table(file = './John Jay Workshop Data/flower.txt', \n                        header = TRUE, sep = \"\\t\", \n                        stringsAsFactors = TRUE)\nplot(x = flowers$weight, y = flowers$shootarea)\nplot(flowers$shootarea ~ flowers$weight)"},{"path":"part-v-basic-data-visualization.html","id":"histogram","chapter":"Part V: Basic Data Visualization","heading":"Histogram","text":"can also display histogram proportion rather frequency using freq = FALSE argument.alternative plotting just straight histogram add kernel density curve plot. can superimpose density curve onto histogram first using density() function compute kernel density estimates use low level function lines() add estimates onto plot line.","code":"\nhist(flowers$height)\nbrk <- seq(from = 0, to = 18, by = 1)\nhist(flowers$height, breaks = brk, main = \"petunia height\")\nbrk <- seq(from = 0, to = 18, by = 1)\nhist(flowers$height, breaks = brk, main = \"petunia height\",\n      freq = FALSE)\ndens <- density(flowers$height)\nhist(flowers$height, breaks = brk, main = \"petunia height\",\n      freq = FALSE)\nlines(dens)"},{"path":"part-v-basic-data-visualization.html","id":"boxplot","chapter":"Part V: Basic Data Visualization","heading":"Boxplot","text":"OK, ’ll just come say , love boxplots close relation violin plot. Boxplots (box--whisker plots give full name) useful want graphically summarise distribution variable, identify potential unusual values compare distributions different groups. reason love ease interpretation, transparency relatively high data--ink ratio (.e. convey lots information efficiently). suggest try use boxplots much possible exploring data avoid temptation use ubiquitous bar plot (even standard error 95% confidence intervals bars). problem bar plots (aka dynamite plots) hide important information reader distribution data assume error bars (confidence intervals) symmetric around mean. course, ’s ’re tempted use bar plots just Google ‘dynamite plots evil’ see hereTo create boxplot R use boxplot() function. example, let’s create boxplot variable weight flowers data frame. can also include y axis label using ylab = argument.want examine distribution variable changes different levels factor need use formula notation boxplot() function. example, let’s plot weight variable , time see changes level nitrogen. use formula notation boxplot() can use data = argument save typing. ’ll also introduce x axis label using xlab = argument.factor levels plotted order defined factor variable nitrogen (often alphabetically). change order need change order levels nitrogen factor data frame using factor() function re-plot graph. Let’s plot boxplot factor levels going low high.can also group variables two factors plot. Let’s plot weight variable time plot separate box nitrogen treatment (treat) combination.plot looks OK, group labels hidden ’re long fit plot. couple ways deal . Perhaps easiest reduce font size tick mark labels plot fit using cex.axis = argument. Let’s set font size 30% smaller default cex.axis = 0.7Violin plots like combination boxplot kernel density plot (saw example kernel density plot histogram section ) rolled one figure. can create violin plot R using vioplot() function vioplot package. ’ll need first install package using install.packages(‘vioplot’) function usual. nice thing vioplot() function use pretty much way use boxplot() function. ’ll also use argument col = “lightblue” change fill colour light blue.","code":"\nboxplot(flowers$weight, ylab = \"weight (g)\")\nboxplot(weight ~ nitrogen, data = flowers, \n         ylab = \"weight (g)\", xlab = \"nitrogen level\")\nflowers$nitrogen <- factor(flowers$nitrogen, \n                            levels = c(\"low\", \"medium\", \"high\"))\nboxplot(weight ~ nitrogen, data = flowers, \n          ylab = \"weight (g)\", xlab = \"nitrogen level\")\nboxplot(weight ~ nitrogen * treat, data = flowers, \n         ylab = \"weight (g)\", xlab = \"nitrogen level\")\nboxplot(weight ~ nitrogen * treat, data = flowers, \n         ylab = \"weight (g)\", xlab = \"nitrogen level\", \n         cex.axis = 0.7)\n#install.packages(\"vioplot\")\nlibrary(vioplot)\n#> Loading required package: sm\n#> Package 'sm', version 2.2-6.0: type help(sm) for summary information\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\nvioplot(weight ~ nitrogen, data = flowers, \n         ylab = \"weight (g)\", xlab = \"nitrogen level\",\n         col = \"lightblue\")"},{"path":"part-v-basic-data-visualization.html","id":"pairs-plot","chapter":"Part V: Basic Data Visualization","heading":"Pairs plot","text":"","code":"\nplot(flowers)\npairs(flowers[, c(\"height\", \"weight\", \"leafarea\", \n                \"shootarea\", \"flowers\")])"},{"path":"part-v-basic-data-visualization.html","id":"exercise-2-2","chapter":"Part V: Basic Data Visualization","heading":"Exercise 2","text":"Now, try creating visualization using iris dataset. ’s can :Create scatter plot using Petal.Length Petal.Width iris dataset.Color points based Species column differentiate species.Add title, x-axis label, y-axis label plot.\nInclude legend indicates color corresponds iris species.","code":""},{"path":"intermediate-r.html","id":"intermediate-r","chapter":"Intermediate R","heading":"Intermediate R","text":"session , learn function control structures writing functions , statements, loops.also explore Data Cleaning Transformations handling missing data , reshaping data using dplyr functions.First , need load packageIt loadggplot2: Used data visualization using grammar graphics.dplyr: Provides set tools efficiently manipulating datasets.tidyr: Used tidying data, , transforming format easy work .readr: Used read rectangular data like CSVs text files R.purrr: Enhances R’s functional programming (FP) toolkit, making easier work lists functions.tibble: modern reimagining data frames, providing cleaner user-friendly data structure.stringr: Simplifies process working strings (text data).forcats: Designed handle categorical variables (factors) ease.session , use ","code":"\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"part-i-functions-and-control-structures.html","id":"part-i-functions-and-control-structures","chapter":"Part I: Functions and Control Structures","heading":"Part I: Functions and Control Structures","text":"","code":""},{"path":"part-i-functions-and-control-structures.html","id":"writing-and-using-functions","chapter":"Part I: Functions and Control Structures","heading":"Writing and using functions","text":"Example: simple function calculate square number\\[\nf(x) = x^2\n\\]","code":"\n\nsquare_function <- function(x) {\n  return(x^2)\n}\n\n# Using the function\nresult <- square_function(4)\nprint(result)\n#> [1] 16"},{"path":"part-i-functions-and-control-structures.html","id":"exercise-1-3","chapter":"Part I: Functions and Control Structures","heading":"Exercise 1","text":"Task: Write Use Function\nObjective: Create function calculates cube number use function calculate cube 3.Hint: Use structure square_function template.reason useful, functions used anything want, R functions just similar one just created, optimized specific tasks designed .now create complex function , one takes vector , finds mean , standard deviation histogram","code":"\nanalyze_vector <- function(x, plot_title = \"Histogram\") {\n  # Check if the input is numeric\n  if (!is.numeric(x)) {\n    stop(\"Input must be a numeric vector\")\n  }\n  \n  # Calculate mean and standard deviation\n  mean_value <- mean(x)\n  std_value <- sd(x)\n  \n  # Output the mean and std\n  cat(\"Mean:\", mean_value, \"\\n\")\n  cat(\"Standard Deviation:\", std_value, \"\\n\")\n  \n  # Create a histogram\n  hist(x, main = plot_title, xlab = \"Values\", col = \"lightblue\", border = \"black\")\n  \n  # Return a list containing the mean and std\n  return(list(mean = mean_value, std = std_value))\n}\n\n# Example usage with the mtcars$mpg vector\nresult <- analyze_vector(mtcars$mpg, \"MPG Histogram\")\n#> Mean: 20.09062 \n#> Standard Deviation: 6.026948"},{"path":"part-i-functions-and-control-structures.html","id":"exercise-2-3","chapter":"Part I: Functions and Control Structures","heading":"Exercise 2","text":"Task: Analyze Numeric Vector\nObjective: Write function named summarize_vector takes numeric vector input calculates median, variance, creates boxplot. function print median variance, return list. Use airquality$Ozone data analysis.Hint: Similar analyze_vector, check input numeric use median, var, boxplot functions.","code":""},{"path":"part-i-functions-and-control-structures.html","id":"if-statements-and-loops-for-and-while","chapter":"Part I: Functions and Control Structures","heading":"If statements and loops (for and while)","text":"","code":"\n# Example: Using if statement\nnumber <- 5\nif (number > 0) {\n  print(\"Positive number\")\n} else {\n  print(\"Non-positive number\")\n}\n#> [1] \"Positive number\""},{"path":"part-i-functions-and-control-structures.html","id":"exercise-3-1","chapter":"Part I: Functions and Control Structures","heading":"Exercise 3","text":"Task: Using Statements\nObjective: Create R script checks number negative, zero, positive prints appropriate message. Test script number -4.Hint: Use statement followed else else.","code":""},{"path":"part-i-functions-and-control-structures.html","id":"example-6","chapter":"Part I: Functions and Control Structures","heading":"Example:","text":"loop calculate factorial number","code":"\n\nfactorial_function <- function(n) {\n  result <- 1\n  for (i in 1:n) {\n    result <- result * i\n  }\n  return(result)\n}\n\nfactorial_of_5 <- factorial_function(5)\nprint(factorial_of_5)\n#> [1] 120"},{"path":"part-i-functions-and-control-structures.html","id":"exercise-4-1","chapter":"Part I: Functions and Control Structures","heading":"Exercise 4","text":"Task: Loop\nObjective: Write function using loop calculates sum squares numbers 1 n. Use function calculate sum squares n=10.Hint: Iterate 1 n, keep adding square number result variable.","code":""},{"path":"part-i-functions-and-control-structures.html","id":"example-7","chapter":"Part I: Functions and Control Structures","heading":"Example:","text":"loop find first square number greater 100","code":"\nnumber <- 1\nwhile (number^2 <= 100) {\n  number <- number + 1\n}\nprint(paste(\"First square number greater than 100 is:\", number^2))\n#> [1] \"First square number greater than 100 is: 121\""},{"path":"part-i-functions-and-control-structures.html","id":"exercise-5-1","chapter":"Part I: Functions and Control Structures","heading":"Exercise 5","text":"Task: Loop\nObjective: Write script using loop finds smallest number whose cube greater 100. Print number cube.Hint: Increment number starting 1, check cube greater 100 loop condition.","code":""},{"path":"part-ii-data-wrangling.html","id":"part-ii-data-wrangling","chapter":"Part II: Data Wrangling","heading":"Part II: Data Wrangling","text":"Data wrangling, also known data munging, process \ntransforming mapping data one “raw” form another format\nintent making appropriate valuable variety\ndownstream purposes, analytics.R, data wrangling often performed using functions base R\nlanguage, well collection packages known tidyverse.\ntidyverse coherent system packages data manipulation,\nexploration, visualization share common design philosophy.tidyverse approach data wrangling typically involves:Tidying Data: Transforming datasets consistent form \nmakes easier work . usually means converting data \ntidy format variable forms column, observation\nforms row, type observational unit forms table.Tidying Data: Transforming datasets consistent form \nmakes easier work . usually means converting data \ntidy format variable forms column, observation\nforms row, type observational unit forms table.Transforming Data: data tidy, series functions \nused data manipulation tasks selecting specific columns\n(select()), filtering certain rows (filter()), creating new\ncolumns modifying existing ones (mutate() transmute()),\nsummarizing data (summarise()), reshaping data (pivot_longer()\npivot_wider()).Transforming Data: data tidy, series functions \nused data manipulation tasks selecting specific columns\n(select()), filtering certain rows (filter()), creating new\ncolumns modifying existing ones (mutate() transmute()),\nsummarizing data (summarise()), reshaping data (pivot_longer()\npivot_wider()).Working Data Types Structures: Functions tidyverse\nallow easy manipulation data types (like converting\ncharacter vectors factors forcats) data structures (like\ntibbles tibble package, modern take data\nframes).Working Data Types Structures: Functions tidyverse\nallow easy manipulation data types (like converting\ncharacter vectors factors forcats) data structures (like\ntibbles tibble package, modern take data\nframes).Joining Data: Combining different datasets variety ways\n(like left_join(), right_join(), inner_join(), full_join(), \nanti_join()) based common keys identifiers.Joining Data: Combining different datasets variety ways\n(like left_join(), right_join(), inner_join(), full_join(), \nanti_join()) based common keys identifiers.Handling Strings Dates: tidyverse includes packages like\nstringr string operations lubridate dealing \ndate-time objects, essential many data wrangling tasks.Handling Strings Dates: tidyverse includes packages like\nstringr string operations lubridate dealing \ndate-time objects, essential many data wrangling tasks.Functional Programming: package purrr introduces powerful\nfunctional programming tools iterate data structures \nperform operations repeatedly.Functional Programming: package purrr introduces powerful\nfunctional programming tools iterate data structures \nperform operations repeatedly.primary goal data wrangling ensure data \nbest possible format analysis. tidyverse provides tools \nmake tasks straightforward, efficient, often intuitive\nbase R equivalents. philosophy tidyverse write\nreadable transparent code can understood even come\nback months years later.","code":""},{"path":"part-ii-data-wrangling.html","id":"reshaping-data-using-dplyr-functions-filter-arrange-mutate-summarize","chapter":"Part II: Data Wrangling","heading":"Reshaping data using dplyr functions (filter, arrange, mutate, summarize)","text":"dplyr package developed Hadley Wickham RStudio \noptimized distilled version plyr package. dplyr package\nprovide “new” functionality R per se, sense \neverything dplyr already done base R, greatly\nsimplifies existing functionality R.One important contribution dplyr package provides \n“grammar” (particular, verbs) data manipulation operating\ndata frames. grammar, can sensibly communicate \ndata frame people can understand\n(assuming also know grammar). useful \nprovides abstraction data manipulation previously \nexist. Another useful contribution dplyr functions \nfast, many key operations coded C++.dplyr grammarSome key “verbs” provided dplyr package areselect: return subset columns data frame, using \nflexible notationselect: return subset columns data frame, using \nflexible notationfilter: extract subset rows data frame based logical\nconditionsfilter: extract subset rows data frame based logical\nconditionsarrange: reorder rows data framearrange: reorder rows data framerename: rename variables data framerename: rename variables data framemutate: add new variables/columns transform existing variablesmutate: add new variables/columns transform existing variablessummarise / summarize: generate summary statistics different\nvariables data frame, possibly within stratasummarise / summarize: generate summary statistics different\nvariables data frame, possibly within strata%>%: “pipe” operator used connect multiple verb actions\ntogether pipeline.%>%: “pipe” operator used connect multiple verb actions\ntogether pipeline.combine naturally group_by() allows perform\noperation “group”.","code":""},{"path":"part-ii-data-wrangling.html","id":"more-on-the-pipe-operator","chapter":"Part II: Data Wrangling","heading":"More on the pipe operator","text":"takes output one statement makes input \nnext statement.describing , can think “”. first\nexample:\ntake diamonds data (ggplot2 package)\nsubset\ntake diamonds data (ggplot2 package)subset","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(ggplot2)\ndiamonds %>% filter(cut == \"Ideal\")\n#> # A tibble: 21,551 × 10\n#>    carat cut   color clarity depth table price     x     y\n#>    <dbl> <ord> <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#>  1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98\n#>  2  0.23 Ideal J     VS1      62.8    56   340  3.93  3.9 \n#>  3  0.31 Ideal J     SI2      62.2    54   344  4.35  4.37\n#>  4  0.3  Ideal I     SI2      62      54   348  4.31  4.34\n#>  5  0.33 Ideal I     SI2      61.8    55   403  4.49  4.51\n#>  6  0.33 Ideal I     SI2      61.2    56   403  4.49  4.5 \n#>  7  0.33 Ideal J     SI1      61.1    56   403  4.49  4.55\n#>  8  0.23 Ideal G     VS1      61.9    54   404  3.93  3.95\n#>  9  0.32 Ideal I     SI1      60.9    55   404  4.45  4.48\n#> 10  0.3  Ideal I     SI2      61      59   405  4.3   4.33\n#> # ℹ 21,541 more rows\n#> # ℹ 1 more variable: z <dbl>"},{"path":"part-ii-data-wrangling.html","id":"filter","chapter":"Part II: Data Wrangling","heading":"Filter()","text":"Extract rows meet logical criteria. go: - inspect \ndiamonds data set - filter observations cut equal Ideal","code":"\nfilter(diamonds, cut == \"Ideal\")\n#> # A tibble: 21,551 × 10\n#>    carat cut   color clarity depth table price     x     y\n#>    <dbl> <ord> <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#>  1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98\n#>  2  0.23 Ideal J     VS1      62.8    56   340  3.93  3.9 \n#>  3  0.31 Ideal J     SI2      62.2    54   344  4.35  4.37\n#>  4  0.3  Ideal I     SI2      62      54   348  4.31  4.34\n#>  5  0.33 Ideal I     SI2      61.8    55   403  4.49  4.51\n#>  6  0.33 Ideal I     SI2      61.2    56   403  4.49  4.5 \n#>  7  0.33 Ideal J     SI1      61.1    56   403  4.49  4.55\n#>  8  0.23 Ideal G     VS1      61.9    54   404  3.93  3.95\n#>  9  0.32 Ideal I     SI1      60.9    55   404  4.45  4.48\n#> 10  0.3  Ideal I     SI2      61      59   405  4.3   4.33\n#> # ℹ 21,541 more rows\n#> # ℹ 1 more variable: z <dbl>"},{"path":"part-ii-data-wrangling.html","id":"overview-of-logical-tests","chapter":"Part II: Data Wrangling","heading":"Overview of logical tests","text":"","code":""},{"path":"part-ii-data-wrangling.html","id":"mutate","chapter":"Part II: Data Wrangling","heading":"Mutate()","text":"Create new columns. go: - inspect diamonds data set -\ncreate new variable price_per_carat","code":"\nmutate(diamonds, price_per_carat = price/carat)\n#> # A tibble: 53,940 × 11\n#>    carat cut     color clarity depth table price     x     y\n#>    <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#>  1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98\n#>  2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84\n#>  3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07\n#>  4  0.29 Premium I     VS2      62.4    58   334  4.2   4.23\n#>  5  0.31 Good    J     SI2      63.3    58   335  4.34  4.35\n#>  6  0.24 Very G… J     VVS2     62.8    57   336  3.94  3.96\n#>  7  0.24 Very G… I     VVS1     62.3    57   336  3.95  3.98\n#>  8  0.26 Very G… H     SI1      61.9    55   337  4.07  4.11\n#>  9  0.22 Fair    E     VS2      65.1    61   337  3.87  3.78\n#> 10  0.23 Very G… H     VS1      59.4    61   338  4     4.05\n#> # ℹ 53,930 more rows\n#> # ℹ 2 more variables: z <dbl>, price_per_carat <dbl>"},{"path":"part-ii-data-wrangling.html","id":"multistep-operations","chapter":"Part II: Data Wrangling","heading":"Multistep operations","text":"Use %>% multistep operations. Passes result left first\nargument function right. go:","code":"\ndiamonds %>% \n  mutate(price_per_carat = price/carat)  %>%\n  filter(price_per_carat > 1500)\n#> # A tibble: 52,821 × 11\n#>    carat cut     color clarity depth table price     x     y\n#>    <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#>  1  0.21 Premium E     SI1      59.8    61   326  3.89  3.84\n#>  2  0.22 Fair    E     VS2      65.1    61   337  3.87  3.78\n#>  3  0.22 Premium F     SI1      60.4    61   342  3.88  3.84\n#>  4  0.2  Premium E     SI2      60.2    62   345  3.79  3.75\n#>  5  0.23 Very G… E     VS2      63.8    55   352  3.85  3.92\n#>  6  0.23 Very G… H     VS1      61      57   353  3.94  3.96\n#>  7  0.23 Very G… G     VVS2     60.4    58   354  3.97  4.01\n#>  8  0.23 Very G… D     VS2      60.5    61   357  3.96  3.97\n#>  9  0.23 Very G… F     VS1      60.9    57   357  3.96  3.99\n#> 10  0.23 Very G… F     VS1      60      57   402  4     4.03\n#> # ℹ 52,811 more rows\n#> # ℹ 2 more variables: z <dbl>, price_per_carat <dbl>"},{"path":"part-ii-data-wrangling.html","id":"summarize","chapter":"Part II: Data Wrangling","heading":"Summarize()","text":"Compute table summaries. go:inspect diamonds data setcalculate mean standard deviation price","code":"\ndiamonds %>% summarize(mean = mean(price), std_dev = sd(price))\n#> # A tibble: 1 × 2\n#>    mean std_dev\n#>   <dbl>   <dbl>\n#> 1 3933.   3989."},{"path":"part-ii-data-wrangling.html","id":"group_by","chapter":"Part II: Data Wrangling","heading":"Group_by()","text":"Groups cases common values one columns. go:\ninspect diamonds data set calculate mean standard deviation \nprice level cut","code":"\ndiamonds %>% \n        group_by(cut) %>% \n        summarize(price = mean(price), carat = mean(carat))\n#> # A tibble: 5 × 3\n#>   cut       price carat\n#>   <ord>     <dbl> <dbl>\n#> 1 Fair      4359. 1.05 \n#> 2 Good      3929. 0.849\n#> 3 Very Good 3982. 0.806\n#> 4 Premium   4584. 0.892\n#> 5 Ideal     3458. 0.703"},{"path":"part-ii-data-wrangling.html","id":"exercise-1-4","chapter":"Part II: Data Wrangling","heading":"Exercise 1","text":"Load data Parade2005.txt.Determine mean earnings California.Determine number individuals residing Idaho.Determine mean median earnings celebrities.","code":""},{"path":"part-ii-data-wrangling.html","id":"transforming-a-dataframe-into-tibbles","chapter":"Part II: Data Wrangling","heading":"Transforming a dataframe into tibbles","text":"Transform mtcars tibble inspect.","code":"\nstr(mtcars)\n#> 'data.frame':    32 obs. of  11 variables:\n#>  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n#>  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#>  $ disp: num  160 160 108 258 360 ...\n#>  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n#>  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n#>  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n#>  $ qsec: num  16.5 17 18.6 19.4 17 ...\n#>  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#>  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n#>  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n#>  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n#library(tidyverse)\nlibrary(tibble)\nas_tibble(mtcars)\n#> # A tibble: 32 × 11\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0\n#> # ℹ 22 more rows\n#> # ℹ 2 more variables: gear <dbl>, carb <dbl>"},{"path":"part-ii-data-wrangling.html","id":"part-ii-data-cleaning-and-transformation","chapter":"Part II: Data Wrangling","heading":"Part II : Data Cleaning and Transformation","text":"Data cleaning fundamental step data analysis process, aimed\nimproving data quality ensuring appropriateness specific\nanalytical tasks. process involves identifying rectifying errors\ninconsistencies data enhance accuracy, completeness, \nreliability.Key aspects data cleaning include:Removing Duplicates: involves detecting eliminating\nduplicate records skew analysis results.Removing Duplicates: involves detecting eliminating\nduplicate records skew analysis results.Handling Missing Data: Missing values can dealt \nimputing data (filling missing values using statistical methods\ndomain knowledge), cases, deleting rows columns\nmany missing values.Handling Missing Data: Missing values can dealt \nimputing data (filling missing values using statistical methods\ndomain knowledge), cases, deleting rows columns\nmany missing values.Correcting Errors: involves identifying outliers \nincorrect entries (due data entry errors, measurement errors,\netc.) correcting based context predefined rules.Correcting Errors: involves identifying outliers \nincorrect entries (due data entry errors, measurement errors,\netc.) correcting based context predefined rules.Standardizing Formats: Ensuring data across different\nsources fields conforms consistent format, \nconverting dates format, standardizing text entries\n(capitalization, removing leading/trailing spaces), ensuring\nconsistent measurement units.Standardizing Formats: Ensuring data across different\nsources fields conforms consistent format, \nconverting dates format, standardizing text entries\n(capitalization, removing leading/trailing spaces), ensuring\nconsistent measurement units.Filtering Irrelevant Information: Removing data \nrelevant specific analysis task focus significant\ndata.Filtering Irrelevant Information: Removing data \nrelevant specific analysis task focus significant\ndata.Validating Accuracy: Checking data known standards \nvalidation rules ensure correctly represents real-world\nconstructs supposed reflect.Validating Accuracy: Checking data known standards \nvalidation rules ensure correctly represents real-world\nconstructs supposed reflect.Consolidating Data Sources: Combining data multiple sources\nensuring combined dataset coherent correctly\nintegrated.Consolidating Data Sources: Combining data multiple sources\nensuring combined dataset coherent correctly\nintegrated.aim data cleaning correct errors also bring\nstructure order data, facilitating effective \naccurate analysis. cleaning data, analysts can ensure \ninsights conclusions based reliable valid data, \ncrucial making informed decisions.","code":""},{"path":"part-ii-data-wrangling.html","id":"the-policy-data-set","chapter":"Part II: Data Wrangling","heading":"The Policy data set","text":"PolicyData.csv available course materialData stored .csv file.Individual records separated semicolon.","code":"\npolicy_data <- read.csv(file = './John Jay Workshop Data/PolicyData.csv', sep = ';')"},{"path":"part-ii-data-wrangling.html","id":"exercise-1-5","chapter":"Part II: Data Wrangling","heading":"Exercise 1","text":"Use skills obtained first R workshop Part 1Inspect top rows data set.many observations data set contain?Calculate total exposure (exposition) region\n(type_territoire).","code":""},{"path":"part-ii-data-wrangling.html","id":"the-gapminder-package","chapter":"Part II: Data Wrangling","heading":"The Gapminder package","text":"Describes evolution number population characteristics\n(GDP, life expectancy, …) time.","code":"\n#install.packages(\"gapminder\")\nlibrary(gapminder)"},{"path":"part-ii-data-wrangling.html","id":"exercise-2-4","chapter":"Part II: Data Wrangling","heading":"Exercise 2","text":"Use skills obtained Part :Inspect top rows data.Select data countries Asia.type variable country?","code":""},{"path":"part-ii-data-wrangling.html","id":"revisit-factor","chapter":"Part II: Data Wrangling","heading":"Revisit factor()","text":"","code":""},{"path":"part-ii-data-wrangling.html","id":"what-is-a-factor-variable","chapter":"Part II: Data Wrangling","heading":"What is a factor variable ?","text":"Representation categorical data.Predefined list outcomes (levels).Protecting data quality.Example , sex categorical value two possible outcomes, m \nfThe factor command creates new factor variable. first input\ncategorical variable.factor command creates new factor variable. first input\ncategorical variable.levels specifies possible outcomes variable.levels specifies possible outcomes variable.Assigning unrecognized level factor variable results \nwarningThis protects quality dataThe value NA assigned invalid observation.","code":"\nsex <- factor(c('m', 'f', 'm', 'f'),\n              levels = c('m', 'f'))\nsex\n#> [1] m f m f\n#> Levels: m f\nsex[1] <- 'male'\n#> Warning in `[<-.factor`(`*tmp*`, 1, value = \"male\"):\n#> invalid factor level, NA generated\nsex\n#> [1] <NA> f    m    f   \n#> Levels: m f"},{"path":"part-ii-data-wrangling.html","id":"levels","chapter":"Part II: Data Wrangling","heading":"levels()","text":"levels print allowed outcomes factor variableAssigning vector levels() renames allowed outcomes.","code":"\nlevels(sex)\n#> [1] \"m\" \"f\"\nlevels(sex) <- c('male', 'female')\nsex\n#> [1] <NA>   female male   female\n#> Levels: male female"},{"path":"part-ii-data-wrangling.html","id":"exercise-4-2","chapter":"Part II: Data Wrangling","heading":"Exercise 4","text":"variable country gapminder data set factor variable.possible levels country subset asia.result expected?add level","code":"\nlevels(sex) <- c(levels(sex), 'x')"},{"path":"part-ii-data-wrangling.html","id":"cut","chapter":"Part II: Data Wrangling","heading":"cut()","text":"","code":"\ngapminder\n#> # A tibble: 1,704 × 6\n#>    country     continent  year lifeExp      pop gdpPercap\n#>    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#>  1 Afghanistan Asia       1952    28.8  8425333      779.\n#>  2 Afghanistan Asia       1957    30.3  9240934      821.\n#>  3 Afghanistan Asia       1962    32.0 10267083      853.\n#>  4 Afghanistan Asia       1967    34.0 11537966      836.\n#>  5 Afghanistan Asia       1972    36.1 13079460      740.\n#>  6 Afghanistan Asia       1977    38.4 14880372      786.\n#>  7 Afghanistan Asia       1982    39.9 12881816      978.\n#>  8 Afghanistan Asia       1987    40.8 13867957      852.\n#>  9 Afghanistan Asia       1992    41.7 16317921      649.\n#> 10 Afghanistan Asia       1997    41.8 22227415      635.\n#> # ℹ 1,694 more rows\nhead(cut(gapminder$pop,\n    breaks = c(0, 10^7, 5*10^7, 10^8, Inf)))\n#> [1] (0,1e+07]     (0,1e+07]     (1e+07,5e+07] (1e+07,5e+07]\n#> [5] (1e+07,5e+07] (1e+07,5e+07]\n#> 4 Levels: (0,1e+07] (1e+07,5e+07] ... (1e+08,Inf]\ngapminder$pop_category = cut(gapminder$pop,\n                             breaks = c(0, 10^7, 5*10^7, 10^8, Inf),\n                             labels = c(\"<= 10M\", \"10M-50M\", \"50M-100M\", \"> 100M\"))\ngapminder\n#> # A tibble: 1,704 × 7\n#>    country     continent  year lifeExp      pop gdpPercap\n#>    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#>  1 Afghanistan Asia       1952    28.8  8425333      779.\n#>  2 Afghanistan Asia       1957    30.3  9240934      821.\n#>  3 Afghanistan Asia       1962    32.0 10267083      853.\n#>  4 Afghanistan Asia       1967    34.0 11537966      836.\n#>  5 Afghanistan Asia       1972    36.1 13079460      740.\n#>  6 Afghanistan Asia       1977    38.4 14880372      786.\n#>  7 Afghanistan Asia       1982    39.9 12881816      978.\n#>  8 Afghanistan Asia       1987    40.8 13867957      852.\n#>  9 Afghanistan Asia       1992    41.7 16317921      649.\n#> 10 Afghanistan Asia       1997    41.8 22227415      635.\n#> # ℹ 1,694 more rows\n#> # ℹ 1 more variable: pop_category <fct>"},{"path":"part-ii-data-wrangling.html","id":"exercise-5-2","chapter":"Part II: Data Wrangling","heading":"Exercise 5","text":"Bin life expectancy 2007 factor variable.Select observations year 2007.Select observations year 2007.Bin life expectancy four bins \nroughly equal size (hint: quantile).Bin life expectancy four bins \nroughly equal size (hint: quantile).many observations \nbin?many observations \nbin?","code":""},{"path":"part-ii-data-wrangling.html","id":"handling-missing-data","chapter":"Part II: Data Wrangling","heading":"Handling missing data","text":"","code":""},{"path":"part-ii-data-wrangling.html","id":"some-history","chapter":"Part II: Data Wrangling","heading":"Some history","text":"practice imputing missing values evolved significantly \nyears statisticians data scientists sought deal \nunavoidable problem incomplete data. history imputation\nreflects broader trends statistical methods computational\ncapabilities, well growing awareness impacts different\nimputation strategies integrity statistical analysis.","code":""},{"path":"part-ii-data-wrangling.html","id":"missing-data-mechanisms","chapter":"Part II: Data Wrangling","heading":"Missing Data Mechanisms","text":"Rubin (1976) classified missing data 3 categories: - Missing\nCompletely Random (MCAR) - Missing Random (MAR) - Missing \nRandom (NMAR), also called Missing Random (MNAR) - Aka \nconfusing statistical terms ever invented","code":""},{"path":"part-ii-data-wrangling.html","id":"early-approaches-and-simple-imputation","chapter":"Part II: Data Wrangling","heading":"Early Approaches and Simple Imputation","text":"Early approaches handling missing data often quite simple,\nincluding methods like listwise deletion (removing record \nmissing value) pairwise deletion (excluding missing values \ncase--case basis analysis). methods, \nstraightforward, can lead biased results reduced statistical\npower missingness completely random.Simple imputation techniques, filling missing values \nmean, median, mode variable, developed way retain\nmuch data possible. methods easy understand \nimplement, contributed widespread use, especially \nera advanced computational methods became widely accessible.","code":""},{"path":"part-ii-data-wrangling.html","id":"limitations-of-mean-and-median-imputation","chapter":"Part II: Data Wrangling","heading":"Limitations of Mean and Median Imputation","text":"Imputing missing values mean median intuitive can \neffective certain contexts, methods significant\nlimitations:Bias Estimation: Mean median imputation account \ninherent uncertainty associated missing data. can lead \nunderestimation variances covariances \nartificially reduce variability imputed variable.Distortion Data Distribution: methods can distort \noriginal distribution data, especially missingness \nrandom (Missing Random - MNAR) proportion missing\ndata high. distortion can affect subsequent analyses, \nregression models, providing misleading results.Ignores Relationships Variables: Mean median imputation\ntreat variable isolation, ignoring potential relationships\nvariables. can particularly problematic multivariate\ndatasets variables may correlated.","code":""},{"path":"part-ii-data-wrangling.html","id":"modern-imputation-techniques","chapter":"Part II: Data Wrangling","heading":"Modern Imputation Techniques","text":"awareness limitations simple imputation methods grew,\nresearchers developed sophisticated techniques designed address\nshortcomings:Multiple Imputation: Developed late 20th century, multiple\nimputation involves creating several imputed datasets drawing \ndistribution reflects uncertainty around true values \nmissing data. datasets analyzed separately, \nresults combined produce estimates account \nuncertainty due missingness. method addresses issue \nunderestimating variability provides reliable statistical\ninferences.Model-Based Imputation: Techniques like Expectation-Maximization\n(EM) algorithms imputation using random forests machine\nlearning models take account relationships variables \ndataset. methods can accurately reflect complex\nstructures data produce imputations preserve statistical\nrelationships.ConclusionThe evolution imputation methods simple mean median filling\nsophisticated model-based multiple imputation techniques reflects\nbroader shift statistical practice. shift characterized \nincreased computational power, complex datasets, deeper\nunderstanding impact missing data statistical inference.\nmean median imputation can still useful specific,\nwell-considered circumstances, modern techniques offer robust \nprincipled approaches handling missing data.","code":""},{"path":"part-ii-data-wrangling.html","id":"missing-values-in-r","chapter":"Part II: Data Wrangling","heading":"Missing Values in R","text":"Missing values denoted NA NaN q undefined mathematical\noperations..na() used test objects NAis.nan() used test NaNNA values class also, integer NA, character NA,\netc.NaN value also NA converse true","code":""},{"path":"part-ii-data-wrangling.html","id":"difference-between-na-and-nan-in-r","chapter":"Part II: Data Wrangling","heading":"0.1.1 Difference Between NA and NaN in R","text":"R, NA NaN represent two different kinds missing \nundefined values, used distinct contexts:","code":""},{"path":"part-ii-data-wrangling.html","id":"na-not-available","chapter":"Part II: Data Wrangling","heading":"0.1.1.1 NA (Not Available)","text":"NA stands Available.used represent missing undefined data, typically \ncases data expected present.NA can used logical statistical operation, unless\nhandled specifically, operations involving NA generally\nresult NA.NA flexible context can used data type\nR, numeric, character, logical.can test NA using .na() function.","code":""},{"path":"part-ii-data-wrangling.html","id":"nan-not-a-number","chapter":"Part II: Data Wrangling","heading":"0.1.1.2 NaN (Not a Number)","text":"NaN stands Number.special value used represent undefined \nunrepresentable numerical results, result 0/0.NaN specific type NA specifically numeric\ncalculations result undefined indeterminate values.Operations result NaN typically \nmathematically indeterminate outside domain mathematical\nfunctions (e.g., square root negative number realm \nreal numbers).can test NaN using .nan() function. Note \n.na() also returns TRUE NaN values, reflecting \nstatus kind missing value, .nan() return\nTRUE NA values.","code":""},{"path":"part-ii-data-wrangling.html","id":"key-differences","chapter":"Part II: Data Wrangling","heading":"Key Differences","text":"Context Use: NA used broadly missing data\nacross data types, NaN specific numerical\noperations produce defined, real number.Nature Undefinedness: NA indicates absence data,\nwhereas NaN indicates calculation failed produce \nmeaningful result.summary, use NA vs. NaN helps distinguish data\nmissing (NA) numerical operations result undefined\nunrepresentable values (NaN).","code":"\ncoffee_data <- data.frame(\n  Age = c(25, 32, NA, 45, 22, 33, NA, 28),\n  Gender = c(\"Female\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female\", NA),\n  Cups_Per_Day = c(1, 3, 2, NA, 2, 3, 1, 2)\n)\ncoffee_data\n#>   Age Gender Cups_Per_Day\n#> 1  25 Female            1\n#> 2  32   Male            3\n#> 3  NA   Male            2\n#> 4  45 Female           NA\n#> 5  22 Female            2\n#> 6  33   Male            3\n#> 7  NA Female            1\n#> 8  28   <NA>            2"},{"path":"part-ii-data-wrangling.html","id":"identifying-missing-values","chapter":"Part II: Data Wrangling","heading":"Identifying Missing Values","text":"can use .na() function check missing values. count\nspecific column:","code":"\nsum(is.na(coffee_data$Age))\n#> [1] 2"},{"path":"part-ii-data-wrangling.html","id":"removing-na-values","chapter":"Part II: Data Wrangling","heading":"Removing NA Values","text":"common task data analysis removing missing values (NAs).can remove byA faster way ,","code":"\nx <- c(1, 2, NA, 4, NA, 5)\nbad <- is.na(x)\nprint(bad)\n#> [1] FALSE FALSE  TRUE FALSE  TRUE FALSE\nx[!bad]\n#> [1] 1 2 4 5\nx[!is.na(x)]\n#> [1] 1 2 4 5"},{"path":"part-ii-data-wrangling.html","id":"in-a-data-frame","chapter":"Part II: Data Wrangling","heading":"In a Data frame","text":"Also, using coffee example,remove rows missing values specific column:multiple R objects want take subset\nmissing values objects?can use complete.cases data frames .","code":"\ncoffee_data_clean <- na.omit(coffee_data)\ncoffee_data_clean\n#>   Age Gender Cups_Per_Day\n#> 1  25 Female            1\n#> 2  32   Male            3\n#> 5  22 Female            2\n#> 6  33   Male            3\n\ncoffee_data_clean2 <- coffee_data[!is.na(coffee_data$Age), ]\ncoffee_data_clean2\n#>   Age Gender Cups_Per_Day\n#> 1  25 Female            1\n#> 2  32   Male            3\n#> 4  45 Female           NA\n#> 5  22 Female            2\n#> 6  33   Male            3\n#> 8  28   <NA>            2\nrow.names(coffee_data_clean2) <- NULL\ncoffee_data_clean2\n#>   Age Gender Cups_Per_Day\n#> 1  25 Female            1\n#> 2  32   Male            3\n#> 3  45 Female           NA\n#> 4  22 Female            2\n#> 5  33   Male            3\n#> 6  28   <NA>            2\nx <- c(1, 2, NA, 4, NA, 5)\ny <- c(\"a\", \"b\", NA, \"d\", NA, \"f\")\ngood <- complete.cases(x, y)\ngood\n#> [1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE\nx[good]\n#> [1] 1 2 4 5\ny[good]\n#> [1] \"a\" \"b\" \"d\" \"f\"\nhead(airquality)\n#>   Ozone Solar.R Wind Temp Month Day\n#> 1    41     190  7.4   67     5   1\n#> 2    36     118  8.0   72     5   2\n#> 3    12     149 12.6   74     5   3\n#> 4    18     313 11.5   62     5   4\n#> 5    NA      NA 14.3   56     5   5\n#> 6    28      NA 14.9   66     5   6\ngood <- complete.cases(airquality)\nhead(airquality[good, ])\n#>   Ozone Solar.R Wind Temp Month Day\n#> 1    41     190  7.4   67     5   1\n#> 2    36     118  8.0   72     5   2\n#> 3    12     149 12.6   74     5   3\n#> 4    18     313 11.5   62     5   4\n#> 7    23     299  8.6   65     5   7\n#> 8    19      99 13.8   59     5   8\nsd(airquality$Ozone)\n#> [1] NA\nsd(airquality$Ozone, na.rm = TRUE)\n#> [1] 32.98788"},{"path":"part-ii-data-wrangling.html","id":"imputing-missing-values","chapter":"Part II: Data Wrangling","heading":"Imputing Missing Values","text":"Replacing missing values specific value, like mean median:","code":"\ncoffee_data2<-coffee_data\n\ncoffee_data2$Age[is.na(coffee_data$Age)] <- mean(coffee_data2$Age, na.rm = TRUE)\ncoffee_data2\n#>        Age Gender Cups_Per_Day\n#> 1 25.00000 Female            1\n#> 2 32.00000   Male            3\n#> 3 30.83333   Male            2\n#> 4 45.00000 Female           NA\n#> 5 22.00000 Female            2\n#> 6 33.00000   Male            3\n#> 7 30.83333 Female            1\n#> 8 28.00000   <NA>            2\n# Assuming 'median' is the mode of the column\nmedian(coffee_data$Age, na.rm = TRUE)\n#> [1] 30\ncoffee_data2$Age[is.na(coffee_data$Age)] <- median(coffee_data$Age, na.rm = TRUE)\ncoffee_data2\n#>   Age Gender Cups_Per_Day\n#> 1  25 Female            1\n#> 2  32   Male            3\n#> 3  30   Male            2\n#> 4  45 Female           NA\n#> 5  22 Female            2\n#> 6  33   Male            3\n#> 7  30 Female            1\n#> 8  28   <NA>            2"},{"path":"part-ii-data-wrangling.html","id":"using-packages-for-advanced-imputation","chapter":"Part II: Data Wrangling","heading":"Using Packages for Advanced Imputation","text":"","code":"\n# install.packages(\"mice\")\nlibrary(mice)\n#> \n#> Attaching package: 'mice'\n#> The following object is masked from 'package:stats':\n#> \n#>     filter\n#> The following objects are masked from 'package:base':\n#> \n#>     cbind, rbind\n# Display the first few rows of the airquality dataset\nhead(airquality)\n#>   Ozone Solar.R Wind Temp Month Day\n#> 1    41     190  7.4   67     5   1\n#> 2    36     118  8.0   72     5   2\n#> 3    12     149 12.6   74     5   3\n#> 4    18     313 11.5   62     5   4\n#> 5    NA      NA 14.3   56     5   5\n#> 6    28      NA 14.9   66     5   6\n\n# Perform multiple imputation\nimputed_data <- mice(airquality, m=5, method='pmm', seed = 123)\n#> \n#>  iter imp variable\n#>   1   1  Ozone  Solar.R\n#>   1   2  Ozone  Solar.R\n#>   1   3  Ozone  Solar.R\n#>   1   4  Ozone  Solar.R\n#>   1   5  Ozone  Solar.R\n#>   2   1  Ozone  Solar.R\n#>   2   2  Ozone  Solar.R\n#>   2   3  Ozone  Solar.R\n#>   2   4  Ozone  Solar.R\n#>   2   5  Ozone  Solar.R\n#>   3   1  Ozone  Solar.R\n#>   3   2  Ozone  Solar.R\n#>   3   3  Ozone  Solar.R\n#>   3   4  Ozone  Solar.R\n#>   3   5  Ozone  Solar.R\n#>   4   1  Ozone  Solar.R\n#>   4   2  Ozone  Solar.R\n#>   4   3  Ozone  Solar.R\n#>   4   4  Ozone  Solar.R\n#>   4   5  Ozone  Solar.R\n#>   5   1  Ozone  Solar.R\n#>   5   2  Ozone  Solar.R\n#>   5   3  Ozone  Solar.R\n#>   5   4  Ozone  Solar.R\n#>   5   5  Ozone  Solar.R\n\n# Extract the first completed dataset\ncompleted_data <- complete(imputed_data, 1)\n\n# Display the first few rows of the completed data\nhead(completed_data)\n#>   Ozone Solar.R Wind Temp Month Day\n#> 1    41     190  7.4   67     5   1\n#> 2    36     118  8.0   72     5   2\n#> 3    12     149 12.6   74     5   3\n#> 4    18     313 11.5   62     5   4\n#> 5    18     150 14.3   56     5   5\n#> 6    28      48 14.9   66     5   6"},{"path":"part-ii-data-wrangling.html","id":"exercise-1-explore-missingness","chapter":"Part II: Data Wrangling","heading":"Exercise 1: Explore Missingness","text":"Dataset: ChickWeightTask: Determine ChickWeight dataset contains missing\nvalues. Print message stating whether dataset missing values\n.Hint Use () function combined .na() applied \ndataset.","code":""},{"path":"part-ii-data-wrangling.html","id":"exercise-2-calculate-summary-statistics-before-handling-na","chapter":"Part II: Data Wrangling","heading":"Exercise 2: Calculate Summary Statistics Before Handling NA","text":"Dataset: mtcarsTask: mtcars dataset almost complete let’s pretend \nvalues missing mpg (miles per gallon) column. First,\nartificially introduce missing values mpg column (e.g., set \nfirst three values mpg NA). , calculate print mean \nstandard deviation mpg without removing imputing missing\nvalues.Hint: Modify mtcars$mpg directly introduce NAs. Use mean() \nsd() functions na.rm = FALSE calculate statistics without\nhandling NA.","code":"\ndata(mtcars)\nmean_mpg <- mean(mtcars$mpg)\nmean_mpg\n#> [1] 20.09062\nsd_mpg <- sd(mtcars$mpg)\nsd_mpg\n#> [1] 6.026948"},{"path":"part-ii-data-wrangling.html","id":"exercise-3-impute-missing-values-with-column-median","chapter":"Part II: Data Wrangling","heading":"Exercise 3: Impute Missing Values with Column Median","text":"Dataset: mtcars modified mpgTask: First Calculate mean standard deviation handling \nmissing values.,Impute artificially introduced missing values mpg column\ncolumn’s median (excluding missing values). Print first\n6 rows modified mtcars dataset.Now, calculate mean standard deviation imputed values.Hint: First, calculate median mpg excluding NAs. , use\nindexing replace NAs median.","code":""},{"path":"part-ii-data-wrangling.html","id":"exercise-4-identifying-complete-rows","chapter":"Part II: Data Wrangling","heading":"Exercise 4: Identifying Complete Rows","text":"Dataset: airqualityTask: analysis, want ensure complete\ncases used. Create new dataset airquality includes \nrows without missing values. Print number rows \noriginal versus cleaned dataset.Hint Use complete.cases() dataset subset .","code":""},{"path":"part-ii-data-wrangling.html","id":"exercise-5-advanced-imputation-on-a-subset","chapter":"Part II: Data Wrangling","heading":"Exercise 5: Advanced Imputation on a Subset","text":"Dataset: mtcarsTask: Create subset mtcars containing mpg, hp\n(horsepower), wt (weight) columns. Introduce missing values hp\nwt columns (e.g., set first two values NA). Perform\nmultiple imputation using mice package subset 3\nimputations, extract third completed dataset. Print first 6\nrows completed dataset.Hint: Subset mtcars first, modify add NAs. Use mice() \nimputation complete() extract desired imputed dataset.","code":""},{"path":"intermediate-r-ii.html","id":"intermediate-r-ii","chapter":"Intermediate R II","heading":"Intermediate R II","text":"session , advanced data visualization ,statistical analysis.time permits, cover Dates Times","code":""},{"path":"part-i-advanced-data-visualization.html","id":"part-i-advanced-data-visualization","chapter":"Part I: Advanced Data Visualization","heading":"Part I: Advanced Data Visualization","text":"aim ggplot2 package create elegant data visualizations using grammar graphics.basic steps:begin plot function ggplot() creating coordinate system can add layers -first argument ggplot() dataset use graphWe use mpg dataset ggplot2Run following code,obtain ?complete graph adding one layers ggplot()example:\n- geom_point() adds layer points plot, creates scatterplot\n- geom_smooth() adds smooth line\n- geom_bar bar plot.geom function ggplot2 takes mapping argument:\n- variables dataset mapped visual properties\n- always paired aes() arguments aes() specify variables map axes.Compare following set instructions:inside aestheticsinside aesthetics, mapped variableoutside aestheticsScatterplotboxplothistogramNow add multiple geoms plot.\nPredict following code :Mappings data can specified global (ggplot()) local.local.","code":"\nlibrary(ggplot2)\nhead(mpg)\n#> # A tibble: 6 × 11\n#>   manufacturer model displ  year   cyl trans     drv     cty\n#>   <chr>        <chr> <dbl> <int> <int> <chr>     <chr> <int>\n#> 1 audi         a4      1.8  1999     4 auto(l5)  f        18\n#> 2 audi         a4      1.8  1999     4 manual(m… f        21\n#> 3 audi         a4      2    2008     4 manual(m… f        20\n#> 4 audi         a4      2    2008     4 auto(av)  f        21\n#> 5 audi         a4      2.8  1999     6 auto(l5)  f        16\n#> 6 audi         a4      2.8  1999     6 manual(m… f        18\n#> # ℹ 3 more variables: hwy <int>, fl <chr>, class <chr>\n\nggplot(data = mpg)\nggplot(mpg)\nlibrary(ggplot2)\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\nggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = class))\nggplot(mpg) + geom_point(aes(x = displ, y = hwy, color = class))\nggplot(mpg) + geom_point(aes(x = displ, y = hwy, color = \"blue\"))\nggplot(mpg) + geom_point(aes(x = displ, y = hwy), color = \"blue\")\nggplot(mpg) + \n  geom_point(mapping = aes(x = class, y = hwy))\nggplot(data = mpg) +\n  geom_boxplot(mapping = aes(x = class, y = hwy))\nggplot(data = mpg) +\n  geom_histogram(mapping = aes(x = hwy))\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\nggplot(data = mpg) +\n  geom_density(mapping = aes(x = hwy))\nggplot(data = mpg) +\n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~\n#> x'\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth() + theme_bw()       # adjust theme\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~\n#> x'\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point(mapping = aes(color = drv)) +\n  geom_smooth() + theme_bw()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~\n#> x'\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point(mapping = aes(color = drv)) +\n  geom_smooth(data = filter(mpg, drv == \"f\")) + theme_bw()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~\n#> x'"},{"path":"part-i-advanced-data-visualization.html","id":"exercise-1-6","chapter":"Part I: Advanced Data Visualization","heading":"Exercise 1","text":"Use Danish fire insurance losses. Plot arrival losses time.Use type= “l” line plot, label -axis, give plot title using main.instructions ggplot2. Use geom_line() create line plot.","code":""},{"path":"part-i-advanced-data-visualization.html","id":"exercise-2-5","chapter":"Part I: Advanced Data Visualization","heading":"Exercise 2","text":"Use data set car_price.csv available documentation. Import data R.Use data set car_price.csv available documentation. Import data R.Explore data.Explore data.Make scatterplot price versus income, use basic plotting instructions\nuse ggplot2.Make scatterplot price versus income, use basic plotting instructions\nuse ggplot2.Add smooth line plots (using lines add line existing plot lowess scatterplot smoothing using geom_smooth ggplot2 grammar).Add smooth line plots (using lines add line existing plot lowess scatterplot smoothing using geom_smooth ggplot2 grammar).","code":""},{"path":"part-i-advanced-data-visualization.html","id":"creating-customized-plots-with-ggplot2","chapter":"Part I: Advanced Data Visualization","heading":"Creating customized plots with ggplot2","text":"","code":"\n\n# Load ggplot2 package\nlibrary(ggplot2)\n\n# Example: Customized scatter plot with ggplot2\ndata <- data.frame(x = rnorm(100), y = rnorm(100))\nggplot(data, aes(x = x, y = y)) +\n  geom_point(aes(color = x*y), size = 3) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  ggtitle(\"Customized Scatter Plot with Color Gradient\") +\n  theme_minimal()"},{"path":"part-i-advanced-data-visualization.html","id":"adding-titles-labels-and-themes-to-plots","chapter":"Part I: Advanced Data Visualization","heading":"Adding titles, labels, and themes to plots","text":"","code":"\n# Example: Enhanced bar plot with titles, labels, and a custom theme\ndata <- data.frame(\n  category = c(\"A\", \"B\", \"C\", \"D\"),\n  value = c(10, 15, 7, 12)\n)\nggplot(data, aes(x = category, y = value, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Enhanced Bar Plot\",\n       subtitle = \"Bar plot with custom labels and theme\",\n       x = \"Category\",\n       y = \"Value\",\n       fill = \"Category\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))"},{"path":"part-ii-statistical-analysis.html","id":"part-ii-statistical-analysis","chapter":"Part II: Statistical Analysis","heading":"Part II: Statistical Analysis","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"introduction-to-hypothesis-testing-and-statistical-tests","chapter":"Part II: Statistical Analysis","heading":"Introduction to hypothesis testing and statistical tests","text":"Hypothesis testing statistical method used make inferences draw conclusions population based sample data. starts null hypothesis (H0) assumes effect difference, alternative hypothesis (H1) contradicts null hypothesis.process involves:\n1. Defining null alternative hypotheses.Selecting significance level (alpha, typically 0.05).Selecting significance level (alpha, typically 0.05).Calculating test statistic based sample data.Calculating test statistic based sample data.Determining p-value, probability observing test statistic something extreme null hypothesis.Determining p-value, probability observing test statistic something extreme null hypothesis.Comparing p-value significance level decide whether reject null hypothesis.Comparing p-value significance level decide whether reject null hypothesis.Statistical tests vary based type data research question. Common tests include t-tests (means), chi-squared tests (categorical data), ANOVA (comparing means across multiple groups), regression analysis (relationships variables).","code":""},{"path":"part-ii-statistical-analysis.html","id":"comparing-variances","chapter":"Part II: Statistical Analysis","heading":"0.2 Comparing Variances","text":"F test compare variances (Parametric)Barlett test: Testing homogeneity (Parametric)Performs Bartlett’s test null variances groups (samples) .Fligner-Killeen Test Homogeneity Variances (Non-parametric)Mood Two-Sample Test Scale (Non-Parametric)Ansari-Bradley Test (Non-parametric)Testing two normal distributions","code":"\nx <- rnorm(50, mean = 0, sd = 2)\ny <- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)\n#> \n#>  F test to compare two variances\n#> \n#> data:  x and y\n#> F = 4.6796, num df = 49, denom df = 29, p-value =\n#> 3.149e-05\n#> alternative hypothesis: true ratio of variances is not equal to 1\n#> 95 percent confidence interval:\n#>  2.351119 8.804199\n#> sample estimates:\n#> ratio of variances \n#>           4.679559\nrequire(graphics)\n\nplot(count ~ spray, data = InsectSprays)\nbartlett.test(InsectSprays$count, InsectSprays$spray)\n#> \n#>  Bartlett test of homogeneity of variances\n#> \n#> data:  InsectSprays$count and InsectSprays$spray\n#> Bartlett's K-squared = 25.96, df = 5, p-value =\n#> 9.085e-05\nfligner.test(InsectSprays$count, InsectSprays$spray)\n#> \n#>  Fligner-Killeen test of homogeneity of variances\n#> \n#> data:  InsectSprays$count and InsectSprays$spray\n#> Fligner-Killeen:med chi-squared = 14.483, df = 5,\n#> p-value = 0.01282\nramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,\n            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)\njung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,\n            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)\nmood.test(ramsay, jung.parekh)\n#> \n#>  Mood two-sample test of scale\n#> \n#> data:  ramsay and jung.parekh\n#> Z = 1.0371, p-value = 0.2997\n#> alternative hypothesis: two.sided\nramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,\n            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)\njung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,\n            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)\nansari.test(ramsay, jung.parekh)\n#> Warning in ansari.test.default(ramsay, jung.parekh): cannot\n#> compute exact p-value with ties\n#> \n#>  Ansari-Bradley test\n#> \n#> data:  ramsay and jung.parekh\n#> AB = 185.5, p-value = 0.1815\n#> alternative hypothesis: true ratio of scales is not equal to 1\nansari.test(rnorm(100), rnorm(100, 0, 2), conf.int = TRUE)\n#> \n#>  Ansari-Bradley test\n#> \n#> data:  rnorm(100) and rnorm(100, 0, 2)\n#> AB = 6036, p-value = 1.446e-06\n#> alternative hypothesis: true ratio of scales is not equal to 1\n#> 95 percent confidence interval:\n#>  0.4059256 0.6601837\n#> sample estimates:\n#> ratio of scales \n#>       0.5191969"},{"path":"part-ii-statistical-analysis.html","id":"performing-tests","chapter":"Part II: Statistical Analysis","heading":"Performing Tests","text":"Tests Comparing MeansOne-Sample t-TestWe’ll test average miles per gallon (mpg) mtcars dataset significantly different 20 mpg.results one-sample t-test suggest average mpg cars mtcars dataset significantly different 20 mpg, p-value far typical alpha level 0.05 used determine statistical significance. data supports null hypothesis true mean 20 mpg, within confidence interval provided.Independent Two-Sample t-TestWe’ll compare means mpg cars automatic (= 0) manual (= 1) transmissions.results two-sample t-test indicate average mpg cars automatic transmissions significantly differs manual transmissions, p-value (0.000285) well alpha level 0.05 typically used determining statistical significance. data strongly support alternative hypothesis true difference means, manual transmission cars averaging higher mpg (24.39 mpg) compared automatics (17.15 mpg), reflected within confidence interval provided.Paired t-TestLet’s use sleep dataThe results paired t-test suggest statistically significant difference extra sleep effects two treatment groups, p-value (0.002833) well typical alpha level 0.05 used determining statistical significance. data strongly support alternative hypothesis true mean difference sleep effects equal zero, average mean difference -1.58 hours. difference indicates one treatment group experienced greater increase sleep duration compared , confirmed confidence interval ranging -2.46 -0.70 hours.One-Way ANOVATest differences mpg across different levels number cylinders (cyl).ANOVA analysis clearly shows number cylinders vehicle significantly affects fuel efficiency, different cylinder groups exhibiting notably different mpg. finding robust, strong statistical significance, suggesting engine size, indicated number cylinders, key factor influencing car’s fuel consumption. information can vital consumers seeking fuel-efficient vehicles manufacturers aiming improve vehicle designs.Repeated Measures ANOVAThe model provides strong evidence type plant whether chilled significantly affect CO2 uptake, independently. lack significant interaction suggests effect chilling differ types way might expected.Tests Comparing MediansMann-Whitney U TestComparing mpg cars 4 6 cylinders.result Wilcoxon rank sum test strongly suggests median mpg values cars 4 cylinders differ significantly 6 cylinders mtcars dataset. Given low p-value, likely 4-cylinder cars either achieve higher lower mpg compared 6-cylinder cars, depending direction rank sums (specified typically inferred data setup). finding crucial automotive studies focusing fuel efficiency based engine size, providing evidence engine size (represented cylinder count) may impact fuel economy.Wilcoxon Signed-Rank TestAgain, hypothetical example paired data.results Wilcoxon signed-rank test suggest statistically significant difference two groups tested, p-value indicating strong evidence null hypothesis difference. significant finding implies treatment condition represented two groups different impact variable measured (extra sleep hours), direction effect (group sleep) needing description data setup. analysis particularly useful clinical psychological studies normality assumption may hold, robust, non-parametric methods required.Kruskal-Wallis TestComparing mpg across different cylinder groups.Kruskal-Wallis test conclusively shows number cylinders significant factor determining car’s miles per gallon, differences mpg across groups statistically significant. insight can inform decisions related car manufacturing consumer choice, particularly contexts fuel efficiency critical concern.Friedman TestThe significant Friedman test result suggests conditions treatments applied RoundingTimes study differing effects, statistically notable. finding may lead investigation specific treatments differ differences might exploited managed practical applications, clinical, psychological, educational settings treatments interventions used.Tests ProportionsChi-Square Test IndependenceTesting transmission type () independent engine cylinders (cyl).results Pearson’s Chi-squared test suggest significant relationship type transmission number cylinders vehicles. finding implies certain transmission types might less common vehicles different numbers cylinders, potentially reflecting design preferences, performance characteristics, market trends specific certain types vehicles. insight valuable automotive manufacturers marketers targeting specific segments car market.Fisher’s Exact TestLet’s create hypothetical dataset suitable testThe results suggest might difference odds event occurring two groups, data provide strong enough evidence assert statistically significant association groups study. wide confidence interval odds ratio underscores need cautious interpretation odds ratio estimate. data additional studies might required clarify nature relationship groups.One-Proportion Z-TestTesting proportion cars 4 cylinders different 50%.results suggest proportion cars 4 cylinders mtcars dataset significantly differ hypothesized 50%. p-value indicates observed difference reasonably occur chance null hypothesis. confidence interval includes null value (0.5), supporting conclusion. finding implies may strong bias towards cars 4 cylinders mtcars dataset, although observed proportion leans slightly towards higher number cylinders. data larger sample might provide clearer insights definitive evidence regarding distribution cylinder numbers cars.Two-Proportion Z-TestComparing proportion manual vs automatic cars 6-cylinder.Given peculiar results, especially p-value chi-squared statistic, prudent double-check input data consider whether test assumptions met different statistical approach might appropriate. data inputs correct assumptions met, findings suggest transmission type significantly influence whether car 6 cylinders mtcars dataset. lack difference important automotive studies examining relationship transmission type engine size, though unusual statistical outputs warrant careful review data method.Correlation TestsPearson Correlation CoefficientCorrelation mpg wt (weight).findings Pearson correlation test provide clear evidence increase car weight associated decrease miles per gallon mtcars dataset. relationship strong statistically significant, nearly chance occurring due random variation sample. insights crucial automotive design consumer choice, particularly discussions around fuel efficiency vehicle performance optimization.Spearman’s Rank CorrelationCorrelation mpg hp (horsepower).test results suggest cars higher horsepower tend lower fuel efficiency, measured miles per gallon, mtcars dataset. finding useful automotive manufacturers buyers prioritize fuel efficiency. high degree correlation provides robust evidence increasing horsepower vehicle design typically comes expense fuel economy. relationship important consideration engineering marketing strategies automotive industry.Kendall’s TauCorrelation mpg disp (displacement).significant results Kendall’s test confirm increases engine displacement associated decreases fuel efficiency across cars sampled mtcars dataset. finding crucial understanding engine size affects fuel economy can guide consumer choices manufacturer designs, especially contexts fuel efficiency priority. correlation essential consideration automotive design, influencing decisions engine specifications relation fuel economy objectives.","code":"\nt.test(mtcars$mpg, mu = 20)\n#> \n#>  One Sample t-test\n#> \n#> data:  mtcars$mpg\n#> t = 0.08506, df = 31, p-value = 0.9328\n#> alternative hypothesis: true mean is not equal to 20\n#> 95 percent confidence interval:\n#>  17.91768 22.26357\n#> sample estimates:\n#> mean of x \n#>  20.09062\nauto_mpg <- mtcars$mpg[mtcars$am == 0]\nmanual_mpg <- mtcars$mpg[mtcars$am == 1]\nt.test(auto_mpg, manual_mpg, var.equal = TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  auto_mpg and manual_mpg\n#> t = -4.1061, df = 30, p-value = 0.000285\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -10.84837  -3.64151\n#> sample estimates:\n#> mean of x mean of y \n#>  17.14737  24.39231\n# Load the dataset\ndata(sleep)\n\n# Perform the paired t-test comparing the effects of two drugs\nt_test_result <- t.test(extra ~ group, data = sleep, paired = TRUE)\n\n# Print the results\nprint(t_test_result)\n#> \n#>  Paired t-test\n#> \n#> data:  extra by group\n#> t = -4.0621, df = 9, p-value = 0.002833\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.4598858 -0.7001142\n#> sample estimates:\n#> mean difference \n#>           -1.58\nanova_model <- aov(mpg ~ factor(cyl), data = mtcars)\nsummary(anova_model)\n#>             Df Sum Sq Mean Sq F value   Pr(>F)    \n#> factor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\n#> Residuals   29  301.3    10.4                     \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Load the CO2 dataset from the datasets package\ndata(CO2)\n\n# Check the structure of the data\nstr(CO2)\n#> Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':   84 obs. of  5 variables:\n#>  $ Plant    : Ord.factor w/ 12 levels \"Qn1\"<\"Qn2\"<\"Qn3\"<..: 1 1 1 1 1 1 1 2 2 2 ...\n#>  $ Type     : Factor w/ 2 levels \"Quebec\",\"Mississippi\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Treatment: Factor w/ 2 levels \"nonchilled\",\"chilled\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...\n#>  $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...\n#>  - attr(*, \"formula\")=Class 'formula'  language uptake ~ conc | Plant\n#>   .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n#>  - attr(*, \"outer\")=Class 'formula'  language ~Treatment * Type\n#>   .. ..- attr(*, \".Environment\")=<environment: R_EmptyEnv> \n#>  - attr(*, \"labels\")=List of 2\n#>   ..$ x: chr \"Ambient carbon dioxide concentration\"\n#>   ..$ y: chr \"CO2 uptake rate\"\n#>  - attr(*, \"units\")=List of 2\n#>   ..$ x: chr \"(uL/L)\"\n#>   ..$ y: chr \"(umol/m^2 s)\"\n\n# Load necessary package for analysis\n#install.packages(\"nlme\") \nlibrary(nlme)  # for linear mixed-effects models\n\n# Fit a repeated measures model\n# Treat 'Plant' as a random effect to account for measurements from the same plant\nmodel <- lme(uptake ~ Type * Treatment, random = ~ 1 | Plant, data = CO2)\n\n# Summary of the model\nsummary(model)\n#> Linear mixed-effects model fit by REML\n#>   Data: CO2 \n#>        AIC      BIC    logLik\n#>   584.0375 598.3297 -286.0188\n#> \n#> Random effects:\n#>  Formula: ~1 | Plant\n#>          (Intercept) Residual\n#> StdDev: 0.0004510923 8.005933\n#> \n#> Fixed effects:  uptake ~ Type * Treatment \n#>                                     Value Std.Error DF\n#> (Intercept)                      35.33333  1.747038 72\n#> TypeMississippi                  -9.38095  2.470685  8\n#> Treatmentchilled                 -3.58095  2.470685  8\n#> TypeMississippi:Treatmentchilled -6.55714  3.494076  8\n#>                                    t-value p-value\n#> (Intercept)                      20.224710  0.0000\n#> TypeMississippi                  -3.796904  0.0053\n#> Treatmentchilled                 -1.449377  0.1853\n#> TypeMississippi:Treatmentchilled -1.876646  0.0974\n#>  Correlation: \n#>                                  (Intr) TypMss Trtmnt\n#> TypeMississippi                  -0.707              \n#> Treatmentchilled                 -0.707  0.500       \n#> TypeMississippi:Treatmentchilled  0.500 -0.707 -0.707\n#> \n#> Standardized Within-Group Residuals:\n#>        Min         Q1        Med         Q3        Max \n#> -2.8044677 -0.4526405  0.2706326  0.7210426  1.3299660 \n#> \n#> Number of Observations: 84\n#> Number of Groups: 12\n\n# Anova table for the model\nanova(model)\n#>                numDF denDF  F-value p-value\n#> (Intercept)        1    72 970.5351  <.0001\n#> Type               1     8  52.5086  0.0001\n#> Treatment          1     8  15.4164  0.0044\n#> Type:Treatment     1     8   3.5218  0.0974\nmpg_4 <- mtcars$mpg[mtcars$cyl == 4]\nmpg_6 <- mtcars$mpg[mtcars$cyl == 6]\nwilcox.test(mpg_4, mpg_6)\n#> Warning in wilcox.test.default(mpg_4, mpg_6): cannot\n#> compute exact p-value with ties\n#> \n#>  Wilcoxon rank sum test with continuity correction\n#> \n#> data:  mpg_4 and mpg_6\n#> W = 76.5, p-value = 0.0006658\n#> alternative hypothesis: true location shift is not equal to 0\n\n# Extracting the groups\n\ngroup1 <- sleep$extra[sleep$group == 1]\ngroup2 <- sleep$extra[sleep$group == 2]\n\n# Wilcoxon Signed-Rank Test\nwilcox_test_results <- wilcox.test(group1, group2, paired = TRUE)\n#> Warning in wilcox.test.default(group1, group2, paired =\n#> TRUE): cannot compute exact p-value with ties\n#> Warning in wilcox.test.default(group1, group2, paired =\n#> TRUE): cannot compute exact p-value with zeroes\n\n# Print the results\nprint(wilcox_test_results)\n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  group1 and group2\n#> V = 0, p-value = 0.009091\n#> alternative hypothesis: true location shift is not equal to 0\nx <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\nwilcox.test(x, y, paired = TRUE, alternative = \"greater\")\n#> \n#>  Wilcoxon signed rank exact test\n#> \n#> data:  x and y\n#> V = 40, p-value = 0.01953\n#> alternative hypothesis: true location shift is greater than 0\nkruskal.test(mpg ~ factor(cyl), data = mtcars)\n#> \n#>  Kruskal-Wallis rank sum test\n#> \n#> data:  mpg by factor(cyl)\n#> Kruskal-Wallis chi-squared = 25.746, df = 2, p-value\n#> = 2.566e-06\nRoundingTimes <-\nmatrix(c(5.40, 5.50, 5.55,\n         5.85, 5.70, 5.75,\n         5.20, 5.60, 5.50,\n         5.55, 5.50, 5.40,\n         5.90, 5.85, 5.70,\n         5.45, 5.55, 5.60,\n         5.40, 5.40, 5.35,\n         5.45, 5.50, 5.35,\n         5.25, 5.15, 5.00,\n         5.85, 5.80, 5.70,\n         5.25, 5.20, 5.10,\n         5.65, 5.55, 5.45,\n         5.60, 5.35, 5.45,\n         5.05, 5.00, 4.95,\n         5.50, 5.50, 5.40,\n         5.45, 5.55, 5.50,\n         5.55, 5.55, 5.35,\n         5.45, 5.50, 5.55,\n         5.50, 5.45, 5.25,\n         5.65, 5.60, 5.40,\n         5.70, 5.65, 5.55,\n         6.30, 6.30, 6.25),\n       nrow = 22,\n       byrow = TRUE,\n       dimnames = list(1 : 22,\n                       c(\"Round Out\", \"Narrow Angle\", \"Wide Angle\")))\nRoundingTimes\n#>    Round Out Narrow Angle Wide Angle\n#> 1       5.40         5.50       5.55\n#> 2       5.85         5.70       5.75\n#> 3       5.20         5.60       5.50\n#> 4       5.55         5.50       5.40\n#> 5       5.90         5.85       5.70\n#> 6       5.45         5.55       5.60\n#> 7       5.40         5.40       5.35\n#> 8       5.45         5.50       5.35\n#> 9       5.25         5.15       5.00\n#> 10      5.85         5.80       5.70\n#> 11      5.25         5.20       5.10\n#> 12      5.65         5.55       5.45\n#> 13      5.60         5.35       5.45\n#> 14      5.05         5.00       4.95\n#> 15      5.50         5.50       5.40\n#> 16      5.45         5.55       5.50\n#> 17      5.55         5.55       5.35\n#> 18      5.45         5.50       5.55\n#> 19      5.50         5.45       5.25\n#> 20      5.65         5.60       5.40\n#> 21      5.70         5.65       5.55\n#> 22      6.30         6.30       6.25\nfriedman.test(RoundingTimes)\n#> \n#>  Friedman rank sum test\n#> \n#> data:  RoundingTimes\n#> Friedman chi-squared = 11.143, df = 2, p-value =\n#> 0.003805\ntable_data <- table(mtcars$am, mtcars$cyl)\nchisq.test(table_data)\n#> Warning in chisq.test(table_data): Chi-squared\n#> approximation may be incorrect\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  table_data\n#> X-squared = 8.7407, df = 2, p-value = 0.01265\n# \n# Data: Drug success (Yes, No) by Treatment group (Drug, Placebo)\ndrug_data <- matrix(c(4, 1, 1, 3), ncol = 2, byrow = TRUE,\n                    dimnames = list(c(\"Drug\", \"Placebo\"),\n                                    c(\"Success\", \"Failure\")))\n\ndrug_data\n#>         Success Failure\n#> Drug          4       1\n#> Placebo       1       3\n# Perform Fisher's Exact Test\nfisher_results <- fisher.test(drug_data)\n\n# Print the results\nprint(fisher_results)\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  drug_data\n#> p-value = 0.2063\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>    0.3071304 776.3482393\n#> sample estimates:\n#> odds ratio \n#>   8.355086\nprop.test(sum(mtcars$cyl > 4), nrow(mtcars), p = 0.5)\n#> \n#>  1-sample proportions test with continuity correction\n#> \n#> data:  sum(mtcars$cyl > 4) out of nrow(mtcars), null probability 0.5\n#> X-squared = 2.5312, df = 1, p-value = 0.1116\n#> alternative hypothesis: true p is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.4677478 0.8082695\n#> sample estimates:\n#>       p \n#> 0.65625\nmanual_six <- sum(mtcars$cyl == 6 & mtcars$am == 1)\nauto_six <- sum(mtcars$cyl == 6 & mtcars$am == 0)\nprop.test(c(manual_six, auto_six), c(sum(mtcars$am == 1), sum(mtcars$am == 0)))\n#> Warning in prop.test(c(manual_six, auto_six),\n#> c(sum(mtcars$am == 1), sum(mtcars$am == : Chi-squared\n#> approximation may be incorrect\n#> \n#>  2-sample test for equality of proportions with\n#>  continuity correction\n#> \n#> data:  c(manual_six, auto_six) out of c(sum(mtcars$am == 1), sum(mtcars$am == 0))\n#> X-squared = 2.8616e-32, df = 1, p-value = 1\n#> alternative hypothesis: two.sided\n#> 95 percent confidence interval:\n#>  -0.2933577  0.3338435\n#> sample estimates:\n#>    prop 1    prop 2 \n#> 0.2307692 0.2105263\nsmokers  <- c( 83, 90, 129, 70 )\npatients <- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n#> \n#>  4-sample test for equality of proportions without\n#>  continuity correction\n#> \n#> data:  smokers out of patients\n#> X-squared = 12.6, df = 3, p-value = 0.005585\n#> alternative hypothesis: two.sided\n#> sample estimates:\n#>    prop 1    prop 2    prop 3    prop 4 \n#> 0.9651163 0.9677419 0.9485294 0.8536585\ncor.test(mtcars$mpg, mtcars$wt, method = \"pearson\")\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  mtcars$mpg and mtcars$wt\n#> t = -9.559, df = 30, p-value = 1.294e-10\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.9338264 -0.7440872\n#> sample estimates:\n#>        cor \n#> -0.8676594\ncor.test(mtcars$mpg, mtcars$hp, method = \"spearman\")\n#> Warning in cor.test.default(mtcars$mpg, mtcars$hp, method =\n#> \"spearman\"): Cannot compute exact p-value with ties\n#> \n#>  Spearman's rank correlation rho\n#> \n#> data:  mtcars$mpg and mtcars$hp\n#> S = 10337, p-value = 5.086e-12\n#> alternative hypothesis: true rho is not equal to 0\n#> sample estimates:\n#>        rho \n#> -0.8946646\ncor.test(mtcars$mpg, mtcars$disp, method = \"kendall\")\n#> Warning in cor.test.default(mtcars$mpg, mtcars$disp, method\n#> = \"kendall\"): Cannot compute exact p-value with ties\n#> \n#>  Kendall's rank correlation tau\n#> \n#> data:  mtcars$mpg and mtcars$disp\n#> z = -6.1083, p-value = 1.007e-09\n#> alternative hypothesis: true tau is not equal to 0\n#> sample estimates:\n#>        tau \n#> -0.7681311"},{"path":"part-ii-statistical-analysis.html","id":"exercises-hypothesis-testing","chapter":"Part II: Statistical Analysis","heading":"Exercises Hypothesis Testing","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-1-7","chapter":"Part II: Statistical Analysis","heading":"Exercise 1","text":"Test average wind speed airquality dataset significantly different 10 mph.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-2-6","chapter":"Part II: Statistical Analysis","heading":"Exercise 2","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"independent-two-sample-t-test-plantgrowth-dataset","chapter":"Part II: Statistical Analysis","heading":"Independent Two-Sample t-Test: PlantGrowth Dataset","text":"Compare means weight two groups plants: ctrl trt1.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-3-2","chapter":"Part II: Statistical Analysis","heading":"Exercise 3","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"paired-t-test","chapter":"Part II: Statistical Analysis","heading":"0.2.1 Paired t-Test","text":"Use following data perform T-test check score greater .data 20 students testing studying .reject null hypothesis.","code":"\nbefore <- c(12.2, 14.6, 13.4, 11.2, 12.7, 10.4, 15.8, 13.9, 9.5, 14.2)\nafter <- c(13.5, 15.2, 13.6, 12.8, 13.7, 11.3, 16.5, 13.4, 8.7, 14.6)\ndata <- data.frame(subject = rep(c(1:10), 2), \n                   time = rep(c(\"before\", \"after\"), each = 10),\n                   score = c(before, after))"},{"path":"part-ii-statistical-analysis.html","id":"exercise-4-3","chapter":"Part II: Statistical Analysis","heading":"Exercise 4","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"one-way-anova-chickweight-dataset","chapter":"Part II: Statistical Analysis","heading":"One-Way ANOVA: ChickWeight Dataset","text":"Test differences weight across different feed types.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-5-3","chapter":"Part II: Statistical Analysis","heading":"Exercise 5","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"repeated-measures-anova-orthodont-dataset","chapter":"Part II: Statistical Analysis","heading":"Repeated Measures ANOVA: Orthodont Dataset","text":"Load Orthodont dataset nlme package fit repeated measures model","code":""},{"path":"part-ii-statistical-analysis.html","id":"tests-for-comparing-medians","chapter":"Part II: Statistical Analysis","heading":"0.3 Tests for Comparing Medians","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-6","chapter":"Part II: Statistical Analysis","heading":"Exercise 6","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"mann-whitney-u-test-insectsprays-dataset","chapter":"Part II: Statistical Analysis","heading":"0.3.1 Mann-Whitney U Test: InsectSprays Dataset","text":"Compare effectiveness two insect sprays.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-7","chapter":"Part II: Statistical Analysis","heading":"Exercise 7","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"wilcoxon-signed-rank-test-airquality-dataset","chapter":"Part II: Statistical Analysis","heading":"Wilcoxon Signed-Rank Test: Airquality Dataset","text":"Compare Ozone levels first half second half dataset.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-8","chapter":"Part II: Statistical Analysis","heading":"Exercise 8","text":"###Kruskal-Wallis Test: ChickWeight Dataset {-}Compare weights across different diets using Kruskal-Wallis test.","code":""},{"path":"part-ii-statistical-analysis.html","id":"tests-for-proportions","chapter":"Part II: Statistical Analysis","heading":"0.4 Tests for Proportions","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-9","chapter":"Part II: Statistical Analysis","heading":"Exercise 9","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"chi-square-test-of-independence-haireyecolor-dataset","chapter":"Part II: Statistical Analysis","heading":"0.4.1 Chi-Square Test of Independence: HairEyeColor Dataset","text":"Test hair color independent eye color.","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-10","chapter":"Part II: Statistical Analysis","heading":"Exercise 10","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"one-proportion-z-test-using-airquality-dataset","chapter":"Part II: Statistical Analysis","heading":"One-Proportion Z-Test: Using Airquality Dataset","text":"Suppose want analyze proportion observations chick weights exceed 250.","code":""},{"path":"part-ii-statistical-analysis.html","id":"correlation-tests","chapter":"Part II: Statistical Analysis","heading":"Correlation Tests","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-11","chapter":"Part II: Statistical Analysis","heading":"Exercise 11","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"pearson-correlation-coefficient-using-usjudgeratings-dataset","chapter":"Part II: Statistical Analysis","heading":"0.4.2 Pearson Correlation Coefficient: Using USJudgeRatings Dataset","text":"Correlation lawyer Judicial integrity judicial Diligence","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-12","chapter":"Part II: Statistical Analysis","heading":"Exercise 12","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"spearmans-rank-correlation-using-usjudgeratings-dataset","chapter":"Part II: Statistical Analysis","heading":"0.4.3 Spearman’s Rank Correlation: Using USJudgeRatings Dataset","text":"Correlation lawyers’ rating integrity number contacts judge","code":""},{"path":"part-ii-statistical-analysis.html","id":"exercise-13","chapter":"Part II: Statistical Analysis","heading":"Exercise 13","text":"","code":""},{"path":"part-ii-statistical-analysis.html","id":"kendalls-tau-using-usjudgeratings-dataset","chapter":"Part II: Statistical Analysis","heading":"Kendall’s Tau: Using USJudgeRatings Dataset","text":"Correlation preparation trial diligence","code":""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"optional-part-v-working-with-dates-and-times-20-minutes","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"(Optional) Part V: Working with Dates and Times (20 minutes)","text":"","code":""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"handling-date-and-time-data-in-r","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"Handling date and time data in R","text":"R provides Date class dates POSIXct POSIXlt classes times.","code":"\n# Converting a string to a Date object\ndate_example <- as.Date(\"2021-01-01\")\nprint(date_example)\n#> [1] \"2021-01-01\"\n\n# Converting a string to a POSIXct datetime object\ndatetime_example <- as.POSIXct(\"2021-01-01 10:00:00\", tz = \"GMT\")\nprint(datetime_example)\n#> [1] \"2021-01-01 10:00:00 GMT\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"common-date-and-time-functions","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"Common date and time functions","text":"Exercises:Convert ‘Date’ column ‘airquality’ dataset week day create new column ‘WeekDay’.Calculate number days first last measurements ‘airquality’ dataset.","code":"\n\n# Extracting parts of a date\nyear <- format(date_example, \"%Y\")\nmonth <- format(date_example, \"%m\")\nday <- format(date_example, \"%d\")\nprint(paste(\"Year:\", year, \"- Month:\", month, \"- Day:\", day))\n#> [1] \"Year: 2021 - Month: 01 - Day: 01\"\n\n# Working with time intervals\nstart_time <- as.POSIXct(\"2021-01-01 08:00:00\", tz = \"GMT\")\nend_time <- as.POSIXct(\"2021-01-01 10:00:00\", tz = \"GMT\")\ntime_diff <- difftime(end_time, start_time, units = \"hours\")\nprint(paste(\"Difference in hours:\", time_diff))\n#> [1] \"Difference in hours: 2\"\n\n# Loading a dataset with date and time data for exercises\n# Using the 'airquality' dataset from the 'datasets' package\ndata(airquality)\nairquality$Date <- as.Date(with(airquality, paste(1973, Month, Day, sep = \"-\")))\nprint(head(airquality))\n#>   Ozone Solar.R Wind Temp Month Day       Date\n#> 1    41     190  7.4   67     5   1 1973-05-01\n#> 2    36     118  8.0   72     5   2 1973-05-02\n#> 3    12     149 12.6   74     5   3 1973-05-03\n#> 4    18     313 11.5   62     5   4 1973-05-04\n#> 5    NA      NA 14.3   56     5   5 1973-05-05\n#> 6    28      NA 14.9   66     5   6 1973-05-06\ndata(airquality)\n\nairquality$Date <- as.Date(with(airquality, paste(1973, Month, Day, sep = \"-\")))\nprint(head(airquality))\n#>   Ozone Solar.R Wind Temp Month Day       Date\n#> 1    41     190  7.4   67     5   1 1973-05-01\n#> 2    36     118  8.0   72     5   2 1973-05-02\n#> 3    12     149 12.6   74     5   3 1973-05-03\n#> 4    18     313 11.5   62     5   4 1973-05-04\n#> 5    NA      NA 14.3   56     5   5 1973-05-05\n#> 6    28      NA 14.9   66     5   6 1973-05-06\nairquality$WeekDay <- weekdays(airquality$Date)\nprint(head(airquality))\n#>   Ozone Solar.R Wind Temp Month Day       Date   WeekDay\n#> 1    41     190  7.4   67     5   1 1973-05-01   Tuesday\n#> 2    36     118  8.0   72     5   2 1973-05-02 Wednesday\n#> 3    12     149 12.6   74     5   3 1973-05-03  Thursday\n#> 4    18     313 11.5   62     5   4 1973-05-04    Friday\n#> 5    NA      NA 14.3   56     5   5 1973-05-05  Saturday\n#> 6    28      NA 14.9   66     5   6 1973-05-06    Sunday\ndate_diff <- difftime(max(airquality$Date), min(airquality$Date), units = \"days\")\nprint(paste(\"Days between first and last measurement:\", date_diff))\n#> [1] \"Days between first and last measurement: 152\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"working-with-dates-and-times","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"0.5 Working with dates and times","text":"","code":""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"create-and-format-dates-1","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"0.6 Create and format dates","text":"create Date object simple character string R, can use .Date() function. character string obey format can defined using set symbols (examples correspond 13 January, 1982):%Y: 4-digit year (1982)\n%y: 2-digit year (82)\n%m: 2-digit month (01)\n%d: 2-digit day month (13)\n%: weekday (Wednesday)\n%: abbreviated weekday (Wed)\n%B: month (January)\n%b: abbreviated month (Jan)information full list use ?strptime","code":""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"as.date","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"0.7 as.Date()","text":"Dates often stored integers.Convert integers dates speciying origin (Day 0).example: SAS stores dates number days elapsed since 1 Jan 1960.","code":"\nas.Date('2019-06-05',format = '%Y-%m-%d')\n#> [1] \"2019-06-05\"\nas.Date(21705, origin = '1960-01-01')\n#> [1] \"2019-06-05\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"exercise-1-8","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"Exercise 1","text":"Work policy_data data set.\n1. Convert start date (debut_pol) end date (fin_pol) R Date objects.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\npolicy_data <- read.csv(file = 'John Jay Workshop Data/PolicyData.csv', sep = ';')\npolicy_data$start <- as.Date(policy_data$debut_pol, '%d/%m/%Y')\npolicy_data$end <- as.Date(policy_data$fin_pol, '%d/%m/%Y')\nhead(policy_data %>% select(c('debut_pol', 'start')))\n#>    debut_pol      start\n#> 1 14/09/1995 1995-09-14\n#> 2 25/04/1996 1996-04-25\n#> 3  1/03/1995 1995-03-01\n#> 4  1/03/1996 1996-03-01\n#> 5 15/01/1997 1997-01-15\n#> 6  1/02/1997 1997-02-01"},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"format","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"format()","text":"Calculate duration contract.can add subtract integers dates.","code":"\ntoday <- as.Date('2019-06-05',\n                format = '%Y-%m-%d')\nformat(today, '%A %d %B %Y')\n#> [1] \"Wednesday 05 June 2019\"\npolicy_duration =\n  policy_data$end - policy_data$start\ntomorrow = today + 1\nprint(tomorrow)\n#> [1] \"2019-06-06\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"lubridate","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"Lubridate","text":"","code":""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"access-date-components","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"0.8 Access date components","text":"components : month(), day(), quarter(), …","code":"\n# install.packages(\"lubridate\")\nlibrary(lubridate)\n#> \n#> Attaching package: 'lubridate'\n#> The following objects are masked from 'package:base':\n#> \n#>     date, intersect, setdiff, union\nyear(today)\n#> [1] 2019"},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"advanced-math","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"0.9 Advanced math","text":"periods : years() days().floor_date rounds nearest unit.example convert daily monthly data.","code":"\ntoday + months(3)\n#> [1] \"2019-09-05\"\nfloor_date(today, unit = \"month\")\n#> [1] \"2019-06-01\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"seq","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"seq()","text":"","code":"\nseq(from = as.Date('2019-01-01'),\n    to = as.Date('2019-12-31'),\n    by = '1 month')\n#>  [1] \"2019-01-01\" \"2019-02-01\" \"2019-03-01\" \"2019-04-01\"\n#>  [5] \"2019-05-01\" \"2019-06-01\" \"2019-07-01\" \"2019-08-01\"\n#>  [9] \"2019-09-01\" \"2019-10-01\" \"2019-11-01\" \"2019-12-01\""},{"path":"optional-part-v-working-with-dates-and-times-20-minutes.html","id":"exercise-2-7","chapter":"(Optional) Part V: Working with Dates and Times (20 minutes)","heading":"Exercise 2","text":"Visualize exposure contribution start month contract policy_data data set.\n1. Add covariate start_month data set. 2. Group data start_month.\n3. Calculate exposure within group.\n4. Plot data.","code":""},{"path":"workshop-1-suplementary-exercises.html","id":"workshop-1-suplementary-exercises","chapter":"Workshop 1: Suplementary Exercises","heading":"Workshop 1: Suplementary Exercises","text":"exercises https://intro2r.com/links explanation found , additionally, can read book chapters interested detailed explanation relates closely exercises. Now practice writing code script editor sourcing code R console. Let’s display help file function mean. script type help('mean') source code console. Notice help file displayed bottom right window (click ‘Help’ tab). Examine different components help file (especially examples section end help file). See Section 2.5 Introduction R book details using help functions. content displayed bottom right window context dependent. example write code plot(1:10) script source R console bottom right window display plot (don’t worry understanding R code right now, hopefully become clear later course!). Next, let’s practice creating variable assigning value variable. Take look Section 2.2 Introduction R book information prefer watch Objects R video. Create variable called first_num assign value 42. Click ‘Environment’ tab top right window display variable value. Now create another variable called first_char assign value \"first character\". Notice variable now also displayed ‘Environment’ along ’s value class (chr - short character class). Remove variable first_num environment using rm() function. Use code rm(first_num) . Check ‘Environment’ tab ensure variable removed. Alternatively, use ls() function list objects environment. Let’s see happens assign another value existing variable. Assign value \"second character\" variable first_char created Q6. Notice value changed ‘Environment’. display value first.char enter name variable console. Don’t forget save R script periodically! OK, let’s leave RStudio minute. Using favourite web browser, navigate R-project website explore links catch eye. Make sure find R manuals page user contributed documents section. Download manuals think might find useful (listed course manual) save computer (USB drive). Click ‘Search’ link R-Project website. Use ‘Rseek’ search term ‘mixed model p values’ (controversial subject!) explore anything looks interesting. Also experiment ‘R site search’ ‘Nabble R Forum’ links. Learning search help run problem using R acquired skill something get better time. One note caution, often find many different solutions solving problem R, written experienced R users others people less experience. Whichever solution choose make sure understand code thoroughly test make sure ’s want. OK, back RStudio. Sometimes may forget exact name function want use useful able search function names. example, want create design plot can remember name function word ‘plot’ . Use apropos() function list functions word plot name (see Section 2.5.1 Introduction R book). Look list figured correct function bring help file function (Hint: function name probably words ‘plot’ ‘design’ !). Another strategy use help.search() function search R’s help files. Search R help system instances character string ‘plot’. Take look Section 2.5.1 information. Also, see can figure narrow search searching ‘plot’ nlme package (hint: see help page help.search()). R’s working directory default location files read R, export R. Although won’t importing exporting files just yet (’s tomorrows job) ’s useful able determine current working directory . , read Section 1.7 Introduction R book introduce working directories figure display current working directory. ##Basic R operations {-}\nRead Chapter 2 help complete questions exercise. \n11. Let’s use R fancy calculator. Find natural log, log base 10, log base 2, square root natural antilog 12.43. See Section 2.1 Introduction R book information mathematical functions R. Don’t forget write code RStudio’s script editor source code console. Next, use R determine area circle diameter 20 cm assign result object called area_circle. can’t remember create assign objects see Section 2.2 watch video. Google friend can’t remember formula calculate area circle! Also, remember R already knows pi. Don’t worry ’re stumped feel free ask one instructors guidance. Now something little tricky. Calculate cube root 14 x 0.51. might need think creatively solution (hint: think exponents), remember R follows usual order mathematical operators might need use brackets code (see page ’ve never heard ). point question torture maths (please don’t stress!), get used writing mathematical equations R highlight order operations. Ok, ’re now ready explore one R’s basic (useful) data structures - vectors. vector sequence elements (components) data type (see Section 3.2.1 introduction vectors). Although technically correct might useful think vector something like single column spreadsheet. multitude ways create vectors R use concatenate function c() create vector called weight containing weight (kg) 10 children: 69, 62, 57, 59, 59, 64, 56, 66, 67, 66 (Section 2.3 watch video information). Now can useful stuff weight vector. Get R calculate mean, variance, standard deviation, range weights number children weight vector (see Section 2.3 details). Now read Section 2.4 R book learn work vectors. reading section able extract weights first five children using Positional indexes store weights new variable called first_five. Remember, need use square brackets [ ] extract (aka index, subset) elements variable. ’re now going use c() function create another vector called height containing height (cm) 10 children: 112, 102, 83, 84, 99, 90, 77, 112, 133, 112. Use summary() function summarise data height object. Extract height 2nd, 3rd, 9th 10th child assign heights variable called some_child (take look section Positional indexes R book ’re stuck). can also extract elements using Logical indexes. Let’s extract heights children less equal 99 cm assign variable called shorter_child. Now can use information weight height variables calculate body mass index (BMI) child. BMI calculated weight (kg) divided square height (meters). Store results calculation variable called bmi. Note: don’t need calculation child individually, can use vectors BMI equation – called vectorisation (see Section 2.4.4 Introduction R book). Now let’s practice useful skill - creating sequences (honestly …). Take look Section 2.3 R book (bit creating sequences) see myriad ways can create sequences R. Let’s use seq() function create sequence numbers ranging 0 1 steps 0.1 (also vector way) assign sequence variable called seq1. Next, see can figure create sequence 10 1 steps 0.5. Assign sequence variable called seq2 (Hint: may find useful include rev() function code). Let’s go sequence crazy! Generate following sequences. need experiment arguments rep() function generate sequences (see Section 2.3 clues):1 2 3 1 2 3 1 2 3“” “” “” “c” “c” “c” “e” “e” “e” “g” “g” “g”“” “c” “e” “g” “” “c” “e” “g” “” “c” “e” “g”1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 31 1 1 1 1 2 2 2 2 3 3 3 4 4 57 7 7 7 2 2 2 8 1 1 1 1 1 Ok, back variable height created Q7. Let’s sort values height ascending order (shortest tallest) assign sorted vector new variable called height_sorted. Take look Section 2.4.3 R book see . Now sort heights descending order assign new vector name choice. Let’s give children names. Create new vector called child_name following names 10 children: \"Alfred\", \"Barbara\", \"James\", \"Jane\", \"John\", \"Judy\", \"Louise\", \"Mary\", \"Ronald\", \"William\". really useful (common) task order values one variable order another variable. need use order() function combination square bracket notation [ ]. peep Section 2.4.3 details. Create new variable called names_sort store names children ordered child height (shortest tallest). shortest? tallest child? ’re sure , please ask one instructors. Now order names children descending values weight assign result variable called weight_rev (Hint: perhaps include rev() function?). heaviest? lightest? Finally, list variables workspace created exercise. Remove variable seq1 workspace using rm() function. ","code":""},{"path":"part-i-advanced-data-manipulation-with-dplyr-30-minutes.html","id":"part-i-advanced-data-manipulation-with-dplyr-30-minutes","chapter":"Part I: Advanced Data Manipulation with dplyr (30 minutes)","heading":"Part I: Advanced Data Manipulation with dplyr (30 minutes)","text":"","code":""},{"path":"part-i-advanced-data-manipulation-with-dplyr-30-minutes.html","id":"grouping-and-summarizing-data","chapter":"Part I: Advanced Data Manipulation with dplyr (30 minutes)","heading":"Grouping and summarizing data","text":"Exercise:Group ‘mtcars’ dataset ‘gear’ calculate average horsepower (‘hp’) gear group.","code":"\n# Loading the dplyr package\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n# Using the 'mtcars' dataset\ndata(mtcars)\n\n# Example: Grouping by 'cyl' (number of cylinders) and calculating mean mpg (miles per gallon)\ngrouped_data <- mtcars %>%\n  group_by(cyl) %>%\n  summarize(mean_mpg = mean(mpg))\nprint(grouped_data)\n#> # A tibble: 3 × 2\n#>     cyl mean_mpg\n#>   <dbl>    <dbl>\n#> 1     4     26.7\n#> 2     6     19.7\n#> 3     8     15.1"},{"path":"part-i-advanced-data-manipulation-with-dplyr-30-minutes.html","id":"joining-and-merging-datasets","chapter":"Part I: Advanced Data Manipulation with dplyr (30 minutes)","heading":"Joining and merging datasets","text":"Exercise:Create new dataframe subset columns ‘iris’ merge original ‘iris’ dataset based common column.","code":"\n# Creating a sample dataset to join with 'mtcars'\ncar_names <- data.frame(model = rownames(mtcars), car_type = rep(c(\"Type A\", \"Type B\", \"Type C\"), length.out = nrow(mtcars)))\n\n# Converting row names of 'mtcars' to a column\nmtcars$model <- rownames(mtcars)\n\n# Example: Joining 'mtcars' and 'car_names'\njoined_data <- left_join(mtcars, car_names, by = \"model\")\nprint(head(joined_data))\n#>    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n#>               model car_type\n#> 1         Mazda RX4   Type A\n#> 2     Mazda RX4 Wag   Type B\n#> 3        Datsun 710   Type C\n#> 4    Hornet 4 Drive   Type A\n#> 5 Hornet Sportabout   Type B\n#> 6           Valiant   Type C"},{"path":"part-ii-text-data-processing-30-minutes.html","id":"part-ii-text-data-processing-30-minutes","chapter":"Part II: Text Data Processing (30 minutes)","heading":"Part II: Text Data Processing (30 minutes)","text":"","code":""},{"path":"part-ii-text-data-processing-30-minutes.html","id":"manipulating-and-analyzing-text-data-using-regular-expressions","chapter":"Part II: Text Data Processing (30 minutes)","heading":"Manipulating and analyzing text data using regular expressions","text":"Exercise:Write regular expression find words starting ‘b’ given text.","code":"\n# --- Part II: Text Data Processing (30 minutes) ---\n\n# Load necessary libraries\nlibrary(stringr)\n\n## Manipulating and analyzing text data using regular expressions\n\n# Example: Extracting email addresses from a string\ntext <- \"Contact us at support@example.com or feedback@example.net\"\nemails <- str_extract_all(text, \"[[:alnum:]_.]+@[[:alnum:]]+\\\\.[[:alpha:]]{2,}\")\nprint(emails)\n#> [[1]]\n#> [1] \"support@example.com\"  \"feedback@example.net\""},{"path":"part-ii-text-data-processing-30-minutes.html","id":"text-mining-basics","chapter":"Part II: Text Data Processing (30 minutes)","heading":"Text mining basics","text":"Exercise:Create corpus text data compute term frequency-inverse document frequency (tf-idf) matrix.","code":"\n\n# Load the 'tm' package for text mining\nlibrary(tm)\n#> Loading required package: NLP\n\n# Example: Basic text mining with a simple corpus\ndocs <- Corpus(VectorSource(c(\"Text mining is awesome\", \"R is a versatile tool for text analysis\")))\ndtm <- DocumentTermMatrix(docs)\ninspect(dtm)\n#> <<DocumentTermMatrix (documents: 2, terms: 7)>>\n#> Non-/sparse entries: 8/6\n#> Sparsity           : 43%\n#> Maximal term length: 9\n#> Weighting          : term frequency (tf)\n#> Sample             :\n#>     Terms\n#> Docs analysis awesome for mining text tool versatile\n#>    1        0       1   0      1    1    0         0\n#>    2        1       0   1      0    1    1         1"},{"path":"part-iii-building-predictive-models-30-minutes.html","id":"part-iii-building-predictive-models-30-minutes","chapter":"Part III: Building Predictive Models (30 minutes)","heading":"Part III: Building Predictive Models (30 minutes)","text":"","code":""},{"path":"part-iii-building-predictive-models-30-minutes.html","id":"introduction-to-machine-learning-in-r","chapter":"Part III: Building Predictive Models (30 minutes)","heading":"Introduction to machine learning in R","text":"Brief overview machine learning: Machine learning R involves using statistical techniques enable computers improve tasks experience. encompasses variety techniques classification, regression, clustering, .Exercise:Load different dataset partition training testing sets.","code":"\n# Load necessary libraries\n#install.packages(\"caret\")\nlibrary(caret)\n#> Loading required package: ggplot2\n#> Loading required package: lattice\n\n\n# Example: Splitting a dataset into training and testing sets\ndata(iris)\nset.seed(123) # Setting seed for reproducibility\ntrainingIndex <- createDataPartition(iris$Species, p = 0.8, list = FALSE)\ntrainingData <- iris[trainingIndex, ]\ntestingData <- iris[-trainingIndex, ]"},{"path":"part-iii-building-predictive-models-30-minutes.html","id":"creating-predictive-models-with-caret","chapter":"Part III: Building Predictive Models (30 minutes)","heading":"Creating predictive models with caret","text":"Exercise:\n2. Build predictive model another dataset evaluate performance.basic structure Shiny app involves two main parts:user interface (UI) script, controls layout appearance app.server script, contains instructions build rebuild app based user input.","code":"\n\n# Example: Building a predictive model for the iris dataset\nmodel <- train(Species ~ ., data = trainingData, method = \"rpart\")\nprint(model)\n#> CART \n#> \n#> 120 samples\n#>   4 predictor\n#>   3 classes: 'setosa', 'versicolor', 'virginica' \n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (25 reps) \n#> Summary of sample sizes: 120, 120, 120, 120, 120, 120, ... \n#> Resampling results across tuning parameters:\n#> \n#>   cp    Accuracy   Kappa    \n#>   0.00  0.9398492  0.9086993\n#>   0.45  0.7426390  0.6253355\n#>   0.50  0.5557896  0.3665192\n#> \n#> Accuracy was used to select the optimal model using\n#>  the largest value.\n#> The final value used for the model was cp = 0.\n\n# Predicting using the model\npredictions <- predict(model, testingData)\nconfusionMatrix(predictions, testingData$Species)\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         10          0         0\n#>   versicolor      0         10         2\n#>   virginica       0          0         8\n#> \n#> Overall Statistics\n#>                                           \n#>                Accuracy : 0.9333          \n#>                  95% CI : (0.7793, 0.9918)\n#>     No Information Rate : 0.3333          \n#>     P-Value [Acc > NIR] : 8.747e-12       \n#>                                           \n#>                   Kappa : 0.9             \n#>                                           \n#>  Mcnemar's Test P-Value : NA              \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor\n#> Sensitivity                 1.0000            1.0000\n#> Specificity                 1.0000            0.9000\n#> Pos Pred Value              1.0000            0.8333\n#> Neg Pred Value              1.0000            1.0000\n#> Prevalence                  0.3333            0.3333\n#> Detection Rate              0.3333            0.3333\n#> Detection Prevalence        0.3333            0.4000\n#> Balanced Accuracy           1.0000            0.9500\n#>                      Class: virginica\n#> Sensitivity                    0.8000\n#> Specificity                    1.0000\n#> Pos Pred Value                 1.0000\n#> Neg Pred Value                 0.9091\n#> Prevalence                     0.3333\n#> Detection Rate                 0.2667\n#> Detection Prevalence           0.2667\n#> Balanced Accuracy              0.9000\n\n<!--chapter:end:13-Advanced-R-Part3.Rmd-->\n\n# Part IV: Interactive Dashboards with Shiny (30 minutes) {-}\n## Introduction to Shiny for building web-based data dashboards {-}\n\nShiny is an R package that makes it easy to build interactive web applications (apps) straight from R. It allows you to turn analyses into interactive web applications without requiring HTML, CSS, or JavaScript knowledge.\n\n\n```r\n# Load the Shiny package\n#install.packages(\"shiny\")\nlibrary(shiny)"},{"path":"part-iii-building-predictive-models-30-minutes.html","id":"creating-a-simple-shiny-app","chapter":"Part III: Building Predictive Models (30 minutes)","heading":"Creating a simple Shiny app","text":"UI Component: UI sliderInput selecting mpg range tableOutput display filtered data.Server Logic: reactive function creates reactive subset mtcars based selected mpg range. renderTable function renders filtered data table main panel.Running App: Shiny app, shinyApp(ui = ui, server = server) runs app.Exercise:Modify example Shiny app include dataset choice create different type plot.Modify example Shiny app include dataset choice create different type plot.Add additional input options, like checkboxes dropdown menus, manipulate plot.Add additional input options, like checkboxes dropdown menus, manipulate plot.","code":"\n\n# Example: A simple Shiny app for displaying a plot\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"Simple Shiny App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"num\", \"Number of bins:\", \n                  min = 1, max = 50, value = 30)\n    ),\n    mainPanel(\n       plotOutput(\"distPlot\")\n    )\n  )\n)\n\n# Define server logic\nserver <- function(input, output) {\n  output$distPlot <- renderPlot({\n    x <- faithful$eruptions\n    bins <- seq(min(x), max(x), length.out = input$num + 1)\n    hist(x, breaks = bins, col = 'darkgray', border = 'white')\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n# Define UI\nui <- fluidPage(\n  titlePanel(\"Data Filtering App\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"mpgRange\", \"Miles per Gallon (mpg):\",\n                  min = min(mtcars$mpg), max = max(mtcars$mpg),\n                  value = c(min(mtcars$mpg), max(mtcars$mpg))\n      )\n    ),\n    mainPanel(\n      tableOutput(\"filteredData\")\n    )\n  )\n)\n\n# Define server logic\nserver <- function(input, output) {\n  filteredData <- reactive({\n    mtcars[mtcars$mpg >= input$mpgRange[1] & mtcars$mpg <= input$mpgRange[2], ]\n  })\n\n  output$filteredData <- renderTable({\n    filteredData()\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"},{"path":"workshop-1.html","id":"workshop-1","chapter":"Workshop 1","heading":"Workshop 1","text":"","code":""},{"path":"workshop-1.html","id":"exercise-solutions","chapter":"Workshop 1","heading":"Exercise solutions","text":"","code":""},{"path":"workshop-1.html","id":"exercise-1-9","chapter":"Workshop 1","heading":"Exercise 1","text":"Run following code, use typeof(), class() functions find data type /class object.","code":"\nmy_numeric <- 42.5\nJohn_jay <- \"university\"\nmy_logical <- TRUE\nmy_date <- as.Date(\"05/29/2018\", \"%m/%d/%Y\")\n# for date\ntypeof(my_date)\n#> [1] \"double\"\nclass(my_date)\n#> [1] \"Date\"\n# for numeric\ntypeof(my_numeric)\n#> [1] \"double\"\nclass(my_numeric)\n#> [1] \"numeric\"\n# for char\ntypeof(John_jay)\n#> [1] \"character\"\nclass(John_jay)\n#> [1] \"character\"\n#for logical\ntypeof(my_logical)\n#> [1] \"logical\"\nclass(my_logical)\n#> [1] \"logical\""},{"path":"workshop-1.html","id":"exercise-2-8","chapter":"Workshop 1","heading":"Exercise 2","text":"Create 1 datatype : Character, numeric, integer, complex, BooleanThe answers may vary . example solution,","code":"\n\nBest_university_in_nyc <- \"John Jay\"\nBest_university_in_nyc\n#> [1] \"John Jay\"\nMy_gpa <- 3.78\nMy_gpa\n#> [1] 3.78\nMy_int_gpa <- as.integer(My_gpa)\nMy_int_gpa\n#> [1] 3\nmy_complex_gpa<- 3.78+2i\nmy_complex_gpa\n#> [1] 3.78+2i\ndo_I_like_chocolate_ice_cream <- FALSE\ndo_I_like_chocolate_ice_cream\n#> [1] FALSE\n\nmy_elements =list(Best_university_in_nyc,My_gpa,My_int_gpa,my_complex_gpa,do_I_like_chocolate_ice_cream)\n\n# Check the classes of each element\nfor (element in my_elements) {\n  print(class(element))\n}\n#> [1] \"character\"\n#> [1] \"numeric\"\n#> [1] \"integer\"\n#> [1] \"complex\"\n#> [1] \"logical\""},{"path":"workshop-1.html","id":"part-iii","chapter":"Workshop 1","heading":"Part III","text":"","code":""},{"path":"workshop-1.html","id":"exercise-1-10","chapter":"Workshop 1","heading":"Exercise 1","text":"","code":""},{"path":"workshop-1.html","id":"exercise-1-11","chapter":"Workshop 1","heading":"Exercise 1:","text":"Create vector favorite numbers.Access third element vector.Create new vector square element original vector.Create vector favorite numbers.Access third element vector.Note R starts indexing 1. somewhat natural since start counting 1 . However, programming languages start indexing 0, , access third element my_favorite_numbers[2] language like python .Create new vector square element original vector.Inspect my_vector using:\nattributes(), length() str() function","code":"\nmy_favorite_numbers <- c(7,22,17,19)\nmy_favorite_numbers[3]\n#> [1] 17\nsquare_favorite_numbers<- my_favorite_numbers^2\nsquare_favorite_numbers\n#> [1]  49 484 289 361\nmy_vector <- c(\"Dilan Caro\", \"Instructor\")\nnames(my_vector) <- c(\"Name\", \"Profession\")\nmy_vector\n#>         Name   Profession \n#> \"Dilan Caro\" \"Instructor\"\nattributes(my_vector)\n#> $names\n#> [1] \"Name\"       \"Profession\"\nlength(my_vector)\n#> [1] 2\nnames(my_vector)\n#> [1] \"Name\"       \"Profession\""},{"path":"workshop-1.html","id":"exercise-2-9","chapter":"Workshop 1","heading":"Exercise 2","text":"Create data frame least three columns four rows.Print number rows columns data frame.Display summary statistics data frame.","code":""},{"path":"workshop-1.html","id":"create-a-data-frame-with-at-least-three-columns-and-four-rows.","chapter":"Workshop 1","heading":"Create a data frame with at least three columns and four rows.","text":"","code":"\ndf <- data.frame(\n  Subject = c(\"Art\", \"Bayesian\", \"Machine learning\", \"Stochastic\"),\n  Grade =c(100,87,90,75),\n  Difficulty =c(6,9,8,10)# from 0 to 5 , 5 being the most difficuly\n  )\nprint(df)\n#>            Subject Grade Difficulty\n#> 1              Art   100          6\n#> 2         Bayesian    87          9\n#> 3 Machine learning    90          8\n#> 4       Stochastic    75         10"},{"path":"workshop-1.html","id":"print-the-number-of-rows-and-columns-of-your-data-frame.","chapter":"Workshop 1","heading":"Print the number of rows and columns of your data frame.","text":"shows 4 rows, 3 columns","code":"\nprint(dim(df)) \n#> [1] 4 3"},{"path":"workshop-1.html","id":"display-summary-statistics-of-your-data-frame.","chapter":"Workshop 1","heading":"Display summary statistics of your data frame.","text":"","code":"\nprint(summary(df))\n#>    Subject              Grade         Difficulty   \n#>  Length:4           Min.   : 75.0   Min.   : 6.00  \n#>  Class :character   1st Qu.: 84.0   1st Qu.: 7.50  \n#>  Mode  :character   Median : 88.5   Median : 8.50  \n#>                     Mean   : 88.0   Mean   : 8.25  \n#>                     3rd Qu.: 92.5   3rd Qu.: 9.25  \n#>                     Max.   :100.0   Max.   :10.00"},{"path":"workshop-1.html","id":"exercise-3-3","chapter":"Workshop 1","heading":"Exercise 3","text":"Inspect built-data frameGet summary variable dataframeNow inspect tibble","code":"\nmtcars\n#>                      mpg cyl  disp  hp drat    wt  qsec vs\n#> Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0\n#> Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0\n#> Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1\n#> Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1\n#> Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0\n#> Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1\n#> Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0\n#> Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1\n#> Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1\n#> Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1\n#> Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1\n#> Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0\n#> Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0\n#> Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0\n#> Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0\n#> Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0\n#> Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0\n#> Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1\n#> Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1\n#> Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1\n#> Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1\n#> Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0\n#> AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0\n#> Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0\n#> Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0\n#> Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1\n#> Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0\n#> Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1\n#> Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0\n#> Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0\n#> Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0\n#> Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1\n#>                     am gear carb\n#> Mazda RX4            1    4    4\n#> Mazda RX4 Wag        1    4    4\n#> Datsun 710           1    4    1\n#> Hornet 4 Drive       0    3    1\n#> Hornet Sportabout    0    3    2\n#> Valiant              0    3    1\n#> Duster 360           0    3    4\n#> Merc 240D            0    4    2\n#> Merc 230             0    4    2\n#> Merc 280             0    4    4\n#> Merc 280C            0    4    4\n#> Merc 450SE           0    3    3\n#> Merc 450SL           0    3    3\n#> Merc 450SLC          0    3    3\n#> Cadillac Fleetwood   0    3    4\n#> Lincoln Continental  0    3    4\n#> Chrysler Imperial    0    3    4\n#> Fiat 128             1    4    1\n#> Honda Civic          1    4    2\n#> Toyota Corolla       1    4    1\n#> Toyota Corona        0    3    1\n#> Dodge Challenger     0    3    2\n#> AMC Javelin          0    3    2\n#> Camaro Z28           0    3    4\n#> Pontiac Firebird     0    3    2\n#> Fiat X1-9            1    4    1\n#> Porsche 914-2        1    5    2\n#> Lotus Europa         1    5    2\n#> Ford Pantera L       1    5    4\n#> Ferrari Dino         1    5    6\n#> Maserati Bora        1    5    8\n#> Volvo 142E           1    4    2\nstr(mtcars)\n#> 'data.frame':    32 obs. of  11 variables:\n#>  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n#>  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n#>  $ disp: num  160 160 108 258 360 ...\n#>  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n#>  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n#>  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n#>  $ qsec: num  16.5 17 18.6 19.4 17 ...\n#>  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n#>  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n#>  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n#>  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\nhead(mtcars)\n#>                    mpg cyl disp  hp drat    wt  qsec vs am\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0\n#>                   gear carb\n#> Mazda RX4            4    4\n#> Mazda RX4 Wag        4    4\n#> Datsun 710           4    1\n#> Hornet 4 Drive       3    1\n#> Hornet Sportabout    3    2\n#> Valiant              3    1\nsummary(mtcars$cyl) # use $ to extract variable from a data frame\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   4.000   4.000   6.000   6.188   8.000   8.000\nlibrary(ggplot2)\ndiamonds\n#> # A tibble: 53,940 × 10\n#>    carat cut     color clarity depth table price     x     y\n#>    <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#>  1  0.23 Ideal   E     SI2      61.5    55   326  3.95  3.98\n#>  2  0.21 Premium E     SI1      59.8    61   326  3.89  3.84\n#>  3  0.23 Good    E     VS1      56.9    65   327  4.05  4.07\n#>  4  0.29 Premium I     VS2      62.4    58   334  4.2   4.23\n#>  5  0.31 Good    J     SI2      63.3    58   335  4.34  4.35\n#>  6  0.24 Very G… J     VVS2     62.8    57   336  3.94  3.96\n#>  7  0.24 Very G… I     VVS1     62.3    57   336  3.95  3.98\n#>  8  0.26 Very G… H     SI1      61.9    55   337  4.07  4.11\n#>  9  0.22 Fair    E     VS2      65.1    61   337  3.87  3.78\n#> 10  0.23 Very G… H     VS1      59.4    61   338  4     4.05\n#> # ℹ 53,930 more rows\n#> # ℹ 1 more variable: z <dbl>\nstr(diamonds)  # built-in in library ggplot2\n#> tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n#>  $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n#>  $ cut    : Ord.factor w/ 5 levels \"Fair\"<\"Good\"<..: 5 4 2 4 2 3 3 3 1 3 ...\n#>  $ color  : Ord.factor w/ 7 levels \"D\"<\"E\"<\"F\"<\"G\"<..: 2 2 2 6 7 7 6 5 2 5 ...\n#>  $ clarity: Ord.factor w/ 8 levels \"I1\"<\"SI2\"<\"SI1\"<..: 2 3 5 4 2 6 7 3 4 5 ...\n#>  $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n#>  $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n#>  $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n#>  $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n#>  $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n#>  $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\nhead(diamonds)\n#> # A tibble: 6 × 10\n#>   carat cut      color clarity depth table price     x     y\n#>   <dbl> <ord>    <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl>\n#> 1  0.23 Ideal    E     SI2      61.5    55   326  3.95  3.98\n#> 2  0.21 Premium  E     SI1      59.8    61   326  3.89  3.84\n#> 3  0.23 Good     E     VS1      56.9    65   327  4.05  4.07\n#> 4  0.29 Premium  I     VS2      62.4    58   334  4.2   4.23\n#> 5  0.31 Good     J     SI2      63.3    58   335  4.34  4.35\n#> 6  0.24 Very Go… J     VVS2     62.8    57   336  3.94  3.96\n#> # ℹ 1 more variable: z <dbl>\nsummary(diamonds$depth)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   43.00   61.00   61.80   61.75   62.50   79.00"},{"path":"workshop-1.html","id":"exercise-4-4","chapter":"Workshop 1","heading":"Exercise 4","text":"Create vector fav_music names favorite artists.Create vector num_records number records \ncollection artists.Create vector num_concerts number times attended concert artists.Put everything together data frame, assign name my_music data frame change labels information stored columns artist, records concerts.Extract variable num_records data frame my_music.Calculate total number records collection (defined\nset artists).Check structure data frame, ask summary.","code":"\nfav_music <- c(\"Prince\", \"REM\", \"Ryan Adams\", \"BLOF\")\nnum_concerts <- c(0, 3, 1, 0)\nnum_records <- c(2, 7, 5, 1)\nmy_music <- data.frame(fav_music, num_concerts, num_records)\nnames(my_music) <- c(\"artist\", \"concerts\", \"records\")\nsummary(my_music)\n#>     artist             concerts      records    \n#>  Length:4           Min.   :0.0   Min.   :1.00  \n#>  Class :character   1st Qu.:0.0   1st Qu.:1.75  \n#>  Mode  :character   Median :0.5   Median :3.50  \n#>                     Mean   :1.0   Mean   :3.75  \n#>                     3rd Qu.:1.5   3rd Qu.:5.50  \n#>                     Max.   :3.0   Max.   :7.00\nmy_music$records\n#> [1] 2 7 5 1\nsum(my_music$records)\n#> [1] 15"},{"path":"workshop-1.html","id":"import-other-data-formats-1","chapter":"Workshop 1","heading":"Import other data formats","text":"haven package enables R read write various data formats used statistical packages.supports:SAS: read_sas() reads .sas7bdat .sas7bcat files read_xpt() reads SAS transport files. write_sas() writes .sas7bdat files.SPSS: read_sav() reads .sav files read_por() reads older .por files. write_sav() writes .sav files.Stata: read_dta() reads .dta files. write_dta() writes .dta files.","code":""},{"path":"workshop-1.html","id":"exercise-5-4","chapter":"Workshop 1","heading":"Exercise 5","text":"Load following data sets, available course material:\n- Danish fire insurance losses, stored danish.txt\n- severity data set, stored severity.sas7bdat.","code":"\npath <- file.path('/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data/')\npath.danish <- file.path(path, \"danish.txt\")\ndanish <- read.table(path.danish, header = TRUE)\ndanish$Date <- as.Date(danish$Date, \"%m/%d/%Y\")\nstr(danish)\n#> 'data.frame':    2167 obs. of  2 variables:\n#>  $ Date       : Date, format: \"1980-01-03\" ...\n#>  $ Loss.in.DKM: num  1.68 2.09 1.73 1.78 4.61 ...\nlibrary(haven)\nseverity <- read_sas('/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data/severity.sas7bdat')\nstr(severity)\n#> tibble [19,287 × 5] (S3: tbl_df/tbl/data.frame)\n#>  $ policyId   : num [1:19287] 6e+05 6e+05 6e+05 6e+05 6e+05 ...\n#>   ..- attr(*, \"format.sas\")= chr \"BEST\"\n#>  $ claimId    : num [1:19287] 9e+05 9e+05 9e+05 9e+05 9e+05 ...\n#>   ..- attr(*, \"format.sas\")= chr \"BEST\"\n#>  $ rc         : num [1:19287] 35306 19773 41639 10649 20479 ...\n#>   ..- attr(*, \"format.sas\")= chr \"BEST\"\n#>  $ deductible : num [1:19287] 1200 50 100 50 50 50 50 50 50 50 ...\n#>   ..- attr(*, \"format.sas\")= chr \"BEST\"\n#>  $ claimAmount: num [1:19287] 35306 19773 41639 10649 20479 ...\n#>   ..- attr(*, \"format.sas\")= chr \"COMMA\""},{"path":"workshop-1.html","id":"part-iv","chapter":"Workshop 1","heading":"Part IV","text":"","code":""},{"path":"workshop-1.html","id":"exercises-1","chapter":"Workshop 1","heading":"Exercises","text":"Subsetting Data FramesCreate data frame named student_info following columns data:\n- student_id (1 5)\n- student_name (‘Alice’, ‘Bob’, ‘Charlie’, ‘David’, ‘Eva’)\n- student_age (25, 30, 22, 28, 24)\n- student_grade (‘’, ‘B’, ‘’, ‘C’, ‘B’)Write command subset data frame include students older 24.Using Conditional FiltersUse subset() function find students grade ‘’.Display names ages students.Manipulating Data dplyrLoad dplyr package convert student_info tibble.Load dplyr package convert student_info tibble.Use filter() select() show name age students grade better ‘B’.Use filter() select() show name age students grade better ‘B’.Adding Removing ColumnsAdd new column student_major values (‘Math’, ‘Science’, ‘Arts’, ‘Math’, ‘Science’) student_info., remove student_grade column using dplyr.Renaming ColumnsRename student_name column name using base R functions using dplyr.Complex dplyr OperationsCreate new tibble student_info includes students except studying ‘Arts’, rename student_id column id, arrange students age descending order.Exploratory Data Analysis dplyrCalculate average age students grouped major using group_by() summarize() dplyr.","code":"\n# Exercise 1: Subsetting Data Frames\nstudent_info <- data.frame(\n  student_id = 1:5,\n  student_name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  student_age = c(25, 30, 22, 28, 24),\n  student_grade = c('A', 'B', 'A', 'C', 'B')\n)\nolder_students <- subset(student_info, student_age > 24)\n\n# Exercise 2: Using Conditional Filters\n\ngrade_a_students <- subset(student_info, student_grade == 'A', select = c(student_name, student_age))\n# Exercise 3: Manipulating Data with dplyr\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nstudent_info <- as_tibble(student_info)\ngood_students <- student_info %>% filter(student_grade > 'B') %>% select(student_name, student_age)\n# Exercise 4: Adding and Removing Columns\nstudent_info <- mutate(student_info, student_major = c('Math', 'Science', 'Arts', 'Math', 'Science'))\nstudent_info <- select(student_info, -student_grade)\n# Exercise 5: Renaming Columns\nstudent_info <- data.frame(\n  student_id = 1:5,\n  student_name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  student_age = c(25, 30, 22, 28, 24),\n  student_grade = c('A', 'B', 'A', 'C', 'B')\n)\n\nnames(student_info)[names(student_info) == \"student_name\"] <- \"name\"\n\nstudent_info <- data.frame(\n  student_id = 1:5,\n  student_name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  student_age = c(25, 30, 22, 28, 24),\n  student_grade = c('A', 'B', 'A', 'C', 'B')\n)\nstudent_info <- rename(student_info, name = student_name)\nstudent_info <- data.frame(\n  student_id = 1:5,\n  student_name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  student_age = c(25, 30, 22, 28, 24),\n  student_grade = c('A', 'B', 'A', 'C', 'B')\n)\n\n# Exercise 6: Complex dplyr Operations\nstudent_info <- mutate(student_info, student_major = c('Math', 'Science', 'Arts', 'Math', 'Science'))\nstudent_info <- select(student_info, -student_grade)\n\nfiltered_info <- student_info %>%\n  filter(student_major != \"Arts\") %>%\n  rename(id = student_id) %>%\n  arrange(desc(student_age))\n# Exercise 7: Exploratory Data Analysis with dplyr\naverage_age_by_major <- student_info %>%\n  group_by(student_major) %>%\n  summarise(average_age = mean(student_age))"},{"path":"workshop-1.html","id":"part-v","chapter":"Workshop 1","heading":"Part V","text":"","code":""},{"path":"workshop-1.html","id":"basic-data-visualization","chapter":"Workshop 1","heading":"Basic Data Visualization","text":"","code":""},{"path":"workshop-1.html","id":"exercise-1-12","chapter":"Workshop 1","heading":"Exercise 1","text":"Making Scatter plot:load journals.txt data set save Journals data frameWork following instructions","code":"\nJournals<-read.table(\"~/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data/journals.txt\")\n\nstr(Journals)\n#> 'data.frame':    180 obs. of  10 variables:\n#>  $ title       : chr  \"Asian-Pacific Economic Literature\" \"South African Journal of Economic History\" \"Computational Economics\" \"MOCT-MOST Economic Policy in Transitional Economics\" ...\n#>  $ publisher   : chr  \"Blackwell\" \"So Afr ec history assn\" \"Kluwer\" \"Kluwer\" ...\n#>  $ society     : chr  \"no\" \"no\" \"no\" \"no\" ...\n#>  $ price       : int  123 20 443 276 295 344 90 242 226 262 ...\n#>  $ pages       : int  440 309 567 520 791 609 602 665 243 386 ...\n#>  $ charpp      : int  3822 1782 2924 3234 3024 2967 3185 2688 3010 2501 ...\n#>  $ citations   : int  21 22 22 22 24 24 24 27 28 30 ...\n#>  $ foundingyear: int  1986 1986 1987 1991 1972 1994 1995 1968 1987 1949 ...\n#>  $ subs        : int  14 59 17 2 96 15 14 202 46 46 ...\n#>  $ field       : chr  \"General\" \"Economic History\" \"Specialized\" \"Area Studies\" ...\nplot(log(Journals$subs), log(Journals$price))\nrug(log(Journals$subs))\nrug(log(Journals$price), side = 2)\nplot(log(Journals$price) ~ log(Journals$subs), pch = 19,\n     col = \"blue\", xlim = c(0, 7), ylim = c(3, 8),\n     main = \"Library subscriptions\")\nrug(log(Journals$subs))\nrug(log(Journals$price), side=2)"},{"path":"workshop-1.html","id":"exercise-2-10","chapter":"Workshop 1","heading":"Exercise 2","text":"Now, try creating visualization using iris dataset. ’s can :Create scatter plot using Petal.Length Petal.Width iris dataset.Color points based Species column differentiate species.Add title, x-axis label, y-axis label plot.\nInclude legend indicates color corresponds iris species.Explanation:iris$Sepal.Length: selects Sepal.Length column iris dataset x-coordinates plot.\niris$Sepal.Width: selects Sepal.Width column iris dataset y-coordinates plot.\ncol=iris$Species: assigns colors points based Species column, means species different color plot.\nmain: Sets title plot “Iris Sepal Measurements”.\nxlab: Sets label x-axis “Sepal Length”.\nylab: Sets label y-axis “Sepal Width”.\npch=19: Sets plotting character (point symbol) solid circle.","code":"\n\ndata(iris)\n\n# Create the scatter plot\nplot(iris$Petal.Length, iris$Petal.Width, col=as.factor(iris$Species),\n     main=\"Iris Petal Measurements\",\n     xlab=\"Petal Length\", ylab=\"Petal Width\",\n     pch=19)\n\n# Add a legend\nlegend(\"topright\", legend=levels(iris$Species), col=1:length(levels(iris$Species)), pch=19)"},{"path":"optional-part-v-version-control-and-collaboration-10-minutes.html","id":"optional-part-v-version-control-and-collaboration-10-minutes","chapter":"(Optional) Part V: Version Control and Collaboration (10 minutes)","heading":"(Optional) Part V: Version Control and Collaboration (10 minutes)","text":"","code":""},{"path":"optional-part-v-version-control-and-collaboration-10-minutes.html","id":"using-git-and-github-for-version-control-and-collaboration-in-r-projects","chapter":"(Optional) Part V: Version Control and Collaboration (10 minutes)","heading":"Using Git and GitHub for version control and collaboration in R projects","text":"Git distributed version control system helps track changes source code software development. GitHub cloud-based hosting service lets manage Git repositories.Integrating Git R:\n1. Install Git set GitHub account.\n2. Configure Git username email.\n- Use Git Bash terminal:\ngit config –global user.name “Name”\ngit config –global user.email “.email@example.com”Initialize Git repository R project:\nRStudio, start new project select option create Git repository.\nRStudio, start new project select option create Git repository.Basic Git commands:\ngit init: Initialize new Git repository.\ngit status: Check status changes.\ngit add: Add files staging area.\ngit commit: Commit changes repository.\ngit push: Push changes remote repository like GitHub.\ngit pull: Pull updates remote repository.\ngit init: Initialize new Git repository.git status: Check status changes.git add: Add files staging area.git commit: Commit changes repository.git push: Push changes remote repository like GitHub.git pull: Pull updates remote repository.Collaborating GitHub:\nFork clone repositories.\nCreate branches features fixes.\nUse pull requests code reviews.\nMerge changes main branch.\nFork clone repositories.Create branches features fixes.Use pull requests code reviews.Merge changes main branch.Exercise:Create new R project Git repository.Make changes project, commit , push GitHub repository.Collaborate colleague friend fork repository submit pull request.","code":""},{"path":"workshop-1-suplementary-exercises-solved.html","id":"workshop-1-suplementary-exercises-solved","chapter":"Workshop 1: Suplementary Exercises Solved","heading":"Workshop 1: Suplementary Exercises Solved","text":"exercises https://intro2r.com/links explanation found , additionally, can read book chapters interested detailed explanation relates closely exercises. Now practice writing code script editor sourcing code R console. Let’s display help file function mean. script type help('mean') source code console. Notice help file displayed bottom right window (click ‘Help’ tab). Examine different components help file (especially examples section end help file). See Section 2.5 Introduction R book details using help functions. content displayed bottom right window context dependent. example write code plot(1:10) script source R console bottom right window display plot (don’t worry understanding R code right now, hopefully become clear later course!). Next, let’s practice creating variable assigning value variable. Take look Section 2.2 Introduction R book information prefer watch Objects R video. Create variable called first_num assign value 42. Click ‘Environment’ tab top right window display variable value. Now create another variable called first_char assign value \"first character\". Notice variable now also displayed ‘Environment’ along ’s value class (chr - short character class). Remove variable first_num environment using rm() function. Use code rm(first_num) . Check ‘Environment’ tab ensure variable removed. Alternatively, use ls() function list objects environment. Let’s see happens assign another value existing variable. Assign value \"second character\" variable first_char created Q6. Notice value changed ‘Environment’. display value first.char enter name variable console. Don’t forget save R script periodically! OK, let’s leave RStudio minute. Using favourite web browser, navigate R-project website explore links catch eye. Make sure find R manuals page user contributed documents section. Download manuals think might find useful (listed course manual) save computer (USB drive). Click ‘Search’ link R-Project website. Use ‘Rseek’ search term ‘mixed model p values’ (controversial subject!) explore anything looks interesting. Also experiment ‘R site search’ ‘Nabble R Forum’ links. Learning search help run problem using R acquired skill something get better time. One note caution, often find many different solutions solving problem R, written experienced R users others people less experience. Whichever solution choose make sure understand code thoroughly test make sure ’s want. OK, back RStudio. Sometimes may forget exact name function want use useful able search function names. example, want create design plot can remember name function word ‘plot’ . Use apropos() function list functions word plot name (see Section 2.5.1 Introduction R book). Look list figured correct function bring help file function (Hint: function name probably words ‘plot’ ‘design’ !).\n\napropos(\"plot\")\n#>  [1] \"assocplot\"           \"barplot\"            \n#>  [3] \"barplot.default\"     \"biplot\"             \n#>  [5] \"boxplot\"             \"boxplot.default\"    \n#>  [7] \"boxplot.matrix\"      \"boxplot.stats\"      \n#>  [9] \"cdplot\"              \"coplot\"             \n#> [11] \"fourfoldplot\"        \"interaction.plot\"   \n#> [13] \"lag.plot\"            \"matplot\"            \n#> [15] \"monthplot\"           \"mosaicplot\"         \n#> [17] \"plot\"                \"plot\"               \n#> [19] \"plot.default\"        \"plot.design\"        \n#> [21] \"plot.ecdf\"           \"plot.function\"      \n#> [23] \"plot.new\"            \"plot.spec.coherency\"\n#> [25] \"plot.spec.phase\"     \"plot.stepfun\"       \n#> [27] \"plot.ts\"             \"plot.window\"        \n#> [29] \"plot.xy\"             \"preplot\"            \n#> [31] \"qqplot\"              \"recordPlot\"         \n#> [33] \"replayPlot\"          \"savePlot\"           \n#> [35] \"screeplot\"           \"spineplot\"          \n#> [37] \"sunflowerplot\"       \"termplot\"           \n#> [39] \"ts.plot\"\nhelp('plot.design')\n OK, back RStudio. Sometimes may forget exact name function want use useful able search function names. example, want create design plot can remember name function word ‘plot’ . Use apropos() function list functions word plot name (see Section 2.5.1 Introduction R book). Look list figured correct function bring help file function (Hint: function name probably words ‘plot’ ‘design’ !). Another strategy use help.search() function search R’s help files. Search R help system instances character string ‘plot’. Take look Section 2.5.1 information. Also, see can figure narrow search searching ‘plot’ nlme package (hint: see help page help.search()).\n\nhelp.search(\"plot\")\n??plot     # shortcut help.search function\n\nhelp.search(\"plot\", package = \"nlme\")\n Another strategy use help.search() function search R’s help files. Search R help system instances character string ‘plot’. Take look Section 2.5.1 information. Also, see can figure narrow search searching ‘plot’ nlme package (hint: see help page help.search()). R’s working directory default location files read R, export R. Although won’t importing exporting files just yet (’s tomorrows job) ’s useful able determine current working directory . , read Section 1.7 Introduction R book introduce working directories figure display current working directory.\n\ngetwd()    # displays current working directory \n#> [1] \"/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay\"\n R’s working directory default location files read R, export R. Although won’t importing exporting files just yet (’s tomorrows job) ’s useful able determine current working directory . , read Section 1.7 Introduction R book introduce working directories figure display current working directory. ##Basic R operations {-}\nRead Chapter 2 help complete questions exercise. \n11. Let’s use R fancy calculator. Find natural log, log base 10, log base 2, square root natural antilog 12.43. See Section 2.1 Introduction R book information mathematical functions R. Don’t forget write code RStudio’s script editor source code console. Next, use R determine area circle diameter 20 cm assign result object called area_circle. can’t remember create assign objects see Section 2.2 watch video. Google friend can’t remember formula calculate area circle! Also, remember R already knows pi. Don’t worry ’re stumped feel free ask one instructors guidance. Now something little tricky. Calculate cube root 14 x 0.51. might need think creatively solution (hint: think exponents), remember R follows usual order mathematical operators might need use brackets code (see page ’ve never heard ). point question torture maths (please don’t stress!), get used writing mathematical equations R highlight order operations. Ok, ’re now ready explore one R’s basic (useful) data structures - vectors. vector sequence elements (components) data type (see Section 3.2.1 introduction vectors). Although technically correct might useful think vector something like single column spreadsheet. multitude ways create vectors R use concatenate function c() create vector called weight containing weight (kg) 10 children: 69, 62, 57, 59, 59, 64, 56, 66, 67, 66 (Section 2.3 watch video information). Now can useful stuff weight vector. Get R calculate mean, variance, standard deviation, range weights number children weight vector (see Section 2.3 details). Now read Section 2.4 R book learn work vectors. reading section able extract weights first five children using Positional indexes store weights new variable called first_five. Remember, need use square brackets [ ] extract (aka index, subset) elements variable. ’re now going use c() function create another vector called height containing height (cm) 10 children: 112, 102, 83, 84, 99, 90, 77, 112, 133, 112. Use summary() function summarise data height object. Extract height 2nd, 3rd, 9th 10th child assign heights variable called some_child (take look section Positional indexes R book ’re stuck). can also extract elements using Logical indexes. Let’s extract heights children less equal 99 cm assign variable called shorter_child. Now can use information weight height variables calculate body mass index (BMI) child. BMI calculated weight (kg) divided square height (meters). Store results calculation variable called bmi. Note: don’t need calculation child individually, can use vectors BMI equation – called vectorisation (see Section 2.4.4 Introduction R book). Now let’s practice useful skill - creating sequences (honestly …). Take look Section 2.3 R book (bit creating sequences) see myriad ways can create sequences R. Let’s use seq() function create sequence numbers ranging 0 1 steps 0.1 (also vector way) assign sequence variable called seq1. Next, see can figure create sequence 10 1 steps 0.5. Assign sequence variable called seq2 (Hint: may find useful include rev() function code). Let’s go sequence crazy! Generate following sequences. need experiment arguments rep() function generate sequences (see Section 2.3 clues):1 2 3 1 2 3 1 2 3“” “” “” “c” “c” “c” “e” “e” “e” “g” “g” “g”“” “c” “e” “g” “” “c” “e” “g” “” “c” “e” “g”1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 31 1 1 1 1 2 2 2 2 3 3 3 4 4 57 7 7 7 2 2 2 8 1 1 1 1 1 Ok, back variable height created Q7. Let’s sort values height ascending order (shortest tallest) assign sorted vector new variable called height_sorted. Take look Section 2.4.3 R book see . Now sort heights descending order assign new vector name choice. Let’s give children names. Create new vector called child_name following names 10 children: \"Alfred\", \"Barbara\", \"James\", \"Jane\", \"John\", \"Judy\", \"Louise\", \"Mary\", \"Ronald\", \"William\". really useful (common) task order values one variable order another variable. need use order() function combination square bracket notation [ ]. peep Section 2.4.3 details. Create new variable called names_sort store names children ordered child height (shortest tallest). shortest? tallest child? ’re sure , please ask one instructors. Now order names children descending values weight assign result variable called weight_rev (Hint: perhaps include rev() function?). heaviest? lightest? Finally, list variables workspace created exercise. Remove variable seq1 workspace using rm() function. ","code":"\n    help(mean)          # different methods of using help\n    ?mean\n    help(\"mean\")\n    plot(1:10)    #dont worry about what 1:10 does just yet\n    first_num <- 42    # create variable first_num and assign the value 42\n    first_char <- \"my first character\"\n    rm(first_num)\n    ls()          # list all variables in the workspace \n#> [1] \"first_char\"\n    first_char <- \"my second variable\"\n    first_char    # display the value \n#> [1] \"my second variable\"\napropos(\"plot\")\n#>  [1] \"assocplot\"           \"barplot\"            \n#>  [3] \"barplot.default\"     \"biplot\"             \n#>  [5] \"boxplot\"             \"boxplot.default\"    \n#>  [7] \"boxplot.matrix\"      \"boxplot.stats\"      \n#>  [9] \"cdplot\"              \"coplot\"             \n#> [11] \"fourfoldplot\"        \"interaction.plot\"   \n#> [13] \"lag.plot\"            \"matplot\"            \n#> [15] \"monthplot\"           \"mosaicplot\"         \n#> [17] \"plot\"                \"plot\"               \n#> [19] \"plot.default\"        \"plot.design\"        \n#> [21] \"plot.ecdf\"           \"plot.function\"      \n#> [23] \"plot.new\"            \"plot.spec.coherency\"\n#> [25] \"plot.spec.phase\"     \"plot.stepfun\"       \n#> [27] \"plot.ts\"             \"plot.window\"        \n#> [29] \"plot.xy\"             \"preplot\"            \n#> [31] \"qqplot\"              \"recordPlot\"         \n#> [33] \"replayPlot\"          \"savePlot\"           \n#> [35] \"screeplot\"           \"spineplot\"          \n#> [37] \"sunflowerplot\"       \"termplot\"           \n#> [39] \"ts.plot\"\nhelp('plot.design')\nhelp.search(\"plot\")\n??plot     # shortcut for help.search function\n\nhelp.search(\"plot\", package = \"nlme\")\ngetwd()    # displays the current working directory \n#> [1] \"/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay\"\n    log(12.43)              # natural log\n#> [1] 2.520113\n    log10(12.43)            # log to base 10\n#> [1] 1.094471\n    log2(12.43)             # log to base 2\n#> [1] 3.635754\n    log(12.43, base = 2)    # alternative log to base 2\n#> [1] 3.635754\n    sqrt(12.43)             # square root\n#> [1] 3.525621\n    exp(12.43)              # exponent\n#> [1] 250196\n    area_circle <- pi * (20/2)^2\n    (14 * 0.51)^(1/3)\n#> [1] 1.9256\n    weight <- c(69, 62, 57, 59, 59, 64, 56, 66, 67, 66)\n    mean(weight)                                # calculate mean \n#> [1] 62.5\n    var(weight)                                 # calculate variance\n#> [1] 20.72222\n    sd(weight)                                  # calculate standard deviation\n#> [1] 4.552167\n    range(weight)                               # range of weight values\n#> [1] 56 69\n    length(weight)                              # number of observations\n#> [1] 10\n    \n    first_five <- weight[1:5]                  # extract first 5 weight values\n    first_five <- weight[c(1, 2, 3, 4, 5)]     # alternative method\n    height <- c(112, 102, 83, 84, 99, 90, 77, 112, 133, 112)\n    \n    summary(height)   # summary statistics of height variable\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    77.0    85.5   100.5   100.4   112.0   133.0\n    \n    some_child <- height[c(2, 3, 9, 10)]      # extract the 2nd, 3rd, 9th, 10th height\n    \n    shorter_child <- height[height <= 99]     # extract all heights less than or equal to 99\n    bmi <- weight/(height/100)^2    # don't forget to convert height to meters\n    seq1 <- seq(from = 0, to = 1, by = 0.1)\n    seq2 <- rev(seq(from = 1, to = 10, by = 0.5))\n    rep(1:3, times = 3)\n#> [1] 1 2 3 1 2 3 1 2 3\n    rep(c(\"a\", \"c\", \"e\", \"g\"), each = 3)\n#>  [1] \"a\" \"a\" \"a\" \"c\" \"c\" \"c\" \"e\" \"e\" \"e\" \"g\" \"g\" \"g\"\n    rep(c(\"a\", \"c\", \"e\", \"g\"), times = 3)\n#>  [1] \"a\" \"c\" \"e\" \"g\" \"a\" \"c\" \"e\" \"g\" \"a\" \"c\" \"e\" \"g\"\n    rep(1:3, each = 3, times = 2)\n#>  [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3\n    rep(1:5, times = 5:1)\n#>  [1] 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5\n    rep(c(7, 2, 8, 1), times = c(4, 3, 1, 5))\n#>  [1] 7 7 7 7 2 2 2 8 1 1 1 1 1\n    height_sorted <- sort(height)\n    \n    height_rev <- rev(sort(height))\n    child_name <- c(\"Alfred\", \"Barbara\", \"James\", \"Jane\", \"John\", \"Judy\", \"Louise\", \"Mary\", \"Ronald\", \"William\")\n    height_ord <- order(height)   # get the indexes of the heights, smallest to tallest\n    names_sort <- child_name[height_ord]     # Louise is shortest, Ronald is tallest\n    weight_ord <- rev(order(weight))\n    weight_rev <- child_name[weight_ord]     # Alfred is heaviest, Louise is lightest\n    ls()          # list all variables in workspace\n#>  [1] \"area_circle\"   \"bmi\"           \"child_name\"   \n#>  [4] \"first_char\"    \"first_five\"    \"height\"       \n#>  [7] \"height_ord\"    \"height_rev\"    \"height_sorted\"\n#> [10] \"names_sort\"    \"seq1\"          \"seq2\"         \n#> [13] \"shorter_child\" \"some_child\"    \"weight\"       \n#> [16] \"weight_ord\"    \"weight_rev\"\n    rm(seq1)      # remove variable seq1 from the workspace\n    ls()          # check seq1 has been removed\n#>  [1] \"area_circle\"   \"bmi\"           \"child_name\"   \n#>  [4] \"first_char\"    \"first_five\"    \"height\"       \n#>  [7] \"height_ord\"    \"height_rev\"    \"height_sorted\"\n#> [10] \"names_sort\"    \"seq2\"          \"shorter_child\"\n#> [13] \"some_child\"    \"weight\"        \"weight_ord\"   \n#> [16] \"weight_rev\""},{"path":"workshop-2.html","id":"workshop-2","chapter":"Workshop 2","heading":"Workshop 2","text":"","code":""},{"path":"workshop-2.html","id":"exercise-solutions-1","chapter":"Workshop 2","heading":"Exercise solutions","text":"","code":"\nlibrary(tidyverse)\n#> ── Attaching core tidyverse packages ──── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#> ✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"},{"path":"workshop-2.html","id":"part-i","chapter":"Workshop 2","heading":"Part I","text":"","code":""},{"path":"workshop-2.html","id":"exercise-1-13","chapter":"Workshop 2","heading":"Exercise 1","text":"Task: Write Use Function\nObjective: Create function calculates cube number use function calculate cube 3.Hint: Use structure square_function template.","code":"\ncube_function <- function(x) {\n  return(x^3)\n}\n\n# Using the function\nresult <- cube_function(3)\nprint(result)\n#> [1] 27"},{"path":"workshop-2.html","id":"exercise-2-11","chapter":"Workshop 2","heading":"Exercise 2","text":"Task: Analyze Numeric Vector\nObjective: Write function named summarize_vector takes numeric vector input calculates median, variance, creates boxplot. function print median variance, return list. Use airquality$Ozone data analysis.Hint: Similar analyze_vector, check input numeric use median, var, boxplot functions.","code":"\nsummarize_vector <- function(x, plot_title = \"Boxplot\") {\n  if (!is.numeric(x)) {\n    stop(\"Input must be a numeric vector\")\n  }\n  \n  median_value <- median(x, na.rm = TRUE)\n  variance_value <- var(x, na.rm = TRUE)\n  \n  cat(\"Median:\", median_value, \"\\n\")\n  cat(\"Variance:\", variance_value, \"\\n\")\n  \n  boxplot(x, main = plot_title, xlab = \"Values\", col = \"coral\", border = \"darkred\")\n  \n  return(list(median = median_value, variance = variance_value))\n}\n\n# Example usage with the airquality$Ozone vector\nresult <- summarize_vector(airquality$Ozone, \"Ozone Levels Boxplot\")\n#> Median: 31.5 \n#> Variance: 1088.201"},{"path":"workshop-2.html","id":"exercise-3-4","chapter":"Workshop 2","heading":"Exercise 3","text":"Task: Using Statements\nObjective: Create R script checks number negative, zero, positive prints appropriate message. Test script number -4.Hint: Use statement followed else else.","code":"\nnumber <- -4\nif (number > 0) {\n  print(\"Positive number\")\n} else if (number == 0) {\n  print(\"Zero\")\n} else {\n  print(\"Negative number\")\n}\n#> [1] \"Negative number\""},{"path":"workshop-2.html","id":"exercise-4-5","chapter":"Workshop 2","heading":"Exercise 4","text":"Task: Loop\nObjective: Write function using loop calculates sum squares numbers 1 n. Use function calculate sum squares n=10.Hint: Iterate 1 n, keep adding square number result variable.","code":"\nsum_of_squares <- function(n) {\n  result <- 0\n  for (i in 1:n) {\n    result <- result + i^2\n  }\n  return(result)\n}\n\nsum_squares_of_10 <- sum_of_squares(10)\nprint(sum_squares_of_10)\n#> [1] 385"},{"path":"workshop-2.html","id":"exercise-5-5","chapter":"Workshop 2","heading":"Exercise 5","text":"Task: Loop\nObjective: Write script using loop finds smallest number whose cube greater 100. Print number cube.Hint: Increment number starting 1, check cube greater 100 loop condition.","code":"\nnumber <- 1\nwhile (number^3 <= 100) {\n  number <- number + 1\n}\nprint(paste(\"Smallest number whose cube is greater than 100 is:\", number))\n#> [1] \"Smallest number whose cube is greater than 100 is: 5\"\nprint(paste(\"Cube of\", number, \"is:\", number^3))\n#> [1] \"Cube of 5 is: 125\""},{"path":"workshop-2.html","id":"part-ii","chapter":"Workshop 2","heading":"Part II","text":"","code":""},{"path":"workshop-2.html","id":"exercise-1-14","chapter":"Workshop 2","heading":"Exercise 1","text":"Load data Parade2005.txt.Determine mean earnings California.Determine number individuals residing Idaho.Determine mean median earnings celebrities.","code":"\nParade2005 <- read.table(file = '/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data/Parade2005.txt')\nhead(Parade2005)\n#>   earnings age gender state celebrity\n#> 1    10000  26   male    ND        no\n#> 2 10000000  18 female    CA       yes\n#> 3    85000  39   male    NE        no\n#> 4    75000  50 female    NC        no\n#> 5    91500  61   male    DE        no\n#> 6    49500  39 female    SD        no\nParade2005 %>% filter(state == \"CA\") %>%\n              summarize(mean = mean(earnings))\n#>      mean\n#> 1 6241430\nParade2005 %>% group_by(state == \"CA\") %>%\n              summarize(mean = mean(earnings))\n#> # A tibble: 2 × 2\n#>   `state == \"CA\"`     mean\n#>   <lgl>              <dbl>\n#> 1 FALSE           1108577.\n#> 2 TRUE            6241430\nParade2005 %>% \n      filter(state == \"ID\") %>% \n      summarize(number = n())\n#>   number\n#> 1      5\nParade2005 %>% \n  group_by(celebrity) %>%\n  summarize(mean = mean(earnings), median = median(earnings))\n#> # A tibble: 2 × 3\n#>   celebrity      mean   median\n#>   <chr>         <dbl>    <dbl>\n#> 1 no           61038.    49500\n#> 2 yes       17107273. 19000000\nParade2005 %>% \n  group_by(celebrity) %>%\n  ggplot(aes(x = celebrity, y = earnings)) + theme_bw() +\n  geom_boxplot(color = \"blue\")\nboxplot(earnings~ celebrity , data= Parade2005)"},{"path":"workshop-2.html","id":"exercise-2-12","chapter":"Workshop 2","heading":"Exercise 2","text":"Use skills obtained Part .Inspect top rows data set.many observations data set contain?Calculate total exposure (exposition) region (type_territoire).","code":"\npolicy_data <- read.csv(file = '/Users/dilancaro/Library/Mobile Documents/com~apple~CloudDocs/Workshops/John Jay/R Workshop/R-workshop-John-Jay/John Jay Workshop Data/PolicyData.csv', sep = ';')\nhead(policy_data)\n#>   numeropol  debut_pol    fin_pol freq_paiement langue\n#> 1         3 14/09/1995 24/04/1996       mensuel      F\n#> 2         3 25/04/1996 23/12/1996       mensuel      F\n#> 3         6  1/03/1995 27/02/1996        annuel      A\n#> 4         6  1/03/1996 14/01/1997        annuel      A\n#> 5         6 15/01/1997 31/01/1997        annuel      A\n#> 6         6  1/02/1997 28/02/1997        annuel      A\n#>    type_prof     alimentation type_territoire\n#> 1 Technicien V\\xe9g\\xe9tarien          Urbain\n#> 2 Technicien V\\xe9g\\xe9tarien          Urbain\n#> 3 Technicien        Carnivore          Urbain\n#> 4 Technicien        Carnivore          Urbain\n#> 5 Technicien        Carnivore          Urbain\n#> 6 Technicien        Carnivore          Urbain\n#>           utilisation presence_alarme marque_voiture sexe\n#> 1   Travail-quotidien             non     VOLKSWAGEN    F\n#> 2   Travail-quotidien             non     VOLKSWAGEN    F\n#> 3 Travail-occasionnel             oui         NISSAN    M\n#> 4 Travail-occasionnel             oui         NISSAN    M\n#> 5 Travail-occasionnel             oui         NISSAN    M\n#> 6 Travail-occasionnel             oui         NISSAN    M\n#>      cout1 cout2 cout3 cout4 nbsin exposition   cout age\n#> 1       NA    NA    NA    NA     0 0.61095890     NA  29\n#> 2       NA    NA    NA    NA     0 0.66301370     NA  30\n#> 3 279.5839    NA    NA    NA     1 0.99452055 279.58  42\n#> 4       NA    NA    NA    NA     0 0.87397260     NA  43\n#> 5       NA    NA    NA    NA     0 0.04383562     NA  44\n#> 6       NA    NA    NA    NA     0 0.07397260     NA  44\n#>   duree_permis annee_vehicule\n#> 1           10           1989\n#> 2           11           1989\n#> 3           21           1994\n#> 4           22           1994\n#> 5           23           1994\n#> 6           23           1994\nnrow(policy_data)\n#> [1] 39075\npolicy_data %>%\n  group_by(type_territoire) %>%\n  summarize(exposure = sum(exposition))\n#> # A tibble: 3 × 2\n#>   type_territoire exposure\n#>   <chr>              <dbl>\n#> 1 Rural               684.\n#> 2 Semi-urbain       16944.\n#> 3 Urbain            11050."},{"path":"workshop-2.html","id":"exercise-3-5","chapter":"Workshop 2","heading":"Exercise 3","text":"Use skills obtained Part :Inspect top rows data.Select data countries Asia.type variable country?","code":"\nlibrary(gapminder)\nhead(gapminder)\n#> # A tibble: 6 × 6\n#>   country     continent  year lifeExp      pop gdpPercap\n#>   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#> 1 Afghanistan Asia       1952    28.8  8425333      779.\n#> 2 Afghanistan Asia       1957    30.3  9240934      821.\n#> 3 Afghanistan Asia       1962    32.0 10267083      853.\n#> 4 Afghanistan Asia       1967    34.0 11537966      836.\n#> 5 Afghanistan Asia       1972    36.1 13079460      740.\n#> 6 Afghanistan Asia       1977    38.4 14880372      786.\nasia <-  filter(gapminder, continent == \"Asia\")\nclass(gapminder$country)\n#> [1] \"factor\""},{"path":"workshop-2.html","id":"exercise-4-6","chapter":"Workshop 2","heading":"Exercise 4","text":"variable country gapminder data set factor variable.possible levels country subset asia.result expected?asia$country allows outcomes gapminder$country. includes many countries outside Asia.cases, even filtering dataset, factor levels subset might still include levels present original dataset. subsetting rows automatically drop unused factor levels. see levels present subset, might need use droplevels():","code":"\nlibrary(gapminder)\ngapminder\n#> # A tibble: 1,704 × 6\n#>    country     continent  year lifeExp      pop gdpPercap\n#>    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#>  1 Afghanistan Asia       1952    28.8  8425333      779.\n#>  2 Afghanistan Asia       1957    30.3  9240934      821.\n#>  3 Afghanistan Asia       1962    32.0 10267083      853.\n#>  4 Afghanistan Asia       1967    34.0 11537966      836.\n#>  5 Afghanistan Asia       1972    36.1 13079460      740.\n#>  6 Afghanistan Asia       1977    38.4 14880372      786.\n#>  7 Afghanistan Asia       1982    39.9 12881816      978.\n#>  8 Afghanistan Asia       1987    40.8 13867957      852.\n#>  9 Afghanistan Asia       1992    41.7 16317921      649.\n#> 10 Afghanistan Asia       1997    41.8 22227415      635.\n#> # ℹ 1,694 more rows\n# Subset data for Asian countries\nasia <- subset(gapminder, continent == \"Asia\")\n\nlevels(asia$country)\n#>   [1] \"Afghanistan\"              \"Albania\"                 \n#>   [3] \"Algeria\"                  \"Angola\"                  \n#>   [5] \"Argentina\"                \"Australia\"               \n#>   [7] \"Austria\"                  \"Bahrain\"                 \n#>   [9] \"Bangladesh\"               \"Belgium\"                 \n#>  [11] \"Benin\"                    \"Bolivia\"                 \n#>  [13] \"Bosnia and Herzegovina\"   \"Botswana\"                \n#>  [15] \"Brazil\"                   \"Bulgaria\"                \n#>  [17] \"Burkina Faso\"             \"Burundi\"                 \n#>  [19] \"Cambodia\"                 \"Cameroon\"                \n#>  [21] \"Canada\"                   \"Central African Republic\"\n#>  [23] \"Chad\"                     \"Chile\"                   \n#>  [25] \"China\"                    \"Colombia\"                \n#>  [27] \"Comoros\"                  \"Congo, Dem. Rep.\"        \n#>  [29] \"Congo, Rep.\"              \"Costa Rica\"              \n#>  [31] \"Cote d'Ivoire\"            \"Croatia\"                 \n#>  [33] \"Cuba\"                     \"Czech Republic\"          \n#>  [35] \"Denmark\"                  \"Djibouti\"                \n#>  [37] \"Dominican Republic\"       \"Ecuador\"                 \n#>  [39] \"Egypt\"                    \"El Salvador\"             \n#>  [41] \"Equatorial Guinea\"        \"Eritrea\"                 \n#>  [43] \"Ethiopia\"                 \"Finland\"                 \n#>  [45] \"France\"                   \"Gabon\"                   \n#>  [47] \"Gambia\"                   \"Germany\"                 \n#>  [49] \"Ghana\"                    \"Greece\"                  \n#>  [51] \"Guatemala\"                \"Guinea\"                  \n#>  [53] \"Guinea-Bissau\"            \"Haiti\"                   \n#>  [55] \"Honduras\"                 \"Hong Kong, China\"        \n#>  [57] \"Hungary\"                  \"Iceland\"                 \n#>  [59] \"India\"                    \"Indonesia\"               \n#>  [61] \"Iran\"                     \"Iraq\"                    \n#>  [63] \"Ireland\"                  \"Israel\"                  \n#>  [65] \"Italy\"                    \"Jamaica\"                 \n#>  [67] \"Japan\"                    \"Jordan\"                  \n#>  [69] \"Kenya\"                    \"Korea, Dem. Rep.\"        \n#>  [71] \"Korea, Rep.\"              \"Kuwait\"                  \n#>  [73] \"Lebanon\"                  \"Lesotho\"                 \n#>  [75] \"Liberia\"                  \"Libya\"                   \n#>  [77] \"Madagascar\"               \"Malawi\"                  \n#>  [79] \"Malaysia\"                 \"Mali\"                    \n#>  [81] \"Mauritania\"               \"Mauritius\"               \n#>  [83] \"Mexico\"                   \"Mongolia\"                \n#>  [85] \"Montenegro\"               \"Morocco\"                 \n#>  [87] \"Mozambique\"               \"Myanmar\"                 \n#>  [89] \"Namibia\"                  \"Nepal\"                   \n#>  [91] \"Netherlands\"              \"New Zealand\"             \n#>  [93] \"Nicaragua\"                \"Niger\"                   \n#>  [95] \"Nigeria\"                  \"Norway\"                  \n#>  [97] \"Oman\"                     \"Pakistan\"                \n#>  [99] \"Panama\"                   \"Paraguay\"                \n#> [101] \"Peru\"                     \"Philippines\"             \n#> [103] \"Poland\"                   \"Portugal\"                \n#> [105] \"Puerto Rico\"              \"Reunion\"                 \n#> [107] \"Romania\"                  \"Rwanda\"                  \n#> [109] \"Sao Tome and Principe\"    \"Saudi Arabia\"            \n#> [111] \"Senegal\"                  \"Serbia\"                  \n#> [113] \"Sierra Leone\"             \"Singapore\"               \n#> [115] \"Slovak Republic\"          \"Slovenia\"                \n#> [117] \"Somalia\"                  \"South Africa\"            \n#> [119] \"Spain\"                    \"Sri Lanka\"               \n#> [121] \"Sudan\"                    \"Swaziland\"               \n#> [123] \"Sweden\"                   \"Switzerland\"             \n#> [125] \"Syria\"                    \"Taiwan\"                  \n#> [127] \"Tanzania\"                 \"Thailand\"                \n#> [129] \"Togo\"                     \"Trinidad and Tobago\"     \n#> [131] \"Tunisia\"                  \"Turkey\"                  \n#> [133] \"Uganda\"                   \"United Kingdom\"          \n#> [135] \"United States\"            \"Uruguay\"                 \n#> [137] \"Venezuela\"                \"Vietnam\"                 \n#> [139] \"West Bank and Gaza\"       \"Yemen, Rep.\"             \n#> [141] \"Zambia\"                   \"Zimbabwe\"\n#alternative\nlibrary(dplyr)\n\nasia <- gapminder %>% \n  filter(continent == \"Asia\")\nlevels(asia$country)\n#>   [1] \"Afghanistan\"              \"Albania\"                 \n#>   [3] \"Algeria\"                  \"Angola\"                  \n#>   [5] \"Argentina\"                \"Australia\"               \n#>   [7] \"Austria\"                  \"Bahrain\"                 \n#>   [9] \"Bangladesh\"               \"Belgium\"                 \n#>  [11] \"Benin\"                    \"Bolivia\"                 \n#>  [13] \"Bosnia and Herzegovina\"   \"Botswana\"                \n#>  [15] \"Brazil\"                   \"Bulgaria\"                \n#>  [17] \"Burkina Faso\"             \"Burundi\"                 \n#>  [19] \"Cambodia\"                 \"Cameroon\"                \n#>  [21] \"Canada\"                   \"Central African Republic\"\n#>  [23] \"Chad\"                     \"Chile\"                   \n#>  [25] \"China\"                    \"Colombia\"                \n#>  [27] \"Comoros\"                  \"Congo, Dem. Rep.\"        \n#>  [29] \"Congo, Rep.\"              \"Costa Rica\"              \n#>  [31] \"Cote d'Ivoire\"            \"Croatia\"                 \n#>  [33] \"Cuba\"                     \"Czech Republic\"          \n#>  [35] \"Denmark\"                  \"Djibouti\"                \n#>  [37] \"Dominican Republic\"       \"Ecuador\"                 \n#>  [39] \"Egypt\"                    \"El Salvador\"             \n#>  [41] \"Equatorial Guinea\"        \"Eritrea\"                 \n#>  [43] \"Ethiopia\"                 \"Finland\"                 \n#>  [45] \"France\"                   \"Gabon\"                   \n#>  [47] \"Gambia\"                   \"Germany\"                 \n#>  [49] \"Ghana\"                    \"Greece\"                  \n#>  [51] \"Guatemala\"                \"Guinea\"                  \n#>  [53] \"Guinea-Bissau\"            \"Haiti\"                   \n#>  [55] \"Honduras\"                 \"Hong Kong, China\"        \n#>  [57] \"Hungary\"                  \"Iceland\"                 \n#>  [59] \"India\"                    \"Indonesia\"               \n#>  [61] \"Iran\"                     \"Iraq\"                    \n#>  [63] \"Ireland\"                  \"Israel\"                  \n#>  [65] \"Italy\"                    \"Jamaica\"                 \n#>  [67] \"Japan\"                    \"Jordan\"                  \n#>  [69] \"Kenya\"                    \"Korea, Dem. Rep.\"        \n#>  [71] \"Korea, Rep.\"              \"Kuwait\"                  \n#>  [73] \"Lebanon\"                  \"Lesotho\"                 \n#>  [75] \"Liberia\"                  \"Libya\"                   \n#>  [77] \"Madagascar\"               \"Malawi\"                  \n#>  [79] \"Malaysia\"                 \"Mali\"                    \n#>  [81] \"Mauritania\"               \"Mauritius\"               \n#>  [83] \"Mexico\"                   \"Mongolia\"                \n#>  [85] \"Montenegro\"               \"Morocco\"                 \n#>  [87] \"Mozambique\"               \"Myanmar\"                 \n#>  [89] \"Namibia\"                  \"Nepal\"                   \n#>  [91] \"Netherlands\"              \"New Zealand\"             \n#>  [93] \"Nicaragua\"                \"Niger\"                   \n#>  [95] \"Nigeria\"                  \"Norway\"                  \n#>  [97] \"Oman\"                     \"Pakistan\"                \n#>  [99] \"Panama\"                   \"Paraguay\"                \n#> [101] \"Peru\"                     \"Philippines\"             \n#> [103] \"Poland\"                   \"Portugal\"                \n#> [105] \"Puerto Rico\"              \"Reunion\"                 \n#> [107] \"Romania\"                  \"Rwanda\"                  \n#> [109] \"Sao Tome and Principe\"    \"Saudi Arabia\"            \n#> [111] \"Senegal\"                  \"Serbia\"                  \n#> [113] \"Sierra Leone\"             \"Singapore\"               \n#> [115] \"Slovak Republic\"          \"Slovenia\"                \n#> [117] \"Somalia\"                  \"South Africa\"            \n#> [119] \"Spain\"                    \"Sri Lanka\"               \n#> [121] \"Sudan\"                    \"Swaziland\"               \n#> [123] \"Sweden\"                   \"Switzerland\"             \n#> [125] \"Syria\"                    \"Taiwan\"                  \n#> [127] \"Tanzania\"                 \"Thailand\"                \n#> [129] \"Togo\"                     \"Trinidad and Tobago\"     \n#> [131] \"Tunisia\"                  \"Turkey\"                  \n#> [133] \"Uganda\"                   \"United Kingdom\"          \n#> [135] \"United States\"            \"Uruguay\"                 \n#> [137] \"Venezuela\"                \"Vietnam\"                 \n#> [139] \"West Bank and Gaza\"       \"Yemen, Rep.\"             \n#> [141] \"Zambia\"                   \"Zimbabwe\"\nasia$country <- droplevels(asia$country)\nlevels(asia$country)\n#>  [1] \"Afghanistan\"        \"Bahrain\"           \n#>  [3] \"Bangladesh\"         \"Cambodia\"          \n#>  [5] \"China\"              \"Hong Kong, China\"  \n#>  [7] \"India\"              \"Indonesia\"         \n#>  [9] \"Iran\"               \"Iraq\"              \n#> [11] \"Israel\"             \"Japan\"             \n#> [13] \"Jordan\"             \"Korea, Dem. Rep.\"  \n#> [15] \"Korea, Rep.\"        \"Kuwait\"            \n#> [17] \"Lebanon\"            \"Malaysia\"          \n#> [19] \"Mongolia\"           \"Myanmar\"           \n#> [21] \"Nepal\"              \"Oman\"              \n#> [23] \"Pakistan\"           \"Philippines\"       \n#> [25] \"Saudi Arabia\"       \"Singapore\"         \n#> [27] \"Sri Lanka\"          \"Syria\"             \n#> [29] \"Taiwan\"             \"Thailand\"          \n#> [31] \"Vietnam\"            \"West Bank and Gaza\"\n#> [33] \"Yemen, Rep.\""},{"path":"workshop-2.html","id":"exercise-5-6","chapter":"Workshop 2","heading":"Exercise 5","text":"Bin life expectancy 2007 factor variable.\n1. Select observations year 2007.\n2. Bin life expectancy four bins roughly equal size (hint: quantile).\n3. many observations bin?y = ..prop.. group = continent plot proportion within group instead absolute count.","code":"\ngapminder2007 <- filter(gapminder, year == 2007)\nbreaks <- c(0, quantile(gapminder2007$lifeExp, c(0.25, 0.5, 0.75)), Inf)\nbreaks\n#>               25%      50%      75%          \n#>  0.00000 57.16025 71.93550 76.41325      Inf\ngapminder2007 <- gapminder2007 %>%\n                 mutate(life_expectancy_binned = cut(gapminder2007$lifeExp, breaks))\ngapminder2007 %>%\n  group_by(life_expectancy_binned) %>%\n  summarise(frequency = n())\n#> # A tibble: 4 × 2\n#>   life_expectancy_binned frequency\n#>   <fct>                      <int>\n#> 1 (0,57.2]                      36\n#> 2 (57.2,71.9]                   35\n#> 3 (71.9,76.4]                   35\n#> 4 (76.4,Inf]                    36\ngapminder2007\n#> # A tibble: 142 × 7\n#>    country     continent  year lifeExp       pop gdpPercap\n#>    <fct>       <fct>     <int>   <dbl>     <int>     <dbl>\n#>  1 Afghanistan Asia       2007    43.8  31889923      975.\n#>  2 Albania     Europe     2007    76.4   3600523     5937.\n#>  3 Algeria     Africa     2007    72.3  33333216     6223.\n#>  4 Angola      Africa     2007    42.7  12420476     4797.\n#>  5 Argentina   Americas   2007    75.3  40301927    12779.\n#>  6 Australia   Oceania    2007    81.2  20434176    34435.\n#>  7 Austria     Europe     2007    79.8   8199783    36126.\n#>  8 Bahrain     Asia       2007    75.6    708573    29796.\n#>  9 Bangladesh  Asia       2007    64.1 150448339     1391.\n#> 10 Belgium     Europe     2007    79.4  10392226    33693.\n#> # ℹ 132 more rows\n#> # ℹ 1 more variable: life_expectancy_binned <fct>\nplot(gapminder2007$life_expectancy_binned, col=c(gapminder2007$continent))\nggplot(gapminder2007) +\n  geom_bar(aes(life_expectancy_binned))\nggplot(gapminder2007) +\n  geom_bar(aes(life_expectancy_binned,\n               fill = continent))\nggplot(gapminder2007) +\n  geom_bar(aes(life_expectancy_binned,\n               fill = continent),\n           position = position_dodge())\nggplot(gapminder2007) +\n  geom_bar(aes(life_expectancy_binned,\n               fill = continent,\n               y = after_stat(prop), group = continent),\n           position = position_dodge())"},{"path":"workshop-2.html","id":"handling-missing-data-1","chapter":"Workshop 2","heading":"Handling Missing Data","text":"","code":""},{"path":"workshop-2.html","id":"exercise-1-explore-missingness-1","chapter":"Workshop 2","heading":"Exercise 1: Explore Missingness","text":"Dataset: ChickWeightTask: Determine ChickWeight dataset contains missing values. Print message stating whether dataset missing values .Hint Use () function combined .na() applied dataset.use ()","code":"\ne<- c(1,2,2,3,2,1,1)\nany(e==1)\n#> [1] TRUE\ndata(ChickWeight)\n# Check for missing values in the ChickWeight dataset\nif (any(is.na(ChickWeight))) {\n  print(\"The dataset contains missing values.\")\n} else {\n  print(\"The dataset does not contain missing values.\")\n}\n#> [1] \"The dataset does not contain missing values.\""},{"path":"workshop-2.html","id":"exercise-2-calculate-summary-statistics-before-handling-na-1","chapter":"Workshop 2","heading":"Exercise 2: Calculate Summary Statistics Before Handling NA","text":"Dataset: mtcarsTask: mtcars dataset almost complete let’s pretend values missing mpg (miles per gallon) column. First, artificially introduce missing values mpg column (e.g., set first three values mpg NA). , calculate print mean standard deviation mpg without removing imputing missing values.Hint: Modify mtcars$mpg directly introduce NAs. Use mean() sd() functions na.rm = FALSE calculate statistics without handling NA.","code":"\n# Artificially introduce missing values into the mpg column of mtcars\nmtcars$mpg[1:3] <- NA\n\n# Calculate and print the mean and standard deviation without removing NA\nmean_mpg <- mean(mtcars$mpg, na.rm = FALSE)\nsd_mpg <- sd(mtcars$mpg, na.rm = FALSE)\n\nprint(paste(\"Mean of mpg without handling NA:\", mean_mpg))\n#> [1] \"Mean of mpg without handling NA: NA\"\nprint(paste(\"Standard deviation of mpg without handling NA:\", sd_mpg))\n#> [1] \"Standard deviation of mpg without handling NA: NA\""},{"path":"workshop-2.html","id":"exercise-3-impute-missing-values-with-column-median-1","chapter":"Workshop 2","heading":"Exercise 3: Impute Missing Values with Column Median","text":"Dataset: mtcars modified mpgTask: First Calculate mean standard deviation handling missing values.,Impute artificially introduced missing values mpg column column’s median (excluding missing values). Print first 6 rows modified mtcars dataset.Now, calculate mean standard deviation imputed values.Hint: First, calculate median mpg excluding NAs. , use indexing replace NAs median.","code":"\n\nmean_mpg <- mean(mtcars$mpg, na.rm = TRUE)\nsd_mpg <- sd(mtcars$mpg, na.rm = TRUE)\n\nprint(paste(\"Mean of mpg  handling NA:\", mean_mpg))\n#> [1] \"Mean of mpg  handling NA: 19.9344827586207\"\nprint(paste(\"Standard deviation of mpg  handling NA:\", sd_mpg))\n#> [1] \"Standard deviation of mpg  handling NA: 6.31422859568932\"\n\n# Calculate the median of mpg excluding NAs\nmedian_mpg <- median(mtcars$mpg, na.rm = TRUE)\nmedian_mpg\n#> [1] 18.7\n# Impute the missing values with the median\nmtcars$mpg[is.na(mtcars$mpg)] <- median_mpg\n\n# Print the first 6 rows of the modified mtcars dataset\nhead(mtcars)\n#>                    mpg cyl disp  hp drat    wt  qsec vs am\n#> Mazda RX4         18.7   6  160 110 3.90 2.620 16.46  0  1\n#> Mazda RX4 Wag     18.7   6  160 110 3.90 2.875 17.02  0  1\n#> Datsun 710        18.7   4  108  93 3.85 2.320 18.61  1  1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0\n#>                   gear carb\n#> Mazda RX4            4    4\n#> Mazda RX4 Wag        4    4\n#> Datsun 710           4    1\n#> Hornet 4 Drive       3    1\n#> Hornet Sportabout    3    2\n#> Valiant              3    1\n##\nmean_mpg <- mean(mtcars$mpg)\nsd_mpg <- sd(mtcars$mpg)\n\nprint(paste(\"Mean of mpg  imputed NA:\", mean_mpg))\n#> [1] \"Mean of mpg  imputed NA: 19.81875\"\nprint(paste(\"Standard deviation of mpg imputed NA:\", sd_mpg))\n#> [1] \"Standard deviation of mpg imputed NA: 6.01205442316491\""},{"path":"workshop-2.html","id":"exercise-4-identifying-complete-rows-1","chapter":"Workshop 2","heading":"Exercise 4: Identifying Complete Rows","text":"Dataset: AirqualityTask: analysis, want ensure complete cases used. Create new dataset airquality includes rows without missing values. Print number rows original versus cleaned dataset.Hint Use complete.cases() dataset subset .","code":"\ndata(\"airquality\")\n# Create a new dataset from airquality that includes only rows without any missing values\ncomplete_Airquality<- airquality[complete.cases(airquality), ]\n\n# Print the number of rows in the original versus the cleaned dataset\nprint(paste(\"Original dataset rows:\", nrow(airquality)))\n#> [1] \"Original dataset rows: 153\"\nprint(paste(\"Cleaned dataset rows:\", nrow(complete_Airquality)))\n#> [1] \"Cleaned dataset rows: 111\""},{"path":"workshop-2.html","id":"exercise-5-advanced-imputation-on-a-subset-1","chapter":"Workshop 2","heading":"Exercise 5: Advanced Imputation on a Subset","text":"Dataset: mtcarsTask: Create subset mtcars containing mpg, hp (horsepower), wt (weight) columns. Introduce missing values hp wt columns (e.g., set first two values NA). Perform multiple imputation using mice package subset 3 imputations, extract third completed dataset. Print first 6 rows completed dataset.Hint: Subset mtcars first, modify add NAs. Use mice() imputation complete() extract desired imputed dataset.","code":"\n# Load the necessary package\nlibrary(mice)\n#> \n#> Attaching package: 'mice'\n#> The following object is masked from 'package:stats':\n#> \n#>     filter\n#> The following objects are masked from 'package:base':\n#> \n#>     cbind, rbind\n\n# Create a subset of mtcars\nmtcars_subset <- mtcars[, c(\"mpg\", \"hp\", \"wt\")]\n\n# Introduce missing values\nmtcars_subset$hp[1:2] <- NA\nmtcars_subset$wt[1:2] <- NA\n\n# Perform multiple imputation\nimputed_data_subset <- mice(mtcars_subset, m=3, method='pmm', seed = 123)\n#> \n#>  iter imp variable\n#>   1   1  hp  wt\n#>   1   2  hp  wt\n#>   1   3  hp  wt\n#>   2   1  hp  wt\n#>   2   2  hp  wt\n#>   2   3  hp  wt\n#>   3   1  hp  wt\n#>   3   2  hp  wt\n#>   3   3  hp  wt\n#>   4   1  hp  wt\n#>   4   2  hp  wt\n#>   4   3  hp  wt\n#>   5   1  hp  wt\n#>   5   2  hp  wt\n#>   5   3  hp  wt\n\n# Extract the third completed dataset\ncompleted_data_subset <- complete(imputed_data_subset, 3)\n\n# Print the first 6 rows of the completed data\nhead(completed_data_subset)\n#>                    mpg  hp    wt\n#> Mazda RX4         18.7 175 3.440\n#> Mazda RX4 Wag     18.7 109 3.440\n#> Datsun 710        18.7  93 2.320\n#> Hornet 4 Drive    21.4 110 3.215\n#> Hornet Sportabout 18.7 175 3.440\n#> Valiant           18.1 105 3.460\n# Sample data for the lengths of bolts\nbolt_lengths <- c(6, 7, 7.5, 5.1, 4.9, 5.2, 6.1, 6.5, 6.8, 7)\n\n# Perform the Wilcoxon Signed-Rank Test\n# H0: median bolt length = 5 cm\n# Ha: median bolt length != 5 cm\ntest_result <- wilcox.test(bolt_lengths, mu = 5, alternative = \"two.sided\", conf.int = TRUE)\n#> Warning in wilcox.test.default(bolt_lengths, mu = 5,\n#> alternative = \"two.sided\", : cannot compute exact p-value\n#> with ties\n#> Warning in wilcox.test.default(bolt_lengths, mu = 5,\n#> alternative = \"two.sided\", : cannot compute exact\n#> confidence interval with ties\n\n# Output the results\nprint(test_result)\n#> \n#>  Wilcoxon signed rank test with continuity correction\n#> \n#> data:  bolt_lengths\n#> V = 53.5, p-value = 0.009252\n#> alternative hypothesis: true location is not equal to 5\n#> 95 percent confidence interval:\n#>  5.500024 6.999965\n#> sample estimates:\n#> (pseudo)median \n#>            6.2\n\n# Extract and print the p-value explicitly\ncat(\"P-value:\", test_result$p.value, \"\\n\")\n#> P-value: 0.009252446"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
